## Reinforcement Learning Example [01-10]

Recommended article: 【Control Theory】 [Stochastic Control Theory](https://jb243.github.io/pages/895) 

---

<br>

## Q5.

Let (X<sub>t</sub>)<sub>t≥0</sub> be a stochastic process with state space S. Let P be a transition operator on functions f : S → ℝ, and let I denote the identity operator. For every bounded function f : S → ℝ, define

M<sub>t</sub><sup>f</sup> = f(X<sub>t</sub>) - f(X<sub>0</sub>) - ∑<sub>τ = 0 to (t-1)</sub> (P - I)f(X<sub>τ</sub>),

and let ℱ<sub>t</sub> = σ(X<sub>0</sub>, …, X<sub>t</sub>) be the natural filtration generated by X. Show that the following two statements are equivalent:

1. (X<sub>t</sub>)<sub>t≥0</sub> is a Markov chain with transition operator P.

2. For every bounded function f, the process (M<sub>t</sub><sup>f</sup>)<sub>t≥0</sub> is a martingale with respect to the filtration (ℱ<sub>t</sub>)<sub>t≥0</sub>.

## A5.

<br>

<br>

## Q8. 

An individual is offered 3 to 1 odds in a coin tossing game where she wins whenever a tail occurs. However, she suspects that the coin is biased and has an a priori probability distribution with CDF F(p) and pdf f(p), for the probability p that a head occurs at each toss. A maximum of T coin tosses is allowed. The individual's objective is to determine a policy of deciding whether to continue or stop participating in the game, given the outcomes of the game so far, so as to maximize her earnings.

(i) Identify an information state for the problem and write down the equation determining its evolution.

(ii) Write down the dynamic program for this problem.

## A8.

**Case 1.** For general information state,

<br>

<img width="658" height="257" alt="스크린샷 2025-11-23 오후 4 54 06" src="https://github.com/user-attachments/assets/c87c2e34-0270-4a64-9710-ab0b2b064a45" />

<br>

**Case 1.** For specific information state,

<br>

<img width="703" height="353" alt="스크린샷 2025-11-23 오후 4 52 32" src="https://github.com/user-attachments/assets/b507139c-5ff2-44dd-863f-d6c91b54e452" />

<br>

---

_Input: 2025.11.21 01:30_
