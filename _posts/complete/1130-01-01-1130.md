## Reinforcement Learning Example [01-10]

Recommended article: ã€Control Theoryã€‘ [Stochastic Control Theory](https://jb243.github.io/pages/895) 

---

<br>

## Q4.

Consider a finite-state ergodic discrete time Markov chain {X<sub>t</sub>}<sub>tâˆˆZ<sub>+</sub></sub> on state-space ğ’® = {1, 2, ..., I} with transition matrix P and initial distribution Î¼ (for X<sub>0</sub>). For (bounded) cost vector C let (L, J) be a (bounded) solution to the Poisson equation for (C, P), i.e., (L, J) satisfy (vector) equation

<br>

<center>L + 1<b>J</b> = C + PL</center>

<br>

Then show the following: 

(a) The above equation can be written as 

<br>

<center>L(X<sub>t</sub>) + J = C(X<sub>t</sub>) + ğ”¼[L(X<sub>t+1</sub>) ã…£ X<sub>t</sub>]</center>

<br>

where L(X<sub>t</sub>) = L(i) (the i<sup>th</sup> component of column vector L) if X<sub>t</sub> = i.

(b) Then, using the Markov property show that the random process {M<sub>t</sub>}<sub>tâˆˆZ<sub>+</sub></sub> given by 

<br>

<center>M<sub>0</sub> := L(X<sub>0</sub>), M<sub>t+1</sub> := L(X<sub>t+1</sub>) + âˆ‘<sub>s=0 to t</sub> C(X<sub>s</sub>) - (t+1) J, t = 0, 1, 2, Â·Â·Â·</center>

<br>

is a martingale with respect to filtration {â„±<sub>t</sub>}<sub>tâˆˆZ<sub>+</sub></sub> where â„±<sub>t</sub> = Ïƒ(X<sub>0</sub>, X<sub>1</sub>, ..., X<sub>t</sub>). As ğ”¼[ã…£M<sub>t</sub>ã…£] < +âˆ follows from the boundedness of C, L and J, you only need to show properties (2)â€•adaptedness, i.e., M<sub>t</sub> is â„±<sub>t</sub> measurableâ€•, and (3)â€•ğ”¼[M<sub>t+1</sub>ã…£â„±<sub>t</sub>] = M<sub>t</sub> w.p. 1. For property (3) you will need to use part (a) of this question. 

## A4. 

(a) 

Let's check the dimensionality of the given Poisson equation 

<br>

<img width="706" height="181" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-12-26 á„‹á…©á„’á…® 10 26 12" src="https://github.com/user-attachments/assets/6f6912f3-3591-463d-b3bf-6225a830e2a6" />

<br>

Here, P(i,j) (i-th row, j-th column) means the transition probability â„™(jã…£i). Thus, the i-th component of the equation is

<br>

<center>L(i) + J = C(i) + âˆ‘<sub>j=1 to I</sub> P(i,j)Â·L(j) = C(i) + ğ”¼[L(j)ã…£i)</center>

<br>

Equivalently, given X<sub>t</sub> = i, it can be represented as

<br>

<center>L(X<sub>t</sub>) + J = C(X<sub>t</sub>) + ğ”¼[L(X<sub>t+1</sub>)ã…£X<sub>t</sub>)</center>

<br>

I replaced L(j) with L(X<sub>t+1</sub>) in light of the meaning of the transition.

(b) 

M<sub>0</sub> = L(X<sub>0</sub>sub>) (measurable by X<sub>0</sub>)

M<sub>1</sub> = L(X<sub>1</sub>) + C(X<sub>0</sub>) - J (measurable by X<sub>0:1</sub>)

M<sub>2</sub> = L(X<sub>2</sub>) + C(X<sub>0</sub>) + C(X<sub>1</sub>) - 2J (measurable by X<sub>0:2</sub>)

Generally speaking, X: Î© â†’ â„ is measurable when X<sup>-1</sup>(A) = {w âˆˆ Î© : X(w) âˆˆ A} âˆˆ â„± âˆ€ A âˆˆ ğ’¢ given a pair (Î©, â„±) and (â„, ğ’¢). Of note, the measurability can be defined without a measure â„™, and ğ’¢ is generally set to ğ’¢ = â„¬(â„), where â„¬ is Borel Ïƒ-algebra. According to the above observations, we can think of the inverse function of X<sub>0:t</sub> for M<sub>0</sub> especially given a finite-stage and a finite state set and the existence of the range of the inverse function with the same reasons. Thus, M<sub>t</sub> is â„±<sub>t</sub>-measurable.

Obviously, â„±<sub>0</sub> âŠ† â„±<sub>1</sub> âŠ† â„±<sub>2</sub> âŠ† â‹¯ (âˆµ Ïƒ(X<sub>0:t-1</sub>) âŠ† Ïƒ(X<sub>0:t</sub>)), so we can find the adaptedness property of filtration. So the question is whether ğ”¼[M<sub>t+1</sub>ã…£â„±<sub>t</sub>] = M<sub>t</sub> is true or not. This means the best prediction of M<sub>t+1</sub> can be projected on â„±<sub>t</sub>-measurable space, which is beneficial for future prediction given the current condition. Using (a), we obtain

<br>

<img width="590" height="379" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-12-26 á„‹á…©á„’á…® 11 50 27" src="https://github.com/user-attachments/assets/30c673b6-bcd0-405d-a839-7e4d8a9d049d" />

<br>

Here, Doob's theorem states that Ïƒ(X<sub>1</sub>, ..., X<sub>n</sub>) is identical to any functions with g(X<sub>1</sub>, ..., X<sub>n</sub>) form. With a filtration nature of {â„±<sub>t</sub>}<sub>tâˆˆâ„¤<sub>+</sub></sub> and already proved properties (1), (2) and (3), {M<sub>t</sub>}<sub>tâˆˆâ„¤<sub>+</sub></sub> is a martingale.

<br>

<br>

<br>

## Q5.

Let (X<sub>t</sub>)<sub>tâ‰¥0</sub> be a stochastic process with state space S. Let P be a transition operator on functions f : S â†’ â„, and let I denote the identity operator. For every bounded function f : S â†’ â„, define

M<sub>t</sub><sup>f</sup> = f(X<sub>t</sub>) - f(X<sub>0</sub>) - âˆ‘<sub>Ï„ = 0 to (t-1)</sub> (P - I)f(X<sub>Ï„</sub>),

and let â„±<sub>t</sub> = Ïƒ(X<sub>0</sub>, â€¦, X<sub>t</sub>) be the natural filtration generated by X. Show that the following two statements are equivalent:

1. (X<sub>t</sub>)<sub>tâ‰¥0</sub> is a Markov chain with transition operator P.

2. For every bounded function f, the process (M<sub>t</sub><sup>f</sup>)<sub>tâ‰¥0</sub> is a martingale with respect to the filtration (â„±<sub>t</sub>)<sub>tâ‰¥0</sub>.

## A5.

_Proof on 1 â†’ 2_

M<sub>t+1</sub><sup>f</sup> - M<sub>t</sub><sup>f</sup> = f(X<sub>t+1</sub>) - f(X<sub>t</sub>) - (P-I)f(X<sub>t</sub>) = f(X<sub>t+1</sub>) - Pf(X<sub>t</sub>)

If (X<sub>t</sub>) is a Markov chain with transition probability matrix P, ğ”¼[f(X<sub>t+1</sub>) ã…£ â„±<sub>t</sub>] = Pf(X<sub>t</sub>)

âˆ´ ğ”¼[M<sub>t+1</sub><sup>f</sup> - M<sub>t</sub><sup>f</sup> ã…£ â„±<sub>t</sub>] = ğ”¼[f(X<sub>t+1</sub>) - Pf(X<sub>t</sub>) ã…£ â„±<sub>t</sub>] = 0

âˆ´ ğ”¼[M<sub>t+1</sub><sup>f</sup> ã…£ â„±<sub>t</sub>] = M<sub>t</sub><sup>f</sup>, so M<sub>t</sub><sup>f</sup> is a Martingale.

<br>

_Proof on 2 â†’ 1_

M<sub>t+1</sub><sup>f</sup> - M<sub>t</sub><sup>f</sup> = f(X<sub>t+1</sub>) - f(X<sub>t</sub>) - (P-I)f(X<sub>t</sub>) = f(X<sub>t+1</sub>) - Pf(X<sub>t</sub>)

Since M<sub>t</sub><sup>f</sup> is a Martingale, ğ”¼[M<sub>t+1</sub><sup>f</sup> - M<sub>t</sub><sup>f</sup> ã…£ â„±<sub>t</sub>] = ğ”¼[f(X<sub>t+1</sub>) - Pf(X<sub>t</sub>) ã…£ â„±<sub>t</sub>] = 0

That is, ğ”¼[f(X<sub>t+1</sub>) ã…£ â„±<sub>t</sub>] = Pf(X<sub>t</sub>) is established for all bounded f.

It indicates that if the total past history â„±<sub>t</sub> is given, the next distribution is determined by only X<sub>t</sub>. â†’ 1-step Markov property.

If it is applied repetitively, ğ”¼[g(X<sub>t+k</sub>) ã…£ â„±<sub>t</sub>] = P<sup>k</sup>g(X<sub>t</sub>) is established, implying the total Markov property.

<br>

<br>

## Q8. 

An individual is offered 3 to 1 odds in a coin tossing game where she wins whenever a tail occurs. However, she suspects that the coin is biased and has an a priori probability distribution with CDF F(p) and pdf f(p), for the probability p that a head occurs at each toss. A maximum of T coin tosses is allowed. The individual's objective is to determine a policy of deciding whether to continue or stop participating in the game, given the outcomes of the game so far, so as to maximize her earnings.

(i) Identify an information state for the problem and write down the equation determining its evolution.

(ii) Write down the dynamic program for this problem.

## A8.

Information state can be defined as the posterior probability â„™(outcome ã…£ prior).

**Case 1.** For general information state,

<br>

<img width="658" height="257" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-23 á„‹á…©á„’á…® 4 54 06" src="https://github.com/user-attachments/assets/c87c2e34-0270-4a64-9710-ab0b2b064a45" />

<br>

**Case 1.** For specific information state,

<br>

<img width="703" height="353" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-23 á„‹á…©á„’á…® 4 52 32" src="https://github.com/user-attachments/assets/b507139c-5ff2-44dd-863f-d6c91b54e452" />

<br>

<br>

## Q9. 

Show that W<sub>âˆ</sub>(i), i=1,â€¦,I is the solution of the linear program: Maximize âˆ‘<sub>i=1 to I</sub> z(i) subject to z(i) â‰¤ c(i,u) + Î²âˆ‘<sub>j=1 to I</sub> P<sub>ij</sub>(u) z(j), âˆ€u,i

## A9.

Let Î² âˆˆ (0, 1), a probabilistic transition matrix P(u), and the one-step cost c be given. Define the Bellman operator T by 

<br>

<center>(Tv)(i) = min<sub>u</sub> { c(i, u) + Î² Î£<sub>j</sub> P<sub>ij</sub>(u) v(j) }</center>

<br>

W<sub>âˆ</sub> is the unique fixed point of this equation: W<sub>âˆ</sub> = TW<sub>âˆ</sub>. For all i and u, we have

<br>

<center>Wâˆ(i) = min<sub>u'</sub> { c(i, u') + Î² Î£<sub>j</sub> P<sub>ij</sub>(u') W<sub>âˆ</sub>(j) } â‰¤ c(i, u) + Î² Î£<sub>j</sub> P<sub>ij</sub>(u) W<sub>âˆ</sub>(j)</center>

<br>

so W<sub>âˆ</sub>(i) also satisfies the given inequality constraint. Since the constraint holds for all u, we have

<br>

<center>z(i) â‰¤ c(i, u) + Î² Î£<sub>j</sub> P<sub>ij</sub>(u) z(j) for all u â‡’ z(i) â‰¤ (Tz)(i)</center>

<br>

Here we can see the monotonicity of the operator T: 

<br>

<center>x â‰¤ y â‡’ c(i, u) + Î² Î£<sub>j</sub> P<sub>ij</sub>(u) x(j) â‰¤ c(i, u) + Î² Î£<sub>j</sub> P<sub>ij</sub>(u) y(j) â‡’ (Tx)(i) â‰¤ (Ty)(i)</center>

<br>

Therefore, z â‰¤ Tz â‰¤ T<sup>2</sup>z â‰¤ Â·Â·Â· â‰¤ lim<sub>nâ†’âˆ</sub> T<sup>n</sup>z = W<sub>âˆ</sub>. The uniqueness of Wâˆ and the fact that any z converges to W<sub>âˆ</sub> are already guaranteed by the Banach fixed-point theorem and the Bellman contraction mapping. Moreover, any feasible z is bounded as follows:

<br>

<center>||z||<sub>âˆ</sub> â‰¤ max<sub>i,u</sub> c(i, u) + Î² ||z||<sub>âˆ</sub> â‡” ||z||<sub>âˆ</sub> â‰¤ max<sub>i,u</sub> c(i, u) / (1 âˆ’ Î²)</center>

<br>

Hence Î£<sub>i</sub> z(i) â‰¤ Î£<sub>i</sub> W<sub>âˆ</sub>(i). In summary, W<sub>âˆ</sub> is a feasible solution that satisfies the constraints of the given linear program, and since z â‰¤ W<sub>âˆ</sub> for all feasible z, the solution that maximizes Î£<sub>i</sub> z(i) is z = W<sub>âˆ</sub>, and this W<sub>âˆ</sub> is unique.

<br>

<br>

## Q10.

Show that the minimum cost is the solution of the linear program: maximize J\* subject to J\* + w(i) â‰¤ c(i,u) + âˆ‘<sub>j=1 to I</sub> P<sub>ij</sub>(u)w(j), 1 â‰¤ i â‰¤ I, u âˆˆ U.

## A10.

We can define J<sub>opt</sub>\* as follows: J<sub>opt</sub>\* + h(i) = min<sub>uâˆˆU</sub> {c(i,u) + âˆ‘<sub>j</sub> P<sub>ij</sub>(u)h(j)}, âˆ€i. If we set w(i) = h(i), we know J<sub>opt</sub>\*, which is LP feasible solution, achieves the minimum cost, implying 

<br>

<img width="342" height="83" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-23 á„‹á…©á„’á…® 6 17 00" src="https://github.com/user-attachments/assets/eec71741-3dfc-49f8-81f9-2ede93d90aee" />

<br>

For any arbitrary i, u, and LP feasible solution L, we have

<br>

<img width="650" height="239" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-23 á„‹á…©á„’á…® 6 20 29" src="https://github.com/user-attachments/assets/c263a250-d760-4bf6-8363-2b3f4852904e" />

<br>

Let g be an **optimal stationary policy** whose long-run average cost is J<sub>opt</sub>*. Then 

<br>

<img width="451" height="117" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-23 á„‹á…©á„’á…® 6 23 00" src="https://github.com/user-attachments/assets/c7542dbd-6944-4e9b-a62e-a484907413d1" />

<br>

Here, I used the sample-path optimality (a stronger statement), which can be derived from the Martingale difference sequence (MDS). Let J\* := sup{J ã…£ âˆƒw s.t. (J, w) is LP feasible} be the optimal value of the LP. Since J<sub>opt</sub>\* â‰¥ J for every LP feasible J, we have J<sub>opt</sub>\* â‰¥ J\*. Therefore, the minimal cost J<sub>opt</sub>\* from the Bellman optimality equation is identical to J\* from: max J\* s.t. J\* + w(i) â‰¤ c(i, u) + âˆ‘<sub>j</sub> P<sub>ij</sub>(u) w(j).

<br>

---

_Input: 2025.11.21 01:30_
