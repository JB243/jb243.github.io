## **Chapter 9-1. Robust MDP**

Recommended reading: 【Control Theory】 [Stochastic Control Theory](https://jb243.github.io/pages/895)

---

**1.** [General Overview](#1-general-overview)

**2.** [Finite-horizon robust MDP](#2-finite-horizon-robust-mdp)

**3.** [Infinite-horizon robust MDP](#3-infinite-horizon-robust-mdp)

**4.** [Applications](#4-applications)

**5.** [Appendix](#5-appendis)

---

**a.**[Garud Iyengar (2005)](https://www.researchgate.net/publication/220442530_Robust_Dynamic_Programming?enrichId=rgreq-820dcdca44aab05ffd6634355d931a44-XXX&enrichSource=Y292ZXJQYWdlOzIyMDQ0MjUzMDtBUzoyNTcwNTczMzI5MjAzMjFAMTQzODI5ODY1MjYxMQ%3D%3D&el=1_x_3&_esc=publicationCoverPdf)

**b.**[Arnab Nilim and Laurent El Ghaoui (2003)](https://papers.nips.cc/paper/2367-robustness-in-markov-decision-problems-with-uncertain-transition-matrices)

--

## **1. General Overview**

⑴ Introduction

> ① Goal: Choosing a robust policy under model ambiguity (i.e., **transition probability P is uncertain**)

>> ○ Uncertainty: The probability distribution is known, but randomness exists according to that probability.

>> ○ Ambiguity: The probability distribution itself is unknown. In other words, it’s unclear which distribution is correct.

> ② Problem Definition

>> ○ Standard MDP

<br>

<img width="320" height="58" alt="스크린샷 2025-10-29 오전 9 03 40" src="https://github.com/user-attachments/assets/1d4acc11-517d-4ebe-9287-e30fc778d22c" />

<br>

>> ○ Robust MDP: For each policy π, the problem is to find the worst-case transition probability P.

<br>

<img width="504" height="58" alt="스크린샷 2025-10-29 오전 9 03 59" src="https://github.com/user-attachments/assets/d06a959b-16a1-4e12-adf2-4debac0d2073" />

<br>

>> ○ **Problem 1.** The DP optimal policy is very sensitive to small changes in transition probabilities.

>>> ○ Rain probability 0.3 → optimal to carry an umbrella.

>>> ○ Rain probability 0.2 → optimal not to carry an umbrella.

>>> ○ Only 0.1 difference, but the optimal policy changes.

>> ○ **Problem 2.** Each (s, a) adds an inner minimization inf<sub>p∈𝒫(s,a)</sub> 𝔼<sup>p</sup>[·], making it heavier computationally than standard MDP.

>> ○ **Problem 3.** Impossibility of linear programming: Depending on the choice of 𝒫, it can become non-convex.

>>> ○ Fixed transition matrix P: intersection of half-spaces → convex.

>>> ○ inf<sub>P</sub> with ≥ constraint: epigraph of a concave roof → generally non-convex.

<br>

<img width="718" height="284" alt="스크린샷 2025-10-29 오후 1 55 28" src="https://github.com/user-attachments/assets/ecfa69be-2e80-4a6a-b7c7-6c8baec91473" />

<br>

> ③ Solution

>> ○ **Adversarial Setting**

>>> ○ Robust MDP is a game between a decision maker defining π and an adversary finding the infimum for each π.

>>> ○ **Question**: Since game theory and optimization are conceptually different, is robust MDP closer to game theory or optimization theory?

>> ○ **Rectangularity Assumption**: Means the “choice” of transition probabilities at each time step is independent of others.

<br>

<img width="501" height="131" alt="스크린샷 2025-10-29 오전 9 05 18" src="https://github.com/user-attachments/assets/6521ca85-03f5-4267-8ec8-e6e56c00739c" />

<br>

>>> ○ **Purpose of introduction**: To make DP decomposition (Bellman recursion) possible.

>>> ○ A stronger assumption than simple statistical independence. Usually holds for stationary policies.

>>> ○ ℳ(ℬ): Set of probability measures over discrete set ℬ.

>>> ○ 𝒯<sup>d<sub>t</sub></sup>: Set of conditional distributions consistent with a history-dependent decision rule d<sub>t</sub>. To describe the conditional distribution, one must specify the joint distribution p<sub>h</sub>(a,s) of action a and next state s<sub>t+1</sub>.

>>> ○ ×: Cartesian product, so each element of 𝒯<sup>π</sup> is a tuple of per-stage distributions like (p<sub>0</sub>, p<sub>1</sub>, ..., p<sub>N-1</sub>).

>>> ○ π doesn’t appear explicitly in 𝒯<sup>π</sup> because it’s defined via d<sub>t</sub>’s, but when π changes, d<sub>t</sub>’s change, hence 𝒯<sup>π</sup> changes.

>>> ○ **Result:** Computation becomes tractable (mitigates curse of dimensionality), and if 𝒫(s, a) is convex, the inner problem is also convex.

>> ○ **Perfect Observation MDP**: Observe state s<sub>t</sub> and choose a<sub>t</sub>.

>> ○ **Time-invariant**: The reward r(s<sub>t</sub>, a<sub>t</sub>, s<sub>t+1</sub>) is known and time-invariant.

⑵ Background Theory

> ① [DP](https://www.youtube.com/watch?v=t9RBuyBmFdQ)(Dynamic Programming)

> ② ϵ-optimal policy

> ③ [Banach Fixed Point Theorem](https://jb243.github.io/pages/1827)

> ④ [Game Theory](https://jb243.github.io/pages/1914)

> ⑤ [Information Theory](https://jb243.github.io/pages/2145)

<br>

<br>

## **2. Finite-horizon Robust MDP**

⑴ **Overview**: For computation, state/action space is finite.

⑵ Value function after policy π is fixed.

<br>

<img width="507" height="71" alt="스크린샷 2025-10-29 오전 9 08 14" src="https://github.com/user-attachments/assets/9e0865bb-d794-4d74-bac8-2693cfe6e7e3" />

<br>

⑶ Bellman equation (DP)

<br>

<img width="728" height="133" alt="스크린샷 2025-10-29 오전 9 08 31" src="https://github.com/user-attachments/assets/76d8f26d-8c96-4821-864e-68ec5bc45cf6" />

<br>

> ① The following induction assumption is used in the proof.

<br>

<img width="532" height="107" alt="스크린샷 2025-10-29 오전 9 08 57" src="https://github.com/user-attachments/assets/a9708e03-c871-410c-9aa7-b362555a1d18" />

<br>

> ② For a rigorous proof, use an ϵ-optimal policy and the equivalence method (show both LHS ≥ RHS and LHS ≤ RHS).

<br>

<br>

## **3. Infinite-horizon Robust MDP**

⑴ Overview: Under discount factor λ < 1 and rectangular uncertainty, the robust Bellman operator is also contractive → value/policy iteration converges.

⑵ Bellman equation (DP)

<br>

<img width="511" height="58" alt="스크린샷 2025-10-29 오전 9 09 36" src="https://github.com/user-attachments/assets/ba023511-5a64-4260-9243-57d8fa5e2189" />

<br>

⑶ Banach Fixed Point Theorem

<br>

<img width="634" height="222" alt="스크린샷 2025-10-29 오전 9 09 55" src="https://github.com/user-attachments/assets/51861a32-578a-4f9b-ba0b-4fe15094fd38" />

<br>

> ① Corollary: inf and sup do not alter the conclusion of the Banach Fixed Point Theorem (**key result**). Note that (b) is an NP-complete problem.

<br>

<img width="727" height="251" alt="스크린샷 2025-10-29 오전 9 10 28" src="https://github.com/user-attachments/assets/fab808ea-bebd-4c4f-beec-273f06d62cda" />

<br>

> ② Algorithm

<br>

<img width="603" height="296" alt="스크린샷 2025-10-29 오전 9 10 50" src="https://github.com/user-attachments/assets/0dc1cef0-8670-4d3a-8e7d-6a28a527da47" />

<br>

>> ○ Contractivity → error bound from current residual.

<br>

<img width="294" height="49" alt="스크린샷 2025-10-29 오전 9 11 10" src="https://github.com/user-attachments/assets/2ba51a14-06d6-46d8-bbc0-502a62e6063e" />

<br>

>> ○ Error of one update step.

<br>

<img width="386" height="50" alt="스크린샷 2025-10-29 오전 9 11 35" src="https://github.com/user-attachments/assets/3f6b0190-2955-4312-a92a-86548ff14ada" />

<br>

>> ○ Therefore, to ensure ㅣㅣṼ - V*ㅣㅣ ≤ ϵ / 2.

<br>

<img width="91" height="44" alt="스크린샷 2025-10-29 오전 9 12 09" src="https://github.com/user-attachments/assets/1236aa49-9fd3-48e2-b52e-da9de110f1e7" />

<br>

>> ○ The paper’s algorithm adds an extra 1/2 multiplier and stops when that condition is met.

<br>

<img width="91" height="41" alt="스크린샷 2025-10-29 오전 9 12 28" src="https://github.com/user-attachments/assets/14634146-5c02-42c3-ba6f-b7f985e09670" />

<br>

>> ○ This stricter limit ensures that even after updating V ← Ṽ, the error remains small enough (extra margin by triangle inequality), so when selecting a greedy policy in the next step, performance loss bound (usually (2λ / (1 - λ)) ㅣㅣV - V*ㅣㅣ) is also satisfied — a conservative (safety margin) setting.

⑷ Asymmetry between Decision Maker and Adversary

<br>

<img width="767" height="73" alt="스크린샷 2025-10-29 오전 9 12 55" src="https://github.com/user-attachments/assets/b9ba2df0-905f-405f-bc57-474feb1a4fb0" />

<br>

> ① If (1) only stationary policies are considered, under rectangularity, discounted infinite-horizon, and convex/compact P(s, a), even if the dynamic adversary can change choices each visit, the optimal response to a stationary policy is also “stationary” (same p for each (s,a)), yielding the same value.

> ② The dynamic adversary appears stronger, but under stationary policies, the actual worst-case response is static, giving equal value.

> ③ However, when the adversary is static, the decision maker’s stationary policy may not be optimal. A better option might be a non-stationary/dynamic (universal) policy that adjusts weights over time/history (Cover, 1991).

<br>

<br>

## **4. Applications**

⑴ Overview

> ① The overall V-problem is hard to solve via linear programming due to non-convexity induced by inf<sub>p</sub>.

> ② The proposed approach handles the outer structure via DP (value/policy iteration) and solves the inner minimization fast and convexly.

⑵ **Application 1.** KL Confidence

> ① Overview: The inner problem reduces to a one-dimensional convex search (exponential tilting form solution).

> ② **Data → KL divergence:** From observed frequency in (s, a), build MLE p̂<sub>sa</sub>, and via algebraic statistics (chi-square approximation), define confidence region (confidence level ω) as 𝒫(s, a) = {p: D(p ㅣㅣ p̂<sub>sa</sub>) ≤ tω}.

> ③ **Closed-form solution of robust expectation + 1D convex problem:** Solving Lagrangian of min<sub>p∈𝒫</sub> 𝔼<sup>p</sup>[v] gives f(γ) = γt + γlog(𝔼<sup>q</sup>[exp(-v / γ)]), a 1D convex function, with optimal distribution p*(s) ∝ q(s) exp((μ - v(s)) / γ), i.e., exponential tilting. So once γ is found, p* is directly obtained.

> ④ **Computation complexity guarantee:** f'(γ) is monotone/convex → bisection finds an ϵ-approximation efficiently (each f'(γ) evaluation is 𝒪(ㅣSㅣ)).

<br>

<img width="312" height="73" alt="스크린샷 2025-10-29 오전 9 14 47" src="https://github.com/user-attachments/assets/00e364db-e61e-412c-9c38-ce25ab3f280b" />

<br>

> ⑤ **Conclusion:** Modeling transition uncertainty in robust MDP with KL confidence sets yields a closed-form and fast 1D convex optimization (exponential tilting), making computation highly practical.

⑶ **Application 2.** L<sub>2</sub> Approximation (χ<sup>2</sup>-type) Sets

> ① Overview: Once sorted, thresholding enables fast computation → makes computationally heavy robust MDPs practically solvable.

> To be updated later.

⑷ **Application 3.** L<sub>1</sub> Approximation Sets

> ① D(p ㅣㅣ q) ≥ (1 / 2 ln 2) ㅣㅣp - qㅣㅣ<sub>1</sub><sup>2</sup> → 𝒫 = {p: ㅣㅣp - qㅣㅣ1 ≤ δ}, δ = √(2t ln2) used.

> ② L1 / L∞ type sets are convenient for modeling but weak as statistical confidence regions → in practice, L2-based sets are more recommended.

> To be updated later.

<br>

<br>

## **5. Appendix**

⑴ Found several typos as follows:

<br>

<img width="715" height="156" alt="스크린샷 2025-10-29 오전 9 16 49" src="https://github.com/user-attachments/assets/0ef1ec43-d346-4137-a6ba-b31e1d16cacc" />

<img width="683" height="165" alt="스크린샷 2025-10-29 오전 9 17 06" src="https://github.com/user-attachments/assets/1ea9366f-1fda-4307-a592-f90bb3555db4" />

<br>

---

_Input: 2025.10.27 20:40_
