## **Chapter 9-1. Robust MDP**

Recommended reading: ã€Control Theoryã€‘ [Stochastic Control Theory](https://jb243.github.io/pages/895)

---

**1.** [General Overview](#1-general-overview)

**2.** [Finite-horizon robust MDP](#2-finite-horizon-robust-mdp)

**3.** [Infinite-horizon robust MDP](#3-infinite-horizon-robust-mdp)

**4.** [Applications](#4-applications)

---

**a.** [Garud Iyengar (2005)](https://www.researchgate.net/publication/220442530_Robust_Dynamic_Programming?enrichId=rgreq-820dcdca44aab05ffd6634355d931a44-XXX&enrichSource=Y292ZXJQYWdlOzIyMDQ0MjUzMDtBUzoyNTcwNTczMzI5MjAzMjFAMTQzODI5ODY1MjYxMQ%3D%3D&el=1_x_3&_esc=publicationCoverPdf)

**b.** [Arnab Nilim and Laurent El Ghaoui (2003)](https://papers.nips.cc/paper/2367-robustness-in-markov-decision-problems-with-uncertain-transition-matrices), ([2004](https://cdn.aaai.org/Workshops/2004/WS-04-08/WS04-08-013.pdf))

---

<br>

## **1. General Overview**

â‘´ Introduction

> â‘  Goal: Choosing a robust policy under model ambiguity (i.e., **transition probability P is ambiguous**)

>> â—‹ Uncertainty: The probability distribution is known, but randomness exists according to that probability.

>> â—‹ Ambiguity: The probability distribution itself is unknown. In other words, itâ€™s unclear which distribution is correct.

> â‘¡ Problem Definition

>> â—‹ Standard MDP

<br>

<img width="320" height="58" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-29 á„‹á…©á„Œá…¥á†« 9 03 40" src="https://github.com/user-attachments/assets/1d4acc11-517d-4ebe-9287-e30fc778d22c" />

<br>

>> â—‹ Robust MDP: For each policy Ï€, the problem is to find the worst-case transition probability P.

<br>

<img width="504" height="58" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-29 á„‹á…©á„Œá…¥á†« 9 03 59" src="https://github.com/user-attachments/assets/d06a959b-16a1-4e12-adf2-4debac0d2073" />

<br>

>> â—‹ **Problem 1.** The DP optimal policy is very sensitive to small changes in transition probabilities.

>>> â—‹ Rain probability 0.3 â†’ optimal to carry an umbrella.

>>> â—‹ Rain probability 0.2 â†’ optimal not to carry an umbrella.

>>> â—‹ Only 0.1 difference, but the optimal policy changes.

>> â—‹ **Problem 2.** Each (s, a) adds an inner minimization inf<sub>pâˆˆğ’«(s,a)</sub> ğ”¼<sup>p</sup>[Â·], making it heavier computationally than standard MDP.

>> â—‹ **Problem 3.** Impossibility of linear programming: Depending on the choice of ğ’«, it can become non-convex.

>>> â—‹ Fixed transition matrix P: intersection of half-spaces â†’ convex.

>>> â—‹ inf<sub>P</sub> with â‰¥ constraint: epigraph of a concave roof â†’ generally non-convex.

>>> â—‹ **Summary:** In this paper, the â€œnon-convex issueâ€ refers to the fact that, when the overall ambiguous set **P** is non-rectangular, natureâ€™s worst-case choices become temporally coupled, and the inner infimum optimization loses its nice convex structure. The rectangularity assumption resolves this by decomposing **P** into convex sets for each stage and each stateâ€“action pair, so that the inner infimum in the robust Bellman equation is well-defined and remains a convex optimization problem.


<br>

<img width="718" height="284" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-29 á„‹á…©á„’á…® 1 55 28" src="https://github.com/user-attachments/assets/ecfa69be-2e80-4a6a-b7c7-6c8baec91473" />

<br>

> â‘¢ Solution

>> â—‹ **Rectangularity Assumption**: Means the â€œchoiceâ€ of transition probabilities at each time step is independent of others.

<br>

<img width="501" height="131" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-29 á„‹á…©á„Œá…¥á†« 9 05 18" src="https://github.com/user-attachments/assets/6521ca85-03f5-4267-8ec8-e6e56c00739c" />

<br>

>>> â—‹ **Purpose of introduction**: To make DP decomposition (Bellman recursion) possible.

>>> â—‹ A stronger assumption than simple statistical independence. Usually holds for stationary policies.

>>> â—‹ â„³(â„¬): Set of probability measures over discrete set â„¬.

>>> â—‹ ğ’¯<sup>d<sub>t</sub></sup>: Set of conditional distributions consistent with a history-dependent decision rule d<sub>t</sub>. To describe the conditional distribution, one must specify the joint distribution p<sub>h</sub>(a,s) of action a and next state s<sub>t+1</sub>.

>>> â—‹ Ã—: Cartesian product, so each element of ğ’¯<sup>Ï€</sup> is a tuple of per-stage distributions like (p<sub>0</sub>, p<sub>1</sub>, ..., p<sub>N-1</sub>).

>>> â—‹ Ï€ doesnâ€™t appear explicitly in ğ’¯<sup>Ï€</sup> because itâ€™s defined via d<sub>t</sub>â€™s, but when Ï€ changes, d<sub>t</sub>â€™s change, hence ğ’¯<sup>Ï€</sup> changes.

>>> â—‹ **Result:** Computation becomes tractable (mitigates curse of dimensionality), and if ğ’«(s, a) is convex, the inner problem is also convex.

>> â—‹ **Adversarial Setting**

>>> â—‹ Robust MDP is a game between a decision maker defining Ï€ and an adversary finding the infimum for each Ï€.

>>> â—‹ **Question 1.** Since game theory and optimization are conceptually different, is robust MDP closer to game theory or optimization theory?

<br>

| **optimization** | **game** |
| --- | --- |
| min<sub>xâˆˆX</sub> f(x) | min<sub>x<sub>i</sub> âˆˆ X<sub>i</sub></sub> f<sub>i</sub>(x<sub>i</sub>, x<sub>-i</sub>), i âˆˆ [N]
| (strong) convexivity | (strong) monotonicity |
| **(cvx)** global minimizer | **(cvx)** NE: f<sub>i</sub>(x<sub>i</sub>*, x<sub>-i</sub>*) â‰¤ f<sub>i</sub>(x<sub>i</sub>, x<sub>-i</sub>*) for i âˆˆ [N]
| **(ncvx)** B-/Clarke stationarity | **(ncvx)** quasi- / Clarke NE |

**Table 1.** Difference between optimization theory and game theory

<br>

>>> â—‹ **Question 2.** Does not single-agent system affect it?

>>> â—‹ **Answer:** The retangularity assumption simplifies the problem with a single-agent optimization theory.

>> â—‹ **Perfect Observation MDP**: Observe state s<sub>t</sub> and choose a<sub>t</sub>.

>> â—‹ **Time-invariant**: The reward r<sub>t</sub>(s<sub>t</sub>, a<sub>t</sub>, s<sub>t+1</sub>) is known and, if infinite-horizon case, time-invariant.

â‘µ Background Theory

> â‘  [DP](https://www.youtube.com/watch?v=t9RBuyBmFdQ)(Dynamic Programming)

> â‘¡ Ïµ-optimal policy

> â‘¢ [Banach Fixed Point Theorem](https://jb243.github.io/pages/1827)

> â‘£ [Game Theory](https://jb243.github.io/pages/1914)

> â‘¤ [Information Theory](https://jb243.github.io/pages/2145)

<br>

<br>

## **2. Finite-horizon Robust MDP**

â‘´ **Overview**: For computation, state/action space is finite.

â‘µ Value function after policy Ï€ is fixed.

<br>

<img width="507" height="71" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-29 á„‹á…©á„Œá…¥á†« 9 08 14" src="https://github.com/user-attachments/assets/9e0865bb-d794-4d74-bac8-2693cfe6e7e3" />

<br>

> â‘  Since s<sub>t+1</sub> is ambiguous, we allow the reward at time t to depend on st+1 as well: r<sub>t</sub>(s<sub>t</sub>, a<sub>t</sub>, s<sub>t+1</sub>) instead of r<sub>t</sub>(s<sub>t</sub>, a<sub>t</sub>).

â‘¶ Bellman equation (DP)

<br>

<img width="728" height="133" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-29 á„‹á…©á„Œá…¥á†« 9 08 31" src="https://github.com/user-attachments/assets/76d8f26d-8c96-4821-864e-68ec5bc45cf6" />

<br>

> â‘  The following induction assumption is used in the proof.

<br>

<img width="532" height="107" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-29 á„‹á…©á„Œá…¥á†« 9 08 57" src="https://github.com/user-attachments/assets/a9708e03-c871-410c-9aa7-b362555a1d18" />

<br>

> â‘¡ For a rigorous proof, use an Ïµ-optimal policy and the equivalence method (show both LHS â‰¥ RHS and LHS â‰¤ RHS).

<br>

<br>

## **3. Infinite-horizon Robust MDP**

â‘´ Overview: Under discount factor Î» < 1 and rectangular uncertainty, the robust Bellman operator is also contractive â†’ value/policy iteration converges.

â‘µ Bellman equation (DP)

<br>

<img width="511" height="58" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-29 á„‹á…©á„Œá…¥á†« 9 09 36" src="https://github.com/user-attachments/assets/ba023511-5a64-4260-9243-57d8fa5e2189" />

<br>

â‘¶ Banach Fixed Point Theorem

<br>

<img width="634" height="222" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-29 á„‹á…©á„Œá…¥á†« 9 09 55" src="https://github.com/user-attachments/assets/51861a32-578a-4f9b-ba0b-4fe15094fd38" />

<br>

> â‘  Corollary: inf and sup do not alter the conclusion of the Banach Fixed Point Theorem (**key result**). Note that (b) is an NP-complete problem.

<br>

<img width="727" height="251" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-29 á„‹á…©á„Œá…¥á†« 9 10 28" src="https://github.com/user-attachments/assets/fab808ea-bebd-4c4f-beec-273f06d62cda" />

<br>

> â‘¡ Value iteration algorithm

<br>

<img width="603" height="296" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-29 á„‹á…©á„Œá…¥á†« 9 10 50" src="https://github.com/user-attachments/assets/0dc1cef0-8670-4d3a-8e7d-6a28a527da47" />

<br>

>> â—‹ Contractivity â†’ error bound from current residual.

<br>

<img width="294" height="49" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-29 á„‹á…©á„Œá…¥á†« 9 11 10" src="https://github.com/user-attachments/assets/2ba51a14-06d6-46d8-bbc0-502a62e6063e" />

<br>

>> â—‹ Error of one update step.

<br>

<img width="386" height="50" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-29 á„‹á…©á„Œá…¥á†« 9 11 35" src="https://github.com/user-attachments/assets/3f6b0190-2955-4312-a92a-86548ff14ada" />

<br>

>> â—‹ Therefore, to ensure ã…£ã…£á¹¼ - V*ã…£ã…£ â‰¤ Ïµ / 2.

<br>

<img width="91" height="44" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-29 á„‹á…©á„Œá…¥á†« 9 12 09" src="https://github.com/user-attachments/assets/1236aa49-9fd3-48e2-b52e-da9de110f1e7" />

<br>

>> â—‹ The paperâ€™s algorithm adds an extra 1/2 multiplier and stops when that condition is met.

<br>

<img width="91" height="41" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-29 á„‹á…©á„Œá…¥á†« 9 12 28" src="https://github.com/user-attachments/assets/14634146-5c02-42c3-ba6f-b7f985e09670" />

<br>

>> â—‹ This stricter limit ensures that even after updating V â† á¹¼, the error remains small enough (extra margin by triangle inequality), so when selecting a greedy policy in the next step, performance loss bound (usually (2Î» / (1 - Î»)) ã…£ã…£V - V*ã…£ã…£) is also satisfied â€” a conservative (safety margin) setting.

â‘· Asymmetry between Decision Maker and Adversary

<br>

<img width="767" height="73" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-29 á„‹á…©á„Œá…¥á†« 9 12 55" src="https://github.com/user-attachments/assets/b9ba2df0-905f-405f-bc57-474feb1a4fb0" />

<br>

> â‘  If (1) only stationary policies are considered, under rectangularity, discounted infinite-horizon, and convex/compact P(s, a), even if the dynamic adversary can change choices each visit, the optimal response to a stationary policy is also â€œstationaryâ€ (same p for each (s,a)), yielding the same value.

> â‘¡ The dynamic adversary appears stronger, but under stationary policies, the actual worst-case response is static, giving equal value.

> â‘¢ However, when the adversary is static, the decision makerâ€™s stationary policy may not be optimal. A better option might be a non-stationary/dynamic (universal) policy that adjusts weights over time/history (Cover, 1991).

<br>

<br>

## **4. Applications**

â‘´ Overview

> â‘  The overall V-problem is hard to solve via linear programming due to non-convexity induced by inf<sub>p</sub>.

> â‘¡ The proposed approach handles the outer structure via DP (value/policy iteration) and solves the inner minimization fast and convexly.

â‘µ **Application 1.** KL Confidence

> â‘  Overview: The inner problem reduces to a one-dimensional convex search (exponential tilting form solution).

> â‘¡ **Data â†’ KL divergence:** From observed frequency in (s, a), build MLE pÌ‚<sub>sa</sub>, and via algebraic statistics (chi-square approximation), define confidence region (confidence level Ï‰) as ğ’«(s, a) = {p: D(p ã…£ã…£ pÌ‚<sub>sa</sub>) â‰¤ tÏ‰}.

> â‘¢ **Closed-form solution of robust expectation + 1D convex problem:** Solving Lagrangian of min<sub>pâˆˆğ’«</sub> ğ”¼<sup>p</sup>[v] gives f(Î³) = Î³t + Î³log(ğ”¼<sup>q</sup>[exp(-v / Î³)]), a 1D convex function, with optimal distribution p*(s) âˆ q(s) exp((Î¼ - v(s)) / Î³), i.e., exponential tilting. So once Î³ is found, p* is directly obtained.

> â‘£ **Computation complexity guarantee:** f'(Î³) is monotone/convex â†’ bisection finds an Ïµ-approximation efficiently (each f'(Î³) evaluation is ğ’ª(ã…£Sã…£)).

<br>

<img width="312" height="73" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-29 á„‹á…©á„Œá…¥á†« 9 14 47" src="https://github.com/user-attachments/assets/00e364db-e61e-412c-9c38-ce25ab3f280b" />

<br>

> â‘¤ **Conclusion:** Modeling transition uncertainty in robust MDP with KL confidence sets yields a closed-form and fast 1D convex optimization (exponential tilting), making computation highly practical.

â‘¶ **Application 2.** L<sub>2</sub> Approximation (Ï‡<sup>2</sup>-type) Sets

> â‘  Overview: Once sorted, thresholding enables fast computation â†’ makes computationally heavy robust MDPs practically solvable.

> To be updated later.

â‘· **Application 3.** L<sub>1</sub> Approximation Sets

> â‘  D(p ã…£ã…£ q) â‰¥ (1 / 2 ln 2) ã…£ã…£p - qã…£ã…£<sub>1</sub><sup>2</sup> â†’ ğ’« = {p: ã…£ã…£p - qã…£ã…£1 â‰¤ Î´}, Î´ = âˆš(2t ln2) used.

> â‘¡ L1 / Lâˆ type sets are convenient for modeling but weak as statistical confidence regions â†’ in practice, L2-based sets are more recommended.

> To be updated later.

<br>

---

_Input: 2025.10.27 20:40_
