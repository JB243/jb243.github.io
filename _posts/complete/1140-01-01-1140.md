## **Chapter 15. Multilayer Perceptron** (MLP)

Recommended Post: ã€Algorithmã€‘ [Algorithm Index](https://jb243.github.io/pages/1278)

---

**1.** [Universal Approximation Theorem](#1-universal-approximation-theorem)

**2.** [Multi-Layer Perceptron Algorithm](#2-multi-layer-perceptron-analysis)

**3.** [MLP Application](#3-mlp-application)

---

<br>

## **1\. Universal Approximation Theorem**

â‘´ Proposed by Cybenko (1989) and Hornik (1991)

â‘µ Details

> â‘  Hornik (1991) states that any bounded and normalized function f: â„d â†’ â„ can be represented by a finite-size neural network with a single hidden layer, having the same activation function and a single linear output neuron ([ref](http://neuralnetworksanddeeplearning.com/chap4.html))

> â‘¡ Let ğœ™ be a bounded, continuous, and monotonically increasing activation function. Also, let ğ¾ğ‘‘ be a compact subset of â„ğ‘‘, and let ğ¶(ğ¾ğ‘‘) be a continuous function on ğ¾ğ‘‘. Then, for any ğœ– > 0, there exist some ğ‘ âˆˆ â„•, real numbers ğ‘£ğ‘–, ğ‘ğ‘–, â„ğ‘‘, and vectors ğœ”ğ‘– such that, when defined as follows,

<br>

<img width="257" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-03-24 á„‹á…©á„Œá…¥á†« 11 42 47" src="https://github.com/user-attachments/assets/5d3dc947-5fbb-4b89-b27f-57cf9f20bee4" />

<br>

> The following is obtained:

<br>

<img width="301" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-03-24 á„‹á…©á„Œá…¥á†« 11 43 06" src="https://github.com/user-attachments/assets/3f9cac30-83a1-4833-ac80-cee0813ee6cd" />

<br>

> â‘¢ The universal approximation theorem for the special case of sigmoid activation functions was already proven by Cybenko (1989)

â‘¶ Limitations

> â‘  This theory is theoretically interesting, but implementing it with a single hidden layer would require an excessively large number of neurons in the hidden layer, making it impractical: The effect of deep learning lies in depth, that is, in the number of hidden layers

<br>

<br>

## **2\. Multi-Layer Perceptron Algorithm** 

â‘´ Overview

> â‘  This section deals with a three-layer perceptron consisting of an input layer, hidden layer, and output layer

<br>

![image](https://github.com/user-attachments/assets/0e8195f1-d94d-49f8-93c0-3773de8c4e36)

**Figure 1.** Two-layer perceptron

<br>

>> â—‹ Input Layer: Generally, the input layer is not included in the number of perceptron layers

>> â—‹ Hidden Layer

>> â—‹ Output Layer

> â‘¡ Parameter Definition

>> â—‹ L: Number of neurons in the input layer

>> â—‹ M: Number of neurons in the hidden layer. Can be set as a hyperparameter

>> â—‹ N: Number of neurons in the output layer

>> â—‹ i: Iteration variable representing input layer neurons

>> â—‹ j: Iteration variable representing hidden layer neurons

>> â—‹ k: Iteration variable representing output layer neurons

>> â—‹ x<sub>Î¹</sub>: Node value of the Î¹-th input layer neuron, i.e., input value

>> â—‹ x<sub>0</sub>: Defined as -1

>> â—‹ h<sub>Î¶</sub>: Synaptic sum of the Î¶-th hidden layer neuron, also represented as h<sub>Î¶</sub><sup>hidden</sup> to avoid confusion

>> â—‹ a<sub>Î¶</sub>: Node value of the Î¶-th hidden layer neuron **(must be less than 1)**

>> â—‹ a<sub>0</sub>: Defined as -1

>> â—‹ h<sub>Îº</sub>: Synaptic sum of the Îº-th output layer neuron, also represented as hÎºoutput to avoid confusion

>> â—‹ y<sub>Îº</sub>: Node value of the Îº-th output layer neuron, i.e., output value **(must be less than 1)**

>> â—‹ t<sub>Îº</sub>: Target value of the Îº-th output layer neuron, i.e., the value provided by the training data

>> â—‹ v<sub>0Î¶</sub>: Bias applied to h<sub>Î¶</sub>

>> â—‹ v<sub>Î¹Î¶</sub>: Synaptic weight between the Î¹-th input layer neuron and Î¶-th hidden layer neuron

>> â—‹ Ï‰<sub>0Îº</sub>: Bias applied to hÎº

>> â—‹ Ï‰<sub>Î¶Îº</sub>: Synaptic weight between the Î¶-th hidden layer neuron and Îº-th output layer neuron

>> â—‹ **v**: Matrix containing all v<sub>Î¹Î¶</sub>

>> â—‹ **w**: Matrix containing all Ï‰<sub>Î¶Îº</sub>

>> â—‹ Î´<sub>h</sub>(Î¶): Error value of the Î¶-th hidden layer neuron

>> â—‹ Î´<sub>o</sub>(Îº): Error value of the Îº-th output layer neuron

>> â—‹ g(**Â·**): Activation function

> â‘¢ Note that the node value is the result of applying the activation function to the synaptic sum

> â‘£ Meaning of Bias: Reflects the pattern of the class itself, independent of individual datasets

 â‘µ Activation Function

> â‘  Identity Function = x: Also known as a [linear classifier](https://jb243.github.io/pages/2161)

> â‘¡ Sigmoid Ïƒ(x) = 1 / (1 + e<sup>-Ïƒx</sup>)

>> â—‹ Differentiable and the function value remains in [0, 1], which is why it has been traditionally used most often as an activation function

>> â—‹ Similar to biological experimental data

>> â—‹ Saturation curve can be adjusted by Ïƒ

>> â—‹ When Ïƒ diverges to âˆ, it becomes a hard limiter

>> â—‹ When the absolute value of x becomes large, the slope becomes close to 0, which reduces the learning capability of the model

> â‘¢ tanh(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))

> â‘£ arctan(x)

> â‘¤ Strong threshold function: Ï†<sub>Î²</sub>(x) = 1 if x â‰¥ Î²

> â‘¥ ReLU (rectified linear unit): max(0, x). Most frequently used

> â‘¦ leaky ReLU: max(0.1x, x)

> â‘§ maxout: max(w<sub>1</sub><sup>T</sup>x + b<sub>1</sub>, w<sub>2</sub><sup>T</sup>x + b<sub>2</sub>)

> â‘¨ elu (exponential linear unit): x if x â‰¥ 0; Î±(e<sup>x</sup> - 1) if x < 0

> â‘© [softmax](https://jb243.github.io/pages/1633)

<br>

<img width="198" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-03-24 á„‹á…©á„Œá…¥á†« 11 47 25" src="https://github.com/user-attachments/assets/88782b2e-39e2-4a21-a145-4671fa3b115a" />

<br>

 â‘¶ Loss Function: Also called error function or cost function

> â‘  Definition: A numerical measure of how accurate the predictions of the currently trained model are

> â‘¡ **Method 1.** L2 Loss Function

>> â—‹ Square the difference between the output and the target values, and use the sum of these squared differences, \( E \), as the error function to calculate the error.

>> â—‹ Here, the coefficient \( \frac{1}{2} \) is used for convenience when differentiating; multiplying by \( \frac{1}{2} \) is not mandatory.

>> â—‹ The loss function can also be defined in the form of an average rather than a sum, i.e., considering \( E/N \).

>> â—‹ Therefore, the error function can be expressed as follows:

<br>

<img width="173" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-03-24 á„‹á…©á„Œá…¥á†« 11 47 58" src="https://github.com/user-attachments/assets/b04682e4-fecd-4617-a06c-53642758b184" />

<br>

> â‘¢ **Method 2.** L1 Loss Function

>> â—‹ Depending on the magnitude of the output and target values, some values output negative and some output positive.

>> â—‹ These may cancel out each other, causing the problem where E0 approaches zero.

> â‘£ **Method 3.** [cross entropy](https://jb243.github.io/pages/2145)

>> â—‹ General Definition

<br>

<img width="234" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-03-24 á„‹á…©á„Œá…¥á†« 11 55 52" src="https://github.com/user-attachments/assets/942de367-94aa-4436-92e3-ea53ca47d566" />

<br>

>> â—‹ Binary Classification

<br>

<img width="376" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-03-24 á„‹á…©á„Œá…¥á†« 11 56 09" src="https://github.com/user-attachments/assets/547d422e-edc6-4dda-8bf9-9d76f43cd270" />

<br>

>> â—‹ If y is represented as a one-hot vector [0, Â·Â·Â·, 1, Â·Â·Â·, 0], it can be expressed as follows:

<br>

<img width="551" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-03-24 á„‹á…©á„Œá…¥á†« 11 56 24" src="https://github.com/user-attachments/assets/4ae26ec9-09ac-4b77-a379-d216a93264f0" />

<br>

> â‘¤ **Method 4.** [KLD](https://jb243.github.io/pages/2145#:,%C%8)(Kullback-Leibler divergence)

<br>

<img width="279" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-03-24 á„‹á…©á„Œá…¥á†« 11 56 40" src="https://github.com/user-attachments/assets/67f9a3a8-8411-40c0-a6f4-4134aecae5b4" />

<br>

> â‘¥ **Method 5.** Laplacian matrix and cost function

>> â—‹ The definition of Laplacian matrix

<br>

<img width="497" height="111" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-12-12 á„‹á…©á„’á…® 3 38 18" src="https://github.com/user-attachments/assets/f12c0b08-fb93-4512-acee-a557e8afec98" />

<br>

>> â—‹ Define the cons function using **f**<sup>T</sup>L**f** = âˆ‘<sub>e<sub>i,j</sub>, i < j</sub> (**f**(v<sub>i</sub>) - **f**(v<sub>j</sub>))<sup>2</sup> = (1/2) âˆ‘e<sub>i,j</sub> (**f**(v<sub>i</sub>) - **f**(v<sub>j</sub>))<sup>2</sup>.

>> â—‹ When a matrix **p** is used instead of **f**, it is denoted as tr(**p**áµ€L**p**).

> â‘¦ **Method 6.** DT (Delaunay triangulation) and cost function

<br>

<img width="395" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-03-24 á„‹á…©á„Œá…¥á†« 11 57 08" src="https://github.com/user-attachments/assets/11b136e4-58f8-468b-abcf-2c31d8ba034d" />

 **Figure 2.** Example of Delaunay triangulation

<br>

>> â—‹ The loss function is defined in a way similar to the Laplacian matrix, but the method of constructing the graph is different.

>> â—‹ Forms triangles such that no point lies inside the circumcircle (the circle that passes through all three vertices) of any triangle.

>> â—‹ Automatically adapts to variations in point density, providing more connections in denser regions and fewer in sparser regions.

> â‘§ [Other Types of Distance and Similarity](https://jb243.github.io/pages/879)

 â‘· **Backpropagation**

> â‘  Overview

>> â—‹ The process of computing the gradients required to use the gradient descent algorithm

>> â—‹ The process of updating weights (training) based on computing the gradient of the difference between the predicted values after forward propagation and the true values

>> â—‹ Weight computation uses differentiation, and the equations are reorganized for understanding using the chain rule

>> â—‹ The backpropagation algorithm was proposed by the PDP group, including Rumelhart, and computational cost issues were addressed by researchers including Geoffrey Hinton

>> â—‹ Neural networks have made major contributions to tasks such as character recognition and image recognition through pattern recognition

> â‘¡ **Example 1**

>> â—‹ **Step 1.** Adjust each weight Ï‰Î¶Îº with fixed Î¶ and Îº values, and update in the direction of -E(**w**).

>>> â—‹ The result from applying the chain rule and the definition of hÎº are as follows:

>>> â—‹ Using the definition of hÎº, the following can be calculated:

>>> â—‹ âˆ‚E / âˆ‚hÎº term represents error or variation, which makes it important enough to consider separately.

>>> â—‹ This can be solved using the chain rule.

>>> â—‹ The output value of neuron Îº in the output layer is as follows:

>>> â—‹ Here, g(Â·) can be considered a general function since the activation function can vary, including the sigmoid function.

>>> â—‹ Therefore, the update formula for output layer weight Ï‰Î¶Îº is as follows:

>> â—‹ **Step 2.** Adjust each weight vÎ¹Î¶ with fixed Î¹ and Î¶ values, and update in the direction of -E(**v**).

>>> â—‹ The result from applying the chain rule and the definition of hÎ¶ are as follows:

>>> â—‹ Using the definition of hÎ¶, the following can be calculated:

>>> â—‹ âˆ‚E / âˆ‚hÎ¶ term is also important enough to consider separately.

>>> â—‹ This can be solved using the chain rule.

>>> â—‹ However, depending on the definition of synaptic sum, activation function, and weight, the following equation emerges:

>>> â—‹ This directly means the following:

>>> â—‹ If the sigmoid function is adopted as the activation function, the equation can be simplified as follows:

>>> â—‹ Therefore, it is derived as follows:

>>> â—‹ Thus, the update rule for vÎ¹Î¶ is as follows:

>> â—‹ **Step 3.** If there are more hidden layers between inputs and outputs, the same calculation can be applied.

>>> â—‹ However, it becomes increasingly difficult to track which function should be differentiated.

> â‘¢ **Example 2**

<br>

![image](https://github.com/user-attachments/assets/b80db166-5285-4259-810c-bc900c266f37)

 **Figure 3.** Example of Gradient Descent Algorithm

<br>

> â‘£ **Example 3**

>> â—‹ Overview

>>> â—‹ Does not require complex differentiation.

>>> â—‹ Works like a recurrence relation.

>>> â—‹ The opposite of backpropagation is the feed-forward algorithm.

>> â—‹ Method

<br>

![image](https://github.com/user-attachments/assets/68978d53-c607-48ac-861e-43d6459da2df)

 **Figure 4.** Example of backpropagation algorithm

<br>

>>> â—‹ **Step 1.** Forward pass

>>> â—‹ **Step 2.** Upstream gradient at the very end is 1

>>> â—‹ **Step 3.** Calculate local gradient **:** The gradient generated at the node itself.

>>> â—‹ **Step 4.** Calculate downstream gradient **:** Computed as upstream gradient Ã— local gradient, based on the chain rule of calculus.

>>> â—‹ **Step 5.** Gradient implementation

>> â—‹ Patterns in Gradient Flow

<br>

![image](https://github.com/user-attachments/assets/86c6ec6e-fb0a-4f58-8446-e9ba8f307389)

 **Figure 5.** Patterns in Gradient Flow

<br>

>>> â—‹ Add gate **:** Gradient distributor

>>> â—‹ Mul gate **:** Swap multiplier

>>> â—‹ Copy gate **:** Gradient adder

>>> â—‹ Max gate **:** Gradient router

 â‘¸ **Gradient Descent Algorithm**

> â‘  Overview

>> â—‹ Definition **:** Method of updating weights using gradients obtained from backpropagation.

>> â—‹ Also called the update algorithm.

> â‘¡ The loss function generally follows the L2 loss function.

> â‘¢ Update equation represented in matrix form

> â‘£ Update equation for a single parameter

> â‘¤ Limitations **:** Still unresolved issues

>> â—‹ **Problem 1.** The surface may not be a convex function but a concave function or a saddle point.

>> â—‹ **Problem 2.** The cost function must be differentiable.

>>> â—‹ 0/1 loss, for example, is not always differentiable.

>>> â—‹ Tries to resolve using an alternative differentiable function.

>> â—‹ **Problem 3.** Learning near a local minimum is not fast.

> â‘¥ **Application 1.** [**Stochastic Gradient Descent**](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) (SGD): Proposed by Rumelhart et al. (1988)

>> â—‹ Definition: SGD is a method designed to address the high computational cost of full-batch gradient descent, which uses the entire dataset to perform each update. Instead of computing gradients on the full dataset at every iteration, SGD approximates the gradient using a randomly selected subset of samples (or even a single sample), enabling more frequent updates. In other words, full-batch gradient descent updates parameters using the gradient computed from the entire dataset, whereas SGD updates parameters using the gradient from a subset (or one sample).

>> â—‹ **Type 1.** Mini-Batch SGD **:** Usually considers minibatches of 32, 64, 128, Â·Â·Â·, 8192.

>> â—‹ **Type 2.** SGD **:** The most extreme case updates parameters based on a single element's result.

>> â—‹ Time cost **:** Mini-Batch SGD < Full-Batch SGD < SGD

>>> â—‹ Mini-Batch SGD reduces time cost by generating batches and reducing the number of variables involved in gradient calculation.

>>> â—‹ SGD, however, incurs time cost when generating batches.

>> â—‹ Optimum loss value **:** Full-Batch SGD < Mini-Batch SGD < SGD

>>> â—‹ Creating batches sacrifices accuracy to save time cost.

>>> â—‹ Batch gradient descent theoretically does not affect convergence value but introduces noise during training.

 â‘¹ **Type 3. Momentum**

> â‘  Algorithm based on the physical law that an object accelerates when a force is applied in the gradient direction.

> â‘¡ Movement resembles a ball rolling toward the optimal point.

 â‘º **Type 4.** **Nesterov Momentum** (NAG, Nesterov accelerated gradient)

> â‘  Calculates the gradient at the position pre-applied in the momentum direction.

 â‘» **Type 5.** **AdaGrad** (adaptive gradient algorithm)

> â‘  Initially learns significantly when the loss function gradient is large but reduces the learning rate as it approaches the optimal point.

 â‘¼ **Type 6.** **Adam** (adaptive moment estimation)

> â‘  Combines the advantages of momentum and AdaGrad methods.

> â‘¡ Moves like momentum but with less side-to-side shaking.

> â‘¢ **6-1.** AdamW

 â‘½ **Type 7.** **RMSProp** (root mean square prop)

> â‘  Uses an exponential moving average instead of simple accumulation to reflect the most recent gradients more strongly.

 â‘¾ **Type 8.** **L-BFGS** (limited memory BFGS) algorithm

â‘¿ **Type 9.** DistributedShampoo

â’€ **Type 10.** SOAP

â’ **Type 11.** Orthogonal-SGDM

â’‚ **Type 12.** stochastic spectral descent & RMSspectral

â’ƒ **Type 13.** [Muon](https://kellerjordan.github.io/posts/muon/) 

<br>

<br>

## **3\. MLP Applications**

 â‘´ **Example 1.** [Classification Algorithm](https://jb243.github.io/pages/2161)

 â‘µ **Example 2.** [Time-Series Prediction](https://jb243.github.io/pages/870#:- 97 B0 84,-3A 98 9E 97 A4%)

 â‘¶ **Example 3.** **Auto-Associative Learning** **:** Also called an [Autoencoder](https://jb243.github.io/pages/956)

<br>

---

_Input: 2018.06.09 10:01_

_Modified: 2021.11.21 22:29_
