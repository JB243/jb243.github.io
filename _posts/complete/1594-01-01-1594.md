## **Chapter 9. Theorems of Statistics I**

Higher category: 【Statistics】 [Statistics Overview](https://jb243.github.io/pages/1641)

---

**1.** [Markov inequality](#1-markov-inequality)

**2.** [Chebyshev inequality](#2-chebyshev-inequality)

**3.** [Cantelli inequality](#3-cantelli-inequality)

**4.** [Cauchy-Schwarz inequality](#4-cauchy-schwartz-inequality)

**5.** [Jensen inequality](#5-jensen-inequality)

**6.** [Law of large numbers](#6-law-of-large-numbers)

**7.** [Central extreme theorem](#7-central-extreme-theorem)

**8.** [Slutsky's theorem](#8-slutskys-theorem)

**9.** [Laplace's rule of succession](#9-laplaces-rule-of-succession)

---

**a.** [Collective wisdom](https://jb243.github.io/pages/1650)

---

<br>

## **1. Markov inequality**

⑴ theorem 

> when X is the random variable and a > 0, the following is established:

<br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcsGphm%2FbtrhVZJK5oy%2FnhTwBSSYKXUy7UW6DSm6ck%2Fimg.png" alt="drawing"/>
</center>
  <br>

⑵ **proof 1**.

> <span>let x<sub>1</sub>, ···, x<sub>n</sub> be events that make up the random variable X. by reordering the events, we can acquire | x<sub>i</sub> | ≥ a ⇔ i = r+1, ···, n</span>

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fda2uiU%2FbtrhThE7Aww%2FSnUERLqO46BPkj1WMKjYI1%2Fimg.png" alt="drawing"/>

<br>

⑶ **proof 2.** 

<br>

<img width="378" alt="스크린샷 2025-02-13 오후 1 53 04" src="https://github.com/user-attachments/assets/d5e82d8e-586e-4c53-b4cd-3c074915ae7d" />

<br>

<br>

## **2. Chebyshev's inequality** 

⑴ theorem

> there is a random variable with expected value of m and variance of σ<sup>2</sup>.  the variance is a finite value. then, for any real number k ＞ 0, the following inequality is established: 

<br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fb7TZ3N%2FbtrhTRMJq1U%2F96loK7jx10VDrAFPLWmI00%2Fimg.png" alt="drawing"/>
</center>
  <br>


> however, meaningful information is provided only when k>1.

⑵ **proof 1.** 

> <span>let x<sub>1</sub>, ···, x<sub>n</sub> be events that make up the random variable X. by rearranging the above events, we can acquire | x<sub>i</sub> - m | ≥ kσ ⇔ i = r+1, ···, n. </span>

<br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbqcHQ4%2FbtrhS4F0kpj%2FyiNZ662zjnVdfyJamkwR2K%2Fimg.png" alt="drawing"/>
</center>
  <br>

⑶ **proof 2.** 

> the case of Markov inequality with k = 2, ε = kσ, X → X - μ<sub>X</sub> is Chebyshev's inequality 

<br>

<br>

## **3. Cantelli inequality** (one-tailed chebyshev inequality) 

⑴ theorem

> there is a random variable with expected value of m and variance of σ<sup>2</sup>. the variance is a finite value. then, for any real number k ＞ 0, the following inequality is established: 

<br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdryNe0%2FbtrhTtSStY1%2FUfwWYTEkSmD0cZ6N2ebM7K%2Fimg.png" alt="drawing"/>
</center>
  <br>

⑵ proof 

> ① m = 0

>> let x<sub>1</sub>, ···, x<sub>n</sub> be events that make up the random variable X. by reordering the events, we can acquire x<sub>i</sub> ＜ kσ ⇔ i = r+1, ···, n. if we put kσ = t,

<br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FNjrlB%2FbtrhUlzZxHy%2FR9CoXzaKn1ZIWQI0w03rlK%2Fimg.png" alt="drawing"/>
</center>
  <br>

> ② m = μ ≠ 0 

>> X' = X - μ is a random variable with an expected value of 0 and a variance of σ<sup>2</sup>. so the following is established:

<br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbHVCXA%2FbtrhSLGI5QC%2FmWDdYv08yQlkskpNKVTbX0%2Fimg.png" alt="drawing"/>
</center>
  <br>

<br>

## **4. Cauchy-Schwarz inequality** 

⑴ theorem  

<br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcKtLG2%2FbtrhVYxjRYD%2FTBMnck7W7l32zkKmkxDrs0%2Fimg.png" alt="drawing"/>
</center>
  <br>

⑵ proof 

<br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FvVbYt%2FbtrhWInsY8d%2FopkKO38stExe6igK5vQIT0%2Fimg.png" alt="drawing"/>
</center>
  <br>

⑶ example 

> ① E(1 / X) ≥ 1 / E(X)

> ② ln (E(X)) ≥ E(ln X)

<br>

<br>

## **5. Jensen's inequality** 

⑴ [convex function and concave function](https://jb243.github.io/pages/1810)

> ① convex set**:** it refers to the case where the internally dividing points of any two points in a set are always elements of the set

> ② convex function**:** the function where ｛(x, y) <span>|</span> y ≥ f(x)｝ is a convex set. it is similar to 'convex downward'

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F7JdWG%2FbtrhS4ssXYe%2FGKWLQtAxYgOADnbU0Y6lcK%2Fimg.png" alt="drawing"/>

<br>

> ③ strictly convex function

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbZapSw%2FbtrhS3UDRiy%2FsRlD5P99zpyXXl5PS1qsi1%2Fimg.png" alt="drawing"/>

<br>

> ④ concave function**:** the function where｛(x, y) <span>|</span> y ≤ f(x)｝is a convex set. it is similar to 'convex upward'

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FSS8LD%2FbtrhVZ32NpB%2F39eKq2wWVoZeyL1uEdb3Y1%2Fimg.png" alt="drawing"/>

<br>

> ⑤ strictly concave function 

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fbwz9Jw%2FbtrhTgsGBjk%2FIifKIKyPWzkQKdmKfMjvW1%2Fimg.png" alt="drawing"/>

<br>

> ⑥ linear function**:** the case of both covex function and concave function

⑵ relationship between convexity and the [secondary derivative](https://jb243.github.io/pages/1810): f"(x) ≥ 0 and the convex function are necessary and sufficient. f"(x) ≤ 0 and the concave function are necessary and sufficient  

> ① **proof 1.** 

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbuSGKq%2FbtrhWJfBpwO%2Fr3tJqQLqH3q7eeZjnSgCYK%2Fimg.png" alt="drawing"/>

<br>

> ② **proof 2.** an easy proof showing that for the convex function, f"(x) ≥ 0 is established and for concave function, f"(x) ≤ 0 is established 

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbcE07S%2FbtrhWH29JXK%2FqJUesOiENXPKyGyZg9igMk%2Fimg.png" alt="drawing"/>

<br>

⑶ Jensen's inequality**:** expansion of dimension 

> ① when f is a convex function, the following inequality is established:

<br>

<img width="352" alt="스크린샷 2025-04-24 오후 3 56 53" src="https://github.com/user-attachments/assets/5e3f363b-629c-4da8-84e9-118a009a9fa2" />

<br>

> ② when f is a concave function, the following inequality is established: 

<br>

<img width="352" alt="스크린샷 2025-04-24 오후 3 57 08" src="https://github.com/user-attachments/assets/2c02d24b-3ce6-4b78-b8df-2f864fc8b2e1" />

<br>

> ③ **proof 1.** mathematical induction

>> **○ proof** 

>> Assuming that f is a convex function, the following inductive hypothesis holds.

<br>

<img width="723" alt="스크린샷 2025-04-24 오후 3 56 23" src="https://github.com/user-attachments/assets/105d3944-96cc-43a4-9888-13ad57bf113a" />

<br>

>> Similarly, in the case where f is a concave function, the inductive hypothesis can also be shown to hold. The bivariate case has already been proven above.

> ④ **proof 2.** introduction of a linear function.

>> **○ proof** 

>> let's define ℓ(x) as a straight-line function that passes (E(X), f(E(X)), and tangles f(x). let's say f is a convex function. then,

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbU9XXO%2FbtrhUltjMKv%2F4uihxtYSqP63mKPg4tSmoK%2Fimg.png" alt="drawing"/>

<br>

⑷ economic statistical meaning.

> ① risk-loving

>> ○ convex function

>> ○ if the utility function for the mean is smaller

>> ○ if you prefer adventure

<br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fcc45NZ%2FbtrhSMewXg8%2FmGKV4sbFeAThK2hRbCi2RK%2Fimg.png" alt="drawing"/>
</center>
  <br>

> ② risk-avoiding 

>> ○ concave function

>> ○ if the utility function for the mean is greater

>> ○ if you prefer predictable results

<br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FH0kY2%2FbtrhVZ32OF2%2FZgNFksXJZtyWxwEBWn1AG0%2Fimg.png" alt="drawing"/>
</center>
  <br>

> ③ risk-neutral

<br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FH0kY2%2FbtrhVZ32OF2%2FZgNFksXJZtyWxwEBWn1AG0%2Fimg.png" alt="drawing"/>
</center>
  <br>

<br>

## **6. law of large numbers** (LLN)

⑴ theorem

> for any real number α > 0, the following equation is established:

<br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbMr3MY%2FbtrhVrGvRYI%2FIEHXnlbjEi7rkjkuXZAGhK%2Fimg.png" alt="drawing"/>
</center>
  <br>

⑵ proof 

> suppose the mean of the random variable X is m, and the standard deviation of it is σ. for any real number k > 0, let's apply Chebyshev inequality 

<br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FPhL90%2FbtrhVtdfSCQ%2Fop0SXmvCAuA40cIRgW1WWK%2Fimg.png" alt="drawing"/>
</center>
  <br>

⑶ sufficient condition

> ① the random variable Xi, i = 1, ···, n is i.i.d

> ② VAR(X<sub>i</sub>) ＜ ∞ 

⑷ Application

> ① Weak Law of Large Numbers (Weak LLN, WLLN)

>> ○ Convergence in probability

>> ○ Guarantees that the probability of the sample mean approaching the expected value converges to 1.  

>> ○ In other words, it only ensures that the likelihood of the sample mean getting close to the expected value increases in a probabilistic sense.  

> ② Strong Law of Large Numbers (Strong LLN, SLLN)

>> ○ Convergence almost surely: If this holds, convergence in probability naturally follows.  

>> ○ Guarantees that in almost all cases, the sample mean will certainly converge to the expected value.  

>> ○ In other words, even when looking at individual realizations, it ensures that the mean ultimately converges to the expected value.

<br>

<br>

## **7. central limit theorem** (CLT)

⑴ outline

> ① definition: when the number of samples is infinitely large, the **sample sum** and **sample mean** are normally distributed regardless of the distribution of the samples

> ② mathematical expression**:** uses convergence in distribution

<br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fytbms%2FbtrhTtL2RNW%2FSxMGkbrixEJlfQQcN0YPs0%2Fimg.png" alt="drawing"/>
</center>
  <br>

⑵ **Proof 1**.

> if the probability of an event occurring is p, the probability of an event occurring k times during n trials is as follows:

<br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FduNc8k%2FbtrhVZiGrr5%2FL4HGWFodZHWBX5jmIhZY01%2Fimg.png" alt="drawing"/>
</center>
  <br>

> as n approaches infinity, this probability distribution changes almost continuously. therefore, we can think of the following function, which takes the real value as the domain of x

<br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FDW9Nx%2FbtrhTujTz5R%2FVkKUdpSPKUkHeathKhCZA0%2Fimg.png" alt="drawing"/>
</center>
  <br>

> ① average

>> if f has the maximum value at the point of x = m in the distribution, the following equation is established:

<br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdiLLDa%2FbtrhUXZWard%2FeCBlgY3aUYNaS6GoPCpA80%2Fimg.png" alt="drawing"/>
</center>
  <br>

>> pay attention to the next

<br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FFb7aQ%2FbtrhTgMWmrm%2FSTuzjRrvEGId7ax9TAl460%2Fimg.png" alt="drawing"/>
</center>
  <br>

>> however, the Sterling approximation can strictly prove that the definition of n! is as follows

<br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbHRqPf%2FbtrhWIViv8I%2FfpRVQIC3KzOsQ73jMh2qw0%2Fimg.png" alt="drawing"/>
</center>
  <br>

>> therefore

<br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcXLAku%2FbtrhThyg1Hy%2FSDwESNQqLoQgJ5rKmPBWIK%2Fimg.png" alt="drawing"/>
</center>
  <br>

> ② normal distribution approximation of the binomial distribution

>> for Δx = x - m that is small enough, the following holds true

<br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbiTyVI%2FbtrhVZJLbhC%2Ffi0U3G6oW17KWuL30zocjK%2Fimg.png" alt="drawing"/>
</center>
  <br>

>> as a result, B(n, p) is close to N(np, npq) around the average (assuming, n ≫ 1)

⑶ **proof 2.** introduction of m(t) and moment generating function 

<br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FyZBxh%2FbtrhVZQwXiN%2FqaFf5d2XJLyWzNulVwisL0%2Fimg.png" alt="drawing"/>
</center>
  <br>

⑷ sufficient condition 

> ① X<sub>i</sub>, i = 1, ···, n is i.i.d

> ② 0 ＜ VAR(X<sub>i</sub>) ＜ ∞

⑸ Application

> ① Conditions for Normal Approximation of [Binomial Distribution](https://jb243.github.io/pages/1626#3-binomial-distribution): Variance = npq ≥ 10

> ② Conditions for Normal Approximation of [Poisson Distribution](https://jb243.github.io/pages/1626#9-poisson-distribution): λ ≥ 30

⑹ [Example problems for central limit theorem](https://blog.kakaocdn.net/dn/ItSHW/btsLIzhUa1e/ZNHcNFvoZI6kgvAkW87ON0/%E1%84%8C%E1%85%AE%E1%86%BC%E1%84%89%E1%85%B5%E1%86%B7%E1%84%80%E1%85%B3%E1%86%A8%E1%84%92%E1%85%A1%E1%86%AB%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2020%E1%84%8C%E1%85%A6.pdf?attach=1&knm=tfile.pdf) 

<br>

<br>

## **8. Slutsky's theorem** 

⑴ convergence in probability**:** for random variables S1, S2, ···, Sn, 

<br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbVVMJM%2FbtrhTS5TNMd%2FnIe09c8gKXpdykylJZsAv1%2Fimg.png" alt="drawing"/>
</center>
  <br>

⑵ convergence in distribution**:** for random variables S1, S2, ···, Sn, for each distribution function F1, F2, ···, Fn, and for S that takes F as its distribution function,

<br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbAEvSB%2FbtrhTSLBZKx%2FoaQqKl7qp4dfUJQOiVNblK%2Fimg.png" alt="drawing"/>
</center>
  <br>


⑶ Slutsky's theorem (algebra of probability limit)**:** when there exist p lim Xn, p lim Yn, 

<br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FCzcPd%2FbtrhTt6mNM5%2FzDFiC5cNBTSAAryNIvCIck%2Fimg.png" alt="drawing"/>
</center>
  <br>

> ① the above 6-th formula is called CMT (continuous mapping theorem)

<br>

<br>

## **9. Laplace's rule of succession**

⑴ definition**:** when k positive results are obtained during n trials, the probability of the positive result in the n+1-th trial is as follows

<br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FclaB1y%2FbtrtLVpjvg4%2FWFeDwl0wCPb2f6Jlg79umk%2Fimg.png" alt="drawing"/>
</center>
  <br>

⑵ proof ([ref](https://jonathanweisberg.org/pdf/inductive-logic-2.pdf)https://jonathanweisberg.org/pdf/inductive-logic-2.pdf)

---

<br>

*Input : 2019.05.03 15:02*
