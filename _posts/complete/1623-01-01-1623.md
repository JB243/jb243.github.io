## **Chapter 3. Probability Space**

Higher category: 【Statistics】 [Statistics Overview](https://jb243.github.io/pages/1641)

---

**1.** [set theory](#1-set-theory)  

**2.** [conditional probability](#2-conditional-probability) 

---

**a.** [Inclusion-Exclusion Principle](https://jb243.github.io/pages/1640) 

**b.** [Monty Hall Problem](https://jb243.github.io/pages/1649) 

---

<br>

## **1. Set theory** 

⑴ case(outcome)**:** can be defined arbitrarily. marked as ω<sub>i</sub>

⑵ sample space**:** set Ω that satisfies the following conditions

> ① Ω = { ω<sub>1</sub>, ω<sub>2</sub>, ···, ω<sub>n</sub> }

> ② ω<sub>i</sub> and ω<sub>j</sub> are disjoint

> ③ Ω includes all possible results

> ④ ((1, 2, 3), (3, 4, 5, 6)) (×)

> ⑤ ((1, 2), (3, 4, 5, 6)) (○)

⑶ field**:** a set F of all subsets of Ω

> ① assumption**:** Ω = {1, 2, 3} 

> ② F = {∅, {1}, {2}, {3}, {1, 2}, {1, 3}, {2, 3}, {1, 2, 3}}

⑷ events: each element of F, _i.e._ **subset** of sample space 

> ① total event: Ω 

> ② null event: ∅

> ③ complementary event: A<sup>c</sup> = Ω - A 

> ④ union event**:** A ∪ B

> ⑤ intersection event: A ∩ B

> ⑥ mutually exclusive events: relationship between events A and B that satisfy A ∩ B = ∅

> ⑦ when number of cases is n, the number of events is 2<sup>n</sup> 

> ⑧ De Morgan's law: (E ∪ F)<sup>c</sup> = E<sup>c</sup> ∩ F<sup>c</sup>, (E ∩ F)<sup>c</sup> = E<sup>c</sup> ∪ F<sup>c</sup> 

⑸ probability

> ① intuitive definition**:** probability of a particular event = number of a particular event ÷ number of all cases

> ② axiomatic approach**:** proposed by Kolmogorov, the father of statistics

>> ○ probability is a kind of function: F → ℝ

>> ○ **axiom 1**. P(E) ≥ 0

>> ○ **axiom 2**. P(Ω) = 1

>> ○ **axiom 3.** addition rule of probability**:** about disjoint E<sub>i</sub> and E<sub>j</sub>

<br>

![image](https://github.com/user-attachments/assets/a5ab2745-0c1e-4bf6-8df1-407524c3e93b)

<br>

>> ○ **Theorem 1.** P(∅) = 0 

>>> ○ P(U) = P(U) + P(∅) ⇔ P(∅) = 0

>>> ○ note that even though E ≠ ∅, P(E) can be 0

>> ○ **Theorem 2.** P(E<sup>c</sup>) = 1 - P(E) 

>>> ○ 1 = P(E ∪ E<sup>c</sup>) = P(E) + P(E<sup>c</sup>)

>> ○ **Theorem 3.** E ⊂ F → P(E) ≤ P(F) 

>>> ○ F = E ∪ (E<sup>c</sup> ∩ F) → P(F) = P(E) + P(E<sup>c</sup> ∩ F) ≥ P(E)

>> ○ **Theorem 4.** P(E ∪ F) = P(E) + P(F) - P(E ∩ F)

>>> ○ E ∪ F = (E<sup>c</sup> ∩ F) ∪ (E ∩ F) ∪ (E ∩ F<sup>c</sup>)

>>> ○ P(E ∪ F) = (P(F) - P(E ∩ F)) + P(E ∩ F) + (P(E) - P(E ∩ F)) 

>> ○ **Theorem 5.** De Morgan's law

>>> ○ (A ∪ B)<sup>c</sup> = ((A<sup>c</sup> ∩ B) ∪ (A ∩ B<sup>c</sup>) ∪ (A ∩ B))<sup>c</sup> = (U - A<sup>c</sup> ∩ B<sup>c</sup>)<sup>c</sup> = A<sup>c</sup> ∩ B<sup>c</sup> 

>>> ○ Let C = A<sup>c</sup>, D = B<sup>c</sup>, then C ∩ D = (C<sup>c</sup> ∪ D<sup>c</sup>)<sup>c</sup> ⇔ (C ∩ D)<sup>c</sup> = C<sup>c</sup> ∪ D<sup>c</sup>

> ③ Cromwell's rule: it states that we should not use probabilities of 1 or 0, except for logical statements 

⑹ [inclusion-exclusion principle](https://jb243.github.io/pages/1640) 

<br>

![image](https://github.com/user-attachments/assets/05b1b3fb-f8c5-48ac-bcf0-4384ccde1080)

<br>

> ① example: P(A ∪ B ∪ C) = P(A) + P(B) + P(C) - P(A ∩ B) - P(B ∩ C) - P(C ∩ A) + P(A ∩ B ∩ C)  

⑺ [σ-algebra](https://jb243.github.io/pages/895) 

⑻ [Example problems for set theory](https://blog.kakaocdn.net/dn/caHvls/btsLz4i1cEu/P7K84APjyv0FXN3HHcHakk/%E1%84%8C%E1%85%B5%E1%86%B8%E1%84%92%E1%85%A1%E1%86%B8%E1%84%85%E1%85%A9%E1%86%AB%2035%E1%84%8C%E1%85%A6.pdf?attach=1&knm=tfile.pdf) 

<br>

<br>

## **2. Conditional** **probability**  

⑴ conditional probability

> ① <span>P(A | B)**:** the probability that event A will occur when event B already occurs</span>

> ② <span>expression <b>:</b> P(A | B) = P(A ∩ B) ÷ P(B)</span>

> ③ <span>P(A | B) ≠ P(B | A) </span>

> ④ <span>multiplication rule of probability**:** P(A ∩ B) = P(A) × P(B | A) = P(B) × P(A | B)</span>

> ⑤ Multiplication theorem of probabilities with more than one premise

>> ○ The following equation is self-evident because there are implicitly certain prerequisites (e.g., α, β) for any probability

<br>

![image](https://github.com/user-attachments/assets/f0e24c18-e93f-42ec-bce8-004dbe260c51)

<br>

> ⑥ [Example problems for conditioinal probability](https://blog.kakaocdn.net/dn/OrMdP/btsLAZhv14L/yJiFUkEojXM2kPRoi5vZ20/%E1%84%8C%E1%85%A9%E1%84%80%E1%85%A5%E1%86%AB%E1%84%87%E1%85%AE%20%E1%84%92%E1%85%AA%E1%86%A8%E1%84%85%E1%85%B2%E1%86%AF%2036%E1%84%8C%E1%85%A6.pdf?attach=1&knm=tfile.pdf) 

⑵ chain rule of conditional probability

<br>

![image](https://github.com/user-attachments/assets/b499e284-e3b2-4b74-8c81-71707d9214f8)

<br>

> ① <span>example <b>:</b> P(A ∩ B ∩ C) = P(A) × P(B | A) × P(C | A ∩ B)  </span>

⑶ independence of events**:** if the conditional probability is equal to the intrinsic probability

> ① Overview

>> ○ Definition: P(A ∩ B) = P(A) × P(B)

>> ○ A ⊥⊥ B represents that A and B are independent 

> ② application

>> ○ <span>P(A | B) = P(A ∩ B) ÷ P(B) = P(A)P(A B) = P(A ∩ B) ÷ P(B) = P(A)</span>

>> ○ <span>P(B | A) = P(A ∩ B) ÷ P(A) = P(B)</span>

>> ○ E and F are independent ⇔ E<sup>c</sup> and F are independent ⇔ E and F<sup>c</sup> are independent ⇔ E<sup>c</sup> and F<sup>c</sup> are independent

>> ○ if P(A) = 0, A and B are always independent

>> ○ P(A), P(B) > 0, and A and B are independent → A and B are not disjoint

>> ○ if A and B are disjoint, P(A) = 0, P(B) = 0, or A and B are not independent (contrapositive proposition)

>> ○ Even if A, B, and G are mutually independent, the intersection A and B, _i.e._ A ∩ B, and G may not be independent. For example, let A be the event that the first coin shows heads, B be the event that the second coin shows heads, and G be the event that both coin tosses show the same face.

> ③ intuitive meaning

>> ○ information about one variable does not tell you any information about the other

> ④ relationship with conditional probabilities

>> ○ <span>Pr(X = x | Y = y) = Pr(X = x)</span>

⑷ mutual independence

> ① **condition 1.** 

<br>

![image](https://github.com/user-attachments/assets/10eadf07-feb1-43b8-80f4-236031a0ae54)

<br>

> ② **condition 2.** includes **condition 1**  

<br>

![image](https://github.com/user-attachments/assets/36fd0360-d2cb-4bb5-a3ad-47cd259c914e)

<br>

> ③ Comparison with pairwise independence 

>> ○ Definition: P(A<sub>m</sub> ∩ A<sub>k</sub>) = P(A<sub>m</sub>) × P(A<sub>k</sub>), ∀ m, k 

>> ○ mutual independence ⊆ pairwise independence

>> ○ Generally, independence between many events means mutual independence

⑸ conditional independence

> ① definition

<br>

![image](https://github.com/user-attachments/assets/51e03780-17a3-4de0-85c5-8ed323d9729d)

<br>

> ② conditional independence is not always independent

> ③ independence is not always conditional independence

⑹ law of total probability

> ① partition

>> ○ **definition 1.** (exhaustive) Ω = B<sub>1</sub> ∪ B<sub>2</sub> ∪ ··· ∪ B<sub>n</sub>

>> ○ **definition 2.** (disjoint) B<sub>i</sub> ∩ B<sub>j</sub> = ∅

> ② conclusion

<br>

![image](https://github.com/user-attachments/assets/9d3be1d9-1b0b-4e08-8add-db453df99c8f)

<br>

>> ○ tip**:** drawing a Ben diagram is easy to understand

> ③ **important****:** used to induce Bayes' theorem

⑺ Bayes' theroem

> ① **a****ssumption 1.** S = B<sub>1</sub> ∪ B<sub>2</sub> ∪ ··· ∪ B<sub>n</sub>

> ② **assumption 2.** ∀ i, j, B<sub>i</sub> ∩ B<sub>j</sub> = ∅

> ③ conclusion

<br>

![image](https://github.com/user-attachments/assets/ce8d03da-a9e3-45b3-9dbb-d6df23dd1f5a)

<br>

> ④ <span><b>Importance 1.</b> a totally unrelated P (A | B) may be used to calculate P (B | A)</span>

> ⑤ **Importance 2.** results (posterior, a posteriori) can predict causes (prior, a priori)

>> ○ A: events observed as a result 

>> ○ B**:** a causal event

>> ○ P(B<sub>i</sub>): **prior**

>> ○ <span>P(B<sub>i</sub> | A) <b>:</b> when A is observed, the probability that B<sub>i</sub> is the cause (<b>posterior</b>) </span>

>> ○ <span>P(A | B<sub>i</sub>) <b>:</b> when B<sub>i</sub> is the cause, the probability of A occurring (<b>likelihood</b>)</span>

> ⑥ [Bayes Statistic](https://jb243.github.io/pages/1192)

>> ○ infers probabilities without identifying the population

>> ○ claims that probability is nothing but human belief

> ⑦ [Example problems for Bayes' theorem](https://blog.kakaocdn.net/dn/LqX2I/btsLBVE3YvO/hsaM4aoMLoLoynYa0Mdbq0/Bayes%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2029%E1%84%8C%E1%85%A6.pdf?attach=1&knm=tfile.pdf) 

⑻ **example** **1.** diagnostic rate

> ① percentage of cancer patients

>> P(A) = 1%

> ② positive probability (hit rate) at diagnosis

>> P(B)

> ③ positive probability when diagnosing cancer patients

>> P(B | A) = 95%

> ④ probability of false positive when diagnosing non-cancer patients

>> P(B | A<sup>c</sup>) = 5%

> ⑤ **question :** probability of a positive person really getting cancer during diagnosis

>> P(A | B) 

> ⑥ proportion of positive in diagnosis**:** the law of total probability is used

>> P(B) = P(B | A) + P(B | A<sup>c</sup>) = 0.95 × 0.01 + 0.05 × 0.99 

> ⑦ probability that any person is a cancer patient and is positive in diagnosis

>> P(A ∩ B) = P(B | A) × P(A) = 0.95 × 0.01 

> ⑧ answer

>> P(A | B) = P(A ∩ B) ÷ P(B) = **0.16**

> ⑨ **interpretation :** since there are many people who are not cancer patients, the false positive rate is quite high

⑼ **Example 2.** Monty hall problem

> ① situation

>> ○ supercar stands behind one of the three doors

>> ○ show participant choose one of the three doors 

>> ○ the show host opens any of the doors that the show participant did not open

>> ○ show participant can stick to or change their existing selections

>> ○ question**:** which choice is reasonable?

> ② premise

>> ○ show participant first select door 1

>> ○ show host opens door 3

> ③ definition

>> ○ P(i): probability of supercar behind door 1

>> ○ P(ii): probability of supercar behind door 2

>> ○ P(iii): probability of supercar behind door 3

>> ○ P(Ⅰ): probability of the show host choosing door 1

>> ○ P(Ⅱ): probability of the show host choosing door 2

>> ○ P(Ⅲ)**:** probability of the show host choosing door 3

>> ○ P(i), P(ii), and P(iii) are priors (causes)

>> ○ P(Ⅰ), P(Ⅱ), and P(Ⅲ) are posteriors (results)

> ④ conditional probability

>> ○ <span>P(ⅰ | Ⅲ)</span>: when the show host selects door 3, the probability that there is a supercar behind door 1

>> ○ <span>P(Ⅲ | ⅰ)</span>: if there's a supercar behind door 1, the probability that the show host will choose door 3

> ⑤ Bayes' theorem

>> ○ <span>P(ⅰ | Ⅲ)</span>

<br>

![image](https://github.com/user-attachments/assets/ab6ef445-5137-4c72-be9d-727de4d72c45)

<br>

>> ○ <span>P(ⅱ | Ⅲ)</span>

<br>

![image](https://github.com/user-attachments/assets/1d395301-9853-4d5e-9d76-5946a45fe1fb)

<br>

>> ○ <span>P(ⅲ | Ⅲ)</span>

<br>

![image](https://github.com/user-attachments/assets/d92af040-ea05-44ad-bfc0-94cf3e75f688)

<br>

> ⑥ conclusion**:** it's advantageous for show participant to change their choices

<br>

---

*Input: 2019.06.16 22:02*
