## **Chapter 5. Statistical Quantity**

Higher category: 【Statistics】 [Statistics Overview](https://jb243.github.io/pages/1641)

---

**1.** [Expected value](#1-expected-value) 

**2.** [Standard deviation](#2-standard-deviation)  

**3.** [Covariance and correlation coefficient](#3-covariance-and-correlation-coefficient)  

**4.** [Anscombe's quartet](#4-anscombe-s-quartet)  

**5.** [Ordinal statistics](#5-ordinal-statistics)

**6.** [Conditional statistics](#6-conditional-statistics)

---

**a.** [SSIM](https://jb243.github.io/pages/2067) 

**b.** [Distance Function and Similarity](https://jb243.github.io/pages/879) 

---

<br>

## **1. Expected value**

⑴ definition: the expected value of the random variable X, _i.e._ E(X), is the X value obtained on average as a result of the implementation

> ① discrete random variable

<br>

![image](https://github.com/user-attachments/assets/8a9aeacd-2723-4650-a0c8-d4782ab5482b)

<br>

> ② continuous random variable

<br>

![image](https://github.com/user-attachments/assets/412dbc29-a960-441a-a1d5-26e6dc981a94)

<br>
  
⑵ joint probability distribution function

> ① discrete random variable

<br>

![image](https://github.com/user-attachments/assets/887939a9-fffb-4b2a-9135-63e9e95e7a92)

<br>

> ② continuous probability variable

<br>

![image](https://github.com/user-attachments/assets/9ee82efc-41d9-4ef4-8711-92dbd4779f43)

![image](https://github.com/user-attachments/assets/df1da130-5fbc-4f33-b370-09ca129def63)

<br>

⑶ Properties of expected values

> ① Linearity: E(aX + bY + c) = aE(X) + bE(Y) + c

> ② If X and Y are **independent**, E(XY) = E(X) × E(Y)

<br>

![image](https://github.com/user-attachments/assets/858f1d67-1b72-476f-b4d5-bf71775bd433)

<br>

⑷ example

> ① X**:** if you mix n hats and extract one without-replacement, the number of people who correctly found their hats

> ② problem purpose**:** it is difficult to calculate E(X) after obtaining p(X)

> ③ X = X<sub>1</sub> + ··· + X<sub>n</sub>.  X<sub>i</sub>: if i-th person found his hat, the value is 1, if not 0

> ④ **Approach 1.** number of cases

<br>

![image](https://github.com/user-attachments/assets/60e82602-5dd9-444c-87a0-458b4cf8fc51)

<br>

> ⑤ **Approach 2.** when the i-th person first extracts or not, the expected value is consistent based on symmetry.

<br>

![image](https://github.com/user-attachments/assets/34717783-e073-4117-b422-82b19a2fa88e)

<br>

⑸ Cauchy distribution**:** the expected value is not defined

<br>

![image](https://github.com/user-attachments/assets/3f1b663c-f384-4502-ab62-eaeb4311f056)

<br>

⑹ [Example problems for expected value](https://blog.kakaocdn.net/dn/lMmnD/btsLCogwUDR/7au6mEcSxCVAA57mmBg681/%E1%84%80%E1%85%B5%E1%84%83%E1%85%A2%E1%86%BA%E1%84%80%E1%85%A1%E1%86%B9%2022%E1%84%8C%E1%85%A6.pdf?attach=1&knm=tfile.pdf) 

<br>

<br>

## **2. Standard deviation**

⑴ Deviation

> ① Definition**:** D = X - E(X)

> ② **Characteristic 1.** E(D) = E(X - E(X)) = E(X) - E(X) = 0 

⑵ Variance

> ① Definition: When E(X) = μ, VAR(X) = E((X - μ)<sup>2</sup>) = E(D<sup>2</sup>)

> ② **Characteristic 1.** VAR(X) = E(X<sup>2</sup>) - μ<sup>2</sup>

>> ○ Proof: VAR(X) = E((X - μ)<sup>2</sup>) = E(X<sup>2</sup>) - 2μE(X) + μ<sup>2</sup> = E(X<sup>2</sup>) - 2μ<sup>2</sup> + μ<sup>2</sup> = E(X<sup>2</sup>) - μ<sup>2</sup>

> ③ **Characteristic 2.** VAR(aX + b) = a<sup>2</sup> VAR(X)

<br>

![image](https://github.com/user-attachments/assets/b5103617-992f-4fe5-9d2d-0ddd461f5fe9)

<br>

> ④ **Characteristic 3.** introduction to covariance**:** VAR(X + Y) = VAR(X) + VAR(Y) + 2 COV(X, Y)

>> ○ Created by R.A. Fisher in 1936.

>> ○ Proof

<br>

![image](https://github.com/user-attachments/assets/4c0ee766-300a-46d7-9895-38c856513ffa)

<br>

>> ○ Generalization

<br>

![image](https://github.com/user-attachments/assets/8a0581e7-840c-47fa-af27-ef581e7a260a)

<br>

>> ○ Linearity: When X and Y are **independent**, VAR(X + Y) = VAR(X) + VAR(Y)

>> ○ Definition of covariance: Given a data set of non-overlapping (x<sub>1</sub>, y<sub>1</sub>), ···, (x<sub>n</sub>, y<sub>n</sub>), the covariance of x and y is given as follows

<br>

![image](https://github.com/user-attachments/assets/c8395c88-9ac2-484f-b408-069aee46b5ce)

<br>

>> ○ If redundancy is allowed, the definition of covariance is modified as follows by introducing the sample ratio p<sub>i</sub>: if y<sub>i</sub> = x<sub>i</sub>, then covariance = variance

<br>

![image](https://github.com/user-attachments/assets/9732f4d0-ae6e-4c0c-ad5c-c82b61913aec)

<br>

>> ○ Two-dimensional covariance matrix Σ (where x = (x<sub>1</sub>, x<sub>2</sub>)<sup>T</sup> = (x, y)<sup>T</sup>)

<br>

![image](https://github.com/user-attachments/assets/974a653c-7b0c-48f1-8cf9-062e95bb8927)

<br>

>> ○ Σ = E[(**x**-E[**x**])(**x**-E[**x**])<sup>T</sup>] is established not only for two dimensions but also for n dimensions.

> ⑤ **Characteristic 4.** VAR(X) = 0 ⇔ P(X = constant) = 1 (**∵** [Chebyshev inequality](https://jb243.github.io/pages/1594))

<br>

![image](https://github.com/user-attachments/assets/d1d615ae-40ee-4cb1-9449-f865ecc1b404)

<br>

> ⑥ [Example problems for variance](https://blog.kakaocdn.net/dn/brtRGb/btsLEim6hJJ/8SBe4mLbvvIOeO8Kdd2Pxk/%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A1%E1%86%AB%20%E1%84%86%E1%85%B5%E1%86%BE%20%E1%84%8C%E1%85%A5%E1%86%A8%E1%84%85%E1%85%B2%E1%86%AF%2022%E1%84%8C%E1%85%A6.pdf?attach=1&knm=tfile.pdf) 

⑶ Standard deviation

> ① Definition: standard deviation of X, _i.e._ σ or SD(X) = √ VAR(X) ⇔ σ<sup>2</sup> = VAR(X) 

> ② Idea: X and variance differ in unit, but X and standard deviation are same in unit 

> ③ Characteristic: variance and σ are always non-negative. covariance can have negative value 

⑷ Coefficient of variation (CV)

> ① Standard deviation divided by mean

> ② Used to relatively compare the degree of scattering of data with different units of measurement

⑸ MAD(mean absolute deviation)

> ① Regarding mean or median x̄,

<br>

<img width="148" alt="스크린샷 2025-03-28 오후 4 46 36" src="https://github.com/user-attachments/assets/04d6a452-f903-47ed-bb5b-d607162d17a4" />

<br>

<br>

## **3. Covariance and correlation coefficient** 

⑴ Covariance 

> ① definition**:** about E(X) = μ<sub>x</sub> , E(Y) = μ<sub>y</sub>, 

>> ○ COV(X, Y) = σ<sub>xy</sub> = E｛(X - μ<sub>x</sub>)(Y - μ<sub>y</sub>)｝

> ② meaning: when X changes, the degree of change of Y

> ③ **characteristic 1.** COV(X, Y) = E(XY) - E(X)E(Y)

>> ○ proof: COV(X, Y) = E((X - μ<sub>x</sub>)(Y - μ<sub>y</sub>)) = E(XY) - μ<sub>x</sub>E(Y) - μ<sub>y</sub>E(X) + μ<sub>x</sub>μ<sub>y</sub> = E(XY) - μ<sub>x</sub>μ<sub>y</sub>

> ④ **characteristic 2.** if X = Y, COV(X, Y) = VAR(X)

> ⑤ **characteristic 3.** if X and Y are independent, COV(X, Y) = 0

>> ○ proof**:** COV(X, Y) = E(XY) - E(X)E(Y) = E(X)E(Y) - E(X)E(Y) = 0

>> ○ because independence is a more stringent condition, even if COV (X, Y) = 0, it is not possible to conclude that X and Y are independent

> ⑥ **characteristic 4.** COV(aX + b, cY + d) = ac COV(X, Y)

> ⑦ **characteristic 5.** COV(a<sub>1</sub> X<sub>1</sub> + a<sub>2</sub> X<sub>2</sub>, Y) = a<sub>1</sub> COV(X<sub>1</sub>, Y) + a<sub>2</sub> COV(X<sub>2</sub>, Y)

> ⑧ Limitation: by characteristic 4, covariance contains both association and size information, so you cannot say only association 

> ⑨ [Example problems for covariance](https://blog.kakaocdn.net/dn/dQV20M/btsLDXqrTLI/qv0f2xUV7bnhVZsaSa9z7K/%E1%84%80%E1%85%A9%E1%86%BC%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A1%E1%86%AB%2019%E1%84%8C%E1%85%A6.pdf?attach=1&knm=tfile.pdf) 

> ⑩ [Example problems for advanced covariance](https://blog.kakaocdn.net/dn/bK76n6/btsLKqSX5HA/vRvMpff8CGHcDMsNpFxSi1/%E1%84%80%E1%85%A9%E1%86%BC%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A1%E1%86%AB%20%E1%84%8B%E1%85%B3%E1%86%BC%E1%84%8B%E1%85%AD%E1%86%BC%209%E1%84%8C%E1%85%A6.pdf?attach=1&knm=tfile.pdf) 

⑵ correlation coefficient**:** also referred to as Pearson correlation coefficient

> ① definition**:** about standard deviation X and Y, _i.e._ σ<sub>x</sub>, σ<sub>y</sub> each, 

<br>

![image](https://github.com/user-attachments/assets/628e1b38-d172-420c-a7c0-d92d37f34042)

<br>

>> ○ Multiple correlation coefficients: the representation of correlation coefficients when there are three or more variables

>> ○ Complete correlation: ρ = 1

>> ○ No correlation: ρ = 0

> ② Background: to show only association information except size information. related to the limitation of covariance 

> ③ Characteristics

>> ○ Correlation between two variables measured on an interval or ratio scale.
   
>> ○ Targeted towards continuous variables.
   
>> ○ Assumption of normality.
   
>> ○ Widely utilized in most cases.
     
> ④ **characteristic 1.** -1 ≤ ρ(X, Y) ≤ 1 (correlation inequality)

>> ○ proof: [Cauchy-Schwarz inequality](https://jb243.github.io/pages/1594) 

<br>

<img width="455" alt="스크린샷 2025-03-31 오후 10 40 04" src="https://github.com/user-attachments/assets/3c6cf8d4-638e-4582-9eac-5cc0906d8b97" />

<br>

>> ○ ρ(X, Y) = 1**:** X and Y are fully proportional

>> ○ ρ(X, Y) = -1**:** complete inverse relationship of X and Y

>> ○ ρ(X, Y) = 0 does not mean X and Y are independent

>> ○ **Exception** **1.** p(x) = ⅓ _I_｛x = -1, 0, 1｝ , Y = X<sup>2</sup>

>>> ○ COV(X, Y) = E(XY) - E(X)E(Y) = E(XY) - E(X<sup>3</sup>) = 0 

>>> ○ because p(1, 1) = ⅓, p(x = 1) = ⅓, p(y = 1) = ⅔, p(x, y) ≠ p(x) × p(y) 

>>> ○ disagreements in the definition of independence

>> ○ <span> <b>Exception 2.</b> S =｛(x, y) | -1 ≤ x ≤ 1, x<sup>2</sup> ≤ y ≤ x<sup>2</sup> + 1/10｝, p = 5 _I_ ｛(x, y) ∈ S｝ </span>

>>> ○ COV(X, Y) = E(XY) - E(X)E(Y) = E(XY) = 0  

>>> ○ in the definition of independence, constant = p(x, y) = p(x) × p(y) should be met. however, p(y) is not constant

>>> ○ disagreements in the definition of independence

> ⑤ **characteristic 2.** ρ(X, X) = 1, ρ(X, -X) = -1

> ⑥ **characteristic 3.** ρ(X, Y) = ρ(Y, X)

> ⑦ **characteristic 4.** exclusion of size information**:** ρ(aX + b, cY + d) = ρ(X, Y)

>> ○ Proof: ρ(aX + b, cY + d) = COV(aX + b, cY + d) ÷ aσx ÷ cσy = COV(X, Y) ÷ σxσy = ρ(X, Y)

> ⑧ <span><b>characteristic 5.</b> association information <b>:</b> | ρ(X, Y) | = 1 and Y = aX + b, (a ≠ 0, b constant) are necessary and sufficient condition </span>

>> ○ proof of forward direction**:** The idea of setting Z comes from [simple regression analysis](https://jb243.github.io/pages/1632)

<br>

![image](https://github.com/user-attachments/assets/d9c827f5-8c7d-42cf-83b9-f930586a6435)

<br>

>> ○ proof of reward direction 

<br>

![image](https://github.com/user-attachments/assets/bb383926-ba28-4b32-9716-6da30c4ba8f3)

<br>

> ⑨ [statistical estimation of correlation coefficient](https://jb243.github.io/pages/1630)

>> ○ null hypothesis H<sub>0</sub>: correlation coefficient = 0

>> ○ alternative hypothesis H<sub>1</sub>: correlation coefficient ≠ 0

>> ○ calculation of t statistics**:** about the correlation coefficient r obtained from the sample,

<br>

![image](https://github.com/user-attachments/assets/ab0a21fe-4fa1-4b9d-b2de-d302b3a2cbf7)

<br>

>> ○ the above statistics follow the student t distribution with a degree of freedom of n-2 (assuming the number of samples is n)

> ⑩ [calculation in R Studio](http://www.sthda.com/english/wiki/correlation-test-between-two-variables-in-r#:~:text=Compute%20correlation%20in%20R-,R%20functions,-Correlation%20coefficient%20can) 

>> ○ `cor(x, y)`

>> ○ `cor(x, y, method = "pearson")`

>> ○ `cor.test(x, y)`

>> ○ `cor.test(x, y, method = "pearson")`

⑶ Spearman correlation coefficient

> ① definition**:** about x' = rank(x) and y' = rank(x), 

<br>

![image](https://github.com/user-attachments/assets/e4772cb0-7358-493a-b742-6870f6788409)

<br>

> ② Characteristics

>> ○ A method of measuring the correlation between two variables that are in ordinal scale.
   
>> ○ A non-parametric method targeting ordinal variables.
   
>> ○ Advantageous in data with many ties (zeroes).

>> ○ Sensitive to deviations or errors within the data.
   
>> ○ Tends to yield higher values than Kendall's correlation coefficient.

> ③ [calculation in R Studio](http://www.sthda.com/english/wiki/correlation-test-between-two-variables-in-r#:~:text=Compute%20correlation%20in%20R-,R%20functions,-Correlation%20coefficient%20can) 

>> ○ `cor(x, y, method = "spearman")`

>> ○ `cor.test(x, y, method = "spearman")`

⑷ Kendall correlation coefficient

> ① definition**:** defined about concordant pair and discordant pair

> ② Characteristics

>> ○ A method of measuring the correlation between two variables that are in ordinal scale.
   
>> ○ A non-parametric method targeting ordinal variables.
   
>> ○ Advantageous in data with many ties (zeroes).
   
>> ○ Useful when the sample size is small or when there are many tied values in the data.

> ③ Procedure

>> ○ **step 1.** sort y values in ascending order for x values

>> ○ **step 2.** for each  y<sub>i</sub>, count the number of concordant pairs in which y<sub>j</sub> ＞ y<sub>i</sub> (assuming j ＞ i)

>> ○ **step 3.** for each y<sub>i</sub>, count the number of discordant pairs in which y<sub>j</sub> ＜ y<sub>i</sub> (assuming j ＞ i)

>> ○ **step 4.** define correlation coefficient as follows:

<br>

![image](https://github.com/user-attachments/assets/c2030848-efdf-4b64-857a-87cc8f7aa4a8)

<br>

>>> ○ n<sub>c</sub>: total number of concordnat pairs 

>>> ○ n<sub>d</sub>: total number of discordant pairs 

>>> ○ n: size of x and y

>> ○ Sample Kendall correlation coefficient

<br>

<img width="400" alt="스크린샷 2025-04-15 오전 1 26 32" src="https://github.com/user-attachments/assets/71f827b4-f9fa-4159-868e-1dc2de256d38" />

<br>

> ④ [calculation in R Studio](http://www.sthda.com/english/wiki/correlation-test-between-two-variables-in-r#:~:text=Compute%20correlation%20in%20R-,R%20functions,-Correlation%20coefficient%20can) 

>> ○ `cor(x, y, method = "kendall")`

>> ○ `cor.test(x, y, method = "kendall")`

⑸ Matthew correlation coefficient (MCC)

<br>

![image](https://github.com/user-attachments/assets/d139984c-add1-4b25-82dc-a898cbf49525)

<br>

⑹ χ<sup>2</sup>: A measure of the suitability of the approximation

> ① If the measurement data is x<sub>m</sub>, y<sub>m</sub>, and the approximate function is f(x)

<br>

![image](https://github.com/user-attachments/assets/c1d7dc7d-e40e-47e2-911e-eddce0474626)

<br>

> ② Calculating the infinitesimal point through the differential of χ<sup>2</sup> when obtaining an approximate function.

> ③ Use in [non-linear regression](https://jb243.github.io/pages/1633) such as quadratic approximation function

<br>

<br>

## **4. Anscombe's quartet**  

⑴ showing that the mean, standard deviation, and correlation coefficient cannot describe the shape of a given data

⑵ **example 1**

<br>

![image](https://github.com/user-attachments/assets/7b1f39e1-d3a3-41c0-92bb-e3c8bcc40ea4)

<b>Figure 1.</b> example of Anscombe's quartet

<br>

⑶ **example 2**

<br>

![image](https://github.com/user-attachments/assets/331da8e9-8b81-401c-983c-54962644f789)

<b>Figure 2.</b> 2nd example of Anscombe's quartet

<br>

<br>

## **5. Ordinal statistics**

⑴ Overview

> ① Assumption: X<sub>i</sub> and X<sub>j</sub> are independent 

> ② Definition: set Y<sub>i</sub> to be Y<sub>1</sub> ＜ ··· ＜ Y<sub>n</sub> by rearranging X<sub>1</sub>, ···, and X<sub>n</sub> 

⑵ Statistic

> ① Joint probability distribution

<br>

![image](https://github.com/user-attachments/assets/62144df4-1781-4e6a-a212-2d9cffb696b7)

<br>

> ② Marginal probability distribution

<br>

![image](https://github.com/user-attachments/assets/2f9d74b8-23bd-46d4-ae92-e2319f55b893)

<br>

> ③ Expected value

<br>

![image](https://github.com/user-attachments/assets/deb26ef5-0455-49a8-b4d8-2792247a62c3)

<br>

⑶ [Example problems for order statistics](https://blog.kakaocdn.net/dn/GvY4O/btsLKbNWEPz/qCFSVKa0N3PNR9rH8c4AD1/%E1%84%89%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%84%90%E1%85%A9%E1%86%BC%E1%84%80%E1%85%A8%E1%84%85%E1%85%A3%E1%86%BC%2018%E1%84%8C%E1%85%A6.pdf?attach=1&knm=tfile.pdf)

> ① Question Type: Questions are asked on the distribution and statistics of the maximum or minimum values out of n values, or the distribution of the k-th order statistic.

> ② **Example 1:** A random sample of size 3 is drawn from a uniform distribution on [0, 1]. Calculate the probability that the maximum value of the sample is greater than 0.7.

>> ○ Solution

>> Pr(Y > 0.7) = 1 - (Pr(X ≤ 0.7))<sup>3</sup> = 1 - 0.7<sup>3</sup> = 0.657 

> ③ **Example 2:** X follows an exponential distribution with a mean of 1. A sample of size 3 is drawn. Calculate the expected value of the median of the three values.

>> ○ Solution

>> f<sub>Y</sub>(x) = (3! / 1!1!1!)·(1 - e<sup>-x</sup>)·e<sup>-x</sup>·e<sup>-x</sup> = 6(e<sup>-2x</sup> - e<sup>-3x</sup>)

>> ∴ E[Y] = ∫<sub>0 to ∞</sub> 6x(e<sup>-2x</sup> - e<sup>-3x</sup>) dx = 5/6 

<br>

## **6. Conditional statistics**

⑴ Conditional expectation

> ① Definition

<br>

![image](https://github.com/user-attachments/assets/5bf71bc7-1584-44a2-8520-ec064b94be05)

<br>

> ② Characteristic

>> ○ <span>E(XY | Y) = YE(X | Y)</span>

>> ○ <span>E(aX<sub>1</sub> + bX<sub>2</sub> | Y) = aE(X<sub>1</sub> | Y) + b(X<sub>2</sub> | Y)</span>

<br>

![image](https://github.com/user-attachments/assets/ee4bdb04-8767-45b8-9357-9e23bbc1ff67)

<br>

> ③ Law of iterated expectation

>> ○ Lemma

<br>

![image](https://github.com/user-attachments/assets/4af327ec-963d-4f00-859c-8466649bc0c7)

<br>

>> ○ Proof

<br>

![image](https://github.com/user-attachments/assets/608fc740-a5f6-4b07-b83d-db22c0868030)

<br>

>> ○ Example

>>> when selecting a point of Y randomly at［0, ℓ］ as a uniform distribution, and then a point of X randomly at ［0, y］ as a uniform distribution,

<br>

![image](https://github.com/user-attachments/assets/40626944-1272-4492-81c1-b6dc01d36052)

<br>

> ④ Mean independence

>> ○ Independence ⊂ mean independence ⊂ uncorrelatedness

>> ○ Average independence

>> ○ Uncorrelatedness: if the correlation coefficient is 0

>> ○ Normal distribution: if X and Y are jointly normal and uncorrelated, then X and Y are independent

> ⑤ [Simple regression analysis](https://jb243.github.io/pages/1632)

<br>

![image](https://github.com/user-attachments/assets/4b24ba17-f29e-4006-852c-c89c4f3828b5)

<br>

⑵ conditional variance

> ① Definition: the conditional variance of Y for a given probability variable X

<br>

![image](https://github.com/user-attachments/assets/bb39f8d9-eddc-4265-863d-3def7f2e3832)

<br>

> ② Law of total variance (decomposition of variance)

>> ○ lemma

<br>

![image](https://github.com/user-attachments/assets/9dfdf823-02e3-4b58-8db3-7cac8aeeb6e3)

<br>

>> ○ Proof

<br>

![image](https://github.com/user-attachments/assets/5390b18b-1c36-46e0-a927-de32f02b263b)

<br>

>> ○ Meaning

>>> ○ Situation: when X ~ P<sub>1</sub>(θ), Y ~ P<sub>2</sub>(X) 

>>> ○ <span>use P<sub>2</sub> to calculate VAR(Y | X) and E(Y | X) </span>

>>> ○ use P<sub>1</sub> to calculate E｛·｝, VAR｛·｝

>>> ○ <span>E(VAR(X | Y))</span>: intra-group variance

>>> ○ <span>VAR(E(X | Y))</span>: inter-group variance

>> ○ **Example 1.**

>>> ○ X: laid-off worker's unemployment period 

>>> ○ probability density function of X: [exponential distribution](https://jb243.github.io/pages/1627)

<br>

![image](https://github.com/user-attachments/assets/acd248e6-7704-4c31-9eff-008ca7919d17)

<br>

>>> ○ 20% of the total workforce**:** skilled labor force. λ = 0.4

>>> ○ 80% of the total workforce**:** unskilled workers. λ = 0.1

>>> ○ calculation of VAR(X)

<br>

![image](https://github.com/user-attachments/assets/69cac599-7c7f-4856-a173-8b6c61c8072b)

<br>

>> ○ **Example 2.**

>>> ○ Question: Let P be the proportion of policyholders that renew their auto policies. P varies by agent. P follows a [beta distribution](https://jb243.github.io/pages/1627#5-beta-distribution) with mean 0.8 and variance 0.25. A group of 10 policyholders is selected from all policyholders of an insurance company. Let N be the number of policyholders who renew their auto policies. Calculate Var[N].

>>> ○ Solution: <span>Var[N] = E[Var[N | P]] + Var[E[N | P]] = E[10P(1-P)] + Var[10P] = 10E[P] - 10E[P<sup>2</sup>] + 100Var[P] = 24.1</span>

>>> ○ Note: The distributions of P<sub>1</sub>, P<sub>2</sub>, ···, P<sub>10</sub> are not completely independent, as they come from the same distribution. Therefore, Var[N] ≠ ∑<sub>i</sub> Var[P<sub>i</sub>].

<br>

---

*Input: 2019.06.17 14:15*
