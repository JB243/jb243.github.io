## **Chapter 6. Discrete probability distribution**

Higher category: 【Statistics】 [Statistics Overview](https://jb243.github.io/pages/1641)

---

**1.** [Uniform distribution](#1-uniform-distribution) 

**2.** [Bernoulli distribution](#2-bernoulli-distribution)  

**3.** [Binary distribution](#3-binary-distribution)  

**4.** [Multinomial distribution](#4-multinomial-distribution) 

**5.** [Hypergeometric distribution](#5-hypergeometric-distribution) 

**6.** [Geometric distribution](#6-geometric-distribution)

**7.** [Negative binomial distribution](#7-negative-binomial-distribution) 

**8.** [Negative hypergeometric distribution](#8-negative-hypergeometric-distribution) 

**9.** [Poisson distribution](#9-poisson-distribution)

---

<br>

![image](https://github.com/user-attachments/assets/76bcd835-5989-4756-8f3f-f5f730de7d2c)

<br>

## **1. Uniform distribution** 

⑴ definition**:** probability distribution with constant probabilities for all random variables

⑵ probability mass function**:** p(x) = (1 / n) _I_｛x = x<sub>1</sub>, ···, x<sub>n</sub>｝

<br>

<img width="409" alt="스크린샷 2024-12-22 오후 11 49 26" src="https://github.com/user-attachments/assets/fdcfca39-81e1-45a7-b412-196ee82944ab" />

<b>Figure 1.</b> probability mass function of uniform distribution

<br>

> ① Python programming**:** Bokeh is used for web-page visualization

<br>

```python
from bokeh.plotting import figure, output_file, show

output_file("uniform_distribution.html")
graph = figure(width = 400, height = 400, title = "Uniform Distribution", 
               tooltips=[("x", "$x"), ("y", "$y")] )
x = [1, 2, 3, 4, 5, 6, 7, 8]
top = [1/8, 1/8, 1/8, 1/8, 1/8, 1/8, 1/8, 1/8]
width = 0.5
graph.vbar(x, top = top, width = width, color = "navy", alpha = 0.5)
show(graph)
```

<br>

⑶ [Example problems for uniform distribution](https://shorturl.at/AaMqD) 

<br>

<br>

## **2. Bernoulli distribution** 

⑴ Bernoulli trials**:** implementation in which the result of implementation is successful (X = 1) or failure (X = 0)

⑵ Bernoulli distribution**:** probability distribution when Bernoulli's implementation is once

⑶ probability mass function**:** p(x) = θ _I_｛x = 1｝+ (1 - θ) _I_｛x = 0｝

<br>

<img width="409" alt="스크린샷 2024-12-22 오후 11 49 43" src="https://github.com/user-attachments/assets/5164a844-e25e-45a9-be7e-d3b05b678dc1" />

<b>Figure 2.</b> probability mass function of Bernoulli distribution at θ = 0.6

<br>

> ① Python programming**:** Bokeh is used for web-page visualization 

<br>

```python
from bokeh.plotting import figure, output_file, show

output_file("Bernoulli_distribution.html")
x = [0, 1]
top = [0.4, 0.6]
width = 0.5

graph = figure(width = 400, height = 400, title = "Bernoulli Distribution", 
               tooltips=[("x", "$x"), ("y", "$y")] )
graph.vbar(x, top = top, width = width, color = "navy", alpha = 0.5)
show(graph)
```

<br>

⑷ statistics

> ① moment generating function 

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fbw53jS%2Fbtq635IPtlS%2FNr4D6NvNyTkIINIc6h4Gwk%2Fimg.png" alt="drawing">
</center>
  <br>

> ② average**:** E(X) = θ

> ③ variances**:** VAR(X) = E(X<sup>2</sup>) - E(X)<sup>2</sup> = θ - θ<sup>2</sup> = θ (1 - θ)

<br>

<br>

## **3. Binomial distribution** 

⑴ definition **:** probability distribution of the number of successes when Bernoulli's trials are repeated **n times**

> ① the number of trials and the probability of implementation are fixed

⑵ probability mass function

> ① p(x) = <sub>n</sub>C<sub>x</sub> θ<sup>x</sup> (1 - θ)<sup>n-x</sup>

> ② p(x) **:** the probability of succeeding only x times out of n

> ③ <sub>n</sub>C<sub>x</sub> **:** the number of x-number combinations among the numbers 1, 2, · · · and n

> ④ θ<sup>x</sup> **:** the probability of success when there are the above x-number combinations

> ⑤ (1 - θ)<sup>n-x</sup> **:** the probability of fail if it's not the above x-number combinations

<br>

<img width="409" alt="스크린샷 2024-12-22 오후 11 50 34" src="https://github.com/user-attachments/assets/2e0c3752-e9ff-4f0d-86c2-24217306fd2e" />

<b>Figure 3.</b> probability mass function of binomial distribution at n = 30, p = 0.6

<br>

> ⑥ Python programming**:** Bokeh is used for web-page visualization

<br>

```python
# see https://www.geeksforgeeks.org/python-binomial-distribution/

from scipy.stats import binom
from bokeh.plotting import figure, output_file, show

output_file("binomial_distribution.html")

n = 30
p = 0.6
x = list(range(n+1))
top = [binom.pmf(r,n,p) for r in x]
width = 0.5

graph = figure(width = 400, height = 400, title = "Binomial Distribution", 
               tooltips=[("x", "$x"), ("y", "$y")] )
graph.vbar(x, top = top, width = width, color = "navy", alpha = 0.5)
show(graph)
```

<br>

⑶ statistics

> ① idea **:** since the i-th Bernoulli trial follows the Bernoulli distribution,

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FtWPOi%2Fbtq6507nxgb%2Fl4qzgMcC1F3kL6eIMeumK0%2Fimg.png" alt="drawing">
</center>
  <br>

> ② moment generating function

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FSpPMa%2Fbtq64v1kDJ6%2FkR4vK3O7QTh6jYAfhLkOR0%2Fimg.png" alt="drawing">
</center>
  <br>
  
> ③ average**:** E(X) = nθ 

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbwThYo%2Fbtq665AmGg2%2FkuJxK1oUzVeDNgW3auuet1%2Fimg.png" alt="drawing">
</center>
  <br>

> ④ variance**:** VAR(X) = nθ(1 - θ)

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbG14gk%2Fbtq64RQEcS0%2FxRBOlvcTRMYTPKkFJqAEE0%2Fimg.png" alt="drawing">
</center>
  <br>

⑷ [Example problems for binomial distribution](https://shorturl.at/uIyGm) 

<br>

<br>

## **4. Multinomial distribution** 

⑴ multinomial trials**:** extend the Bernoulli's trial by three or more in case of results

⑵ multinomial distribution**:** probability distribution when multinomial trials are repeated n times.

⑶ probability mass function

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbpBpr9%2Fbtq62VGriYB%2FKMhvTd4XNTY5AVO84slFWk%2Fimg.png" alt="drawing">
</center>
  <br>

> ① premise **:** x<sub>1</sub> + x<sub>2</sub> + ··· + x<sub>k</sub> = n

> ② p(x<sub>1</sub>, x<sub>2</sub>, ··· , x<sub>k</sub>) = <sub>n</sub>C<sub>x1</sub> × <sub>n-x1</sub>C<sub>x2</sub> × ··· × <sub>xk</sub>C<sub>xk</sub> × θ<sub>1</sub><sup>x1</sup> θ<sub>2</sub><sup>x2</sup> ··· θ<sub>k</sub><sup>xk</sup>

<br>

<br>

## **5. Hypergeometric distribution** 

⑴ definition **:** If the number of successes is M out of all N, the probability distribution of the number of successes extracted when n are extracted **without-replacement** 

⑵ probability mass function

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FUSQd5%2Fbtq66jeyQ4K%2FBK45xAJm1hxt0QCQDy8kOK%2Fimg.png" alt="drawing">
</center>
  <br>
  
<center>Or as shown in the picture below</center>

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fcc53sA%2FbtrICPBuxGg%2FMxPGzXOEIjCCBB5qgohNHK%2Fimg.jpg" alt="drawing">
</center>
  <br>

<img width="409" alt="스크린샷 2024-12-22 오후 11 51 17" src="https://github.com/user-attachments/assets/a3c7b9ad-265b-41a6-af56-7fef8f82aa78" />

<b>Figure 4.</b> probability mass function of hypergeometric distribution at [M, n, N] = [20, 7, 12] 

<br>

> ① Python programming**:** Bokeh is used for web-page visualization

<br>

```python
# see https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.hypergeom.html

from scipy.stats import hypergeom
from bokeh.plotting import figure, output_file, show

output_file("hypergeometric_distribution.html")

[M, n, N] = [20, 7, 12]
rv = hypergeom(M, n, N)
x = np.arange(0, n+1)
top = rv.pmf(x)
width = 0.5

graph = figure(width = 400, height = 400, title = "Hypergeometric Distribution", 
               tooltips=[("x", "$x"), ("y", "$y")] )
graph.vbar(x, top = top, width = width, color = "navy", alpha = 0.5)
show(graph)
```

<br>

⑶ statistics

> ① average**:** E(X) = nM / N 

>> ○ similar to the binary distribution of E(X) = nθ = nM / N

> ② variance**:** VAR(X) = ［(N-n) / (N-1)］ × ［nM / N］ × ［1 - M / N］

⑷ the relationship with the binomial distribution 

> ① the conditional distribution of binomial distribution**:** hypergeometric distribution 

> ② the limit of the hypergeometric distribution (n → ∞) **:** binominal distribution (n → ∞)

> ③ binary distribution is based on **with-replacement** 

<br>

<br>

## **6. Geometric distribution** 

⑴ definition**:** for extraction with a probability of success of θ, the probability distribution for the number of trials until successful.

> ① the probability of implementation is fixed and the number of implementations changes

⑵ probability mass function**:** p(x) = θ (1 - θ)<sup>x-1</sup> _I_｛x = 1, 2, ···｝

<br>

<img width="409" alt="스크린샷 2024-12-22 오후 11 51 47" src="https://github.com/user-attachments/assets/9674e15d-722d-42d9-beb7-e1a745256999" />

<b>Figure 5.</b> geometric distribution at θ = 0.5

<br>

> ① Python programming**:** Bokeh is used for web-page visualization

<br>

```python
# see https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.geom.html

from scipy.stats import geom
from bokeh.plotting import figure, output_file, show

output_file("geometric_distribution.html")

n = 10
p = 0.6
x = np.arange(0, n+1)
top = geom.pmf(x, p)
width = 0.5

graph = figure(width = 400, height = 400, title = "Geometric Distribution", 
               tooltips=[("x", "$x"), ("y", "$y")] )
graph.vbar(x, top = top, width = width, color = "navy", alpha = 0.5)
show(graph)
```

<br>

⑶ statistics

> ① moment generating function 

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fz10ms%2Fbtq66jr5WIw%2FxhtCYz3NJCxwUHkMfs5WH0%2Fimg.png" alt="drawing">
</center>
  <br>

> ② average**:** E(X) = 1 / θ

>> ○ meaning**:** intuitively, average number of trials × probability of success = 1 is established

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FEyGt1%2Fbtq65rxrtPE%2FRSViuInEivLYkHG5DiQVk0%2Fimg.png" alt="drawing">
</center>
  <br>

> ③ variance**:** VAR(X) = (1 - θ) / θ<sup>2</sup>

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FsQOp4%2Fbtq66UMnee4%2FcKNkRgkuCKwvL62OtWY5q1%2Fimg.png" alt="drawing">
</center>
  <br>

<br>

## **7. Negative binomial distribution**  

⑴ definition**:** if the probability of success is θ, the probability distribution for the number of trials until the r-th success is achieved

> ① in the binomial distribution, the number of trials and the probability of implementation are fixed, and the number of successes varies

> ② in the negative binomial distribution, the number of successes and probability of implementation are fixed, and the number of trials varies

⑵ probability mass function

> ① **type 1.** fixes the number of successes with r

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fclh8zP%2Fbtq67OLUCFz%2FRBL3G4aaKd42PxKEGauxaK%2Fimg.png" alt="drawing">
</center>
  <br>

>> ○ x**:** number of trials

>> ○ r**:** number of successes

>> ○ θ**:** probability of success

>> ○ <sub>x-1</sub>C<sub>r-1</sub> **:** the number of cases where the x-th is success, and only r-1 trials is successful in the previous x-1 trials.

> ② **type 2.** fixes the number of failures with r\*

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbE8y9k%2Fbtq64wMLiNi%2FNlEpbuepGRuyWKCadaDjuk%2Fimg.png" alt="drawing">
</center>
  <br>

>> ○ k**:** number of successes

>> ○ r\***:** number of failures

>> ○ p**:** probability of success

>> ○ <sub>k+r*-1</sub>C<sub>k</sub> **:** the number of cases where k+r*th is a failure, and only r*-1 fails in the previous k+r*-1 trial

> ③ graph

<br>

<img width="409" alt="스크린샷 2024-12-22 오후 11 52 16" src="https://github.com/user-attachments/assets/8f9dab15-7490-4a7a-9a5b-6ab283fae1e3" />

<b>Figure 6.</b> probability mass function of negative binomial distribution at r = 5, θ = 0.6

<br>

> ④ Python programming**:** Bokeh is used for web-page visualization

<br>

```python
# see https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.nbinom.html

from scipy.stats import nbinom
from bokeh.plotting import figure, output_file, show

output_file("negative_binomial_distribution.html")

n = 5
p = 0.6
x = np.arange(0, 13)
top = nbinom.pmf(x, n, p)
width = 0.5

graph = figure(width = 400, height = 400, title = "Negative Binomial Distribution", 
               tooltips=[("x", "$x"), ("y", "$y")] )
graph.vbar(x, top = top, width = width, color = "navy", alpha = 0.5)
show(graph)
```

<br>

⑶ statistics

> ① statistics for **type 1**

>> ○ idea**:** X = ∑X<sub>i</sub>

>> ○ X<sub>i</sub> **:** the number of trials of i-th success after the i-1 times of successes. follows **geometric** distribution

>> ○ moment generating function

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdG4B2z%2Fbtq64THIEm1%2F2Q369uYsqciAhU1ifk7At1%2Fimg.png" alt="drawing">
</center>
  <br>

>> ○ average**:** E(X) = r / θ

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbLMvnC%2Fbtq67NfaL4k%2F4beKTXU5qZv1oJEYuK7Gp0%2Fimg.png" alt="drawing">
</center>
  <br>

>> ○ variance**:** VAR(X) = r(1-θ) / θ<sup>2</sup>

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FWU4NW%2Fbtq64wFWvoO%2FR9ekKRRtGxaX4dEVFuKNi0%2Fimg.png" alt="drawing">
</center>
  <br>

> ② statistics for **type 2** 

>> ○ average**:** E(X) = r*p / (1-p)

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcTB0GA%2Fbtq636uamqW%2FPChCKBZGcYm75Kt1nt5x4k%2Fimg.png" alt="drawing">
</center>
  <br>

>> ○ variance**:** VAR(X) = r\*p / (1-p)<sup>2</sup>

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FvW12D%2Fbtq63529M2F%2FUNOnaJkDHC7EzkTRBk1T2k%2Fimg.png" alt="drawing">
</center>
  <br>

⑷ example

> ① situation**:** one of the n types of figures will be randomly provided in each game

> ② X **:** the number of games to watch until all figures are collected

> ③ question**:** E(X)

> ④ idea**:** X = X<sub>1</sub> + ··· + X<sub>n</sub>

> ⑤ X<sub>i</sub> **:** the number of games you have to watch until you collect the i-th new figure. follows **geometric** distribution 

> ⑥ E(X)

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FmmLLV%2Fbtq64SPvMvf%2F03izKBdvBVfpoxAYRWbBp0%2Fimg.png" alt="drawing">
</center>
  <br>

⑸ [Example problems for negative binomial distribution](https://shorturl.at/JdqMl) 

<br>

<br>

## **8\. Negative hypergeometric distribution** 

⑴ definition

> ① situation: out of N, the number of successes is k

> ② question**:** if you pick one success by without-replacement, the number of failures you picked until then

<br>

<br>

## **9. Poisson distribution**

⑴ definition**:** for an event that occurs λ times on average during a unit time, Poisson distribution is defined as the probability distribution of the number of times the event occurs in a unit time

> ① λ **:** parameter (∈ ℝ)

> ② for a time interval k times of the unit time, the Poisson distribution of λ\* = kλ is considered 

> ③ indeed, actively used.

⑵ probability mass function

> ① idea**:** binominal distribution and limit

> ② if the unit time is divided by n equal parts, the probability of the event occurring in each equal part is λ / n

> ③ the probability that an event will occur x times in a unit time.

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FE7OJS%2Fbtq63529Nbq%2Fu7ezJciHeF2VWe7ffSLar0%2Fimg.png" alt="drawing">
</center>
  <br>

> ④ probability mass function**:** you just have to take the limit of ③ by n → ∞

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbbqXOL%2Fbtq64gXEBVZ%2FCQU51x8MV936W4Xs5hOoY0%2Fimg.jpg" alt="drawing">
</center>
  <br>

> ⑤ graph 

<br>

<img width="409" alt="스크린샷 2024-12-22 오후 11 52 54" src="https://github.com/user-attachments/assets/92bb0cfa-794b-40ef-bbcd-f41bd2f9411f" />

<b>Figure 7.</b> probability mass function of Poisson distribution at λ = 0.6

<br>

> ⑥ Python programming**:** Bokeh is used for web-page visualization

<br>

```python
# see https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.poisson.html

from scipy.stats import poisson
from bokeh.plotting import figure, output_file, show

output_file("poisson_distribution.html")

lam = 0.6
x = np.arange(0, 4)
top = poisson.pmf(x, lam)
width = 0.5

graph = figure(width = 400, height = 400, title = "Poisson Distribution", 
               tooltips=[("x", "$x"), ("y", "$y")] )
graph.vbar(x, top = top, width = width, color = "navy", alpha = 0.5)
show(graph)
```

<br>

⑶ statistics

> ① moment generating function

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fcq4lFK%2Fbtq64g4nVos%2FrXiT83ua4JHuPn2NOdvLUK%2Fimg.png" alt="drawing">
</center>
  <br>

② average**:** E(X) = λ

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FQs90R%2Fbtq616gWBta%2Fw7SaLhuIVpWuz4kSlb6CU1%2Fimg.png" alt="drawing">
</center>
  <br>

③ variance**:** VAR(X) = λ

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbphY1b%2Fbtq66v0mbZe%2FOorlSzS4MIXRrkgSayWNbk%2Fimg.png" alt="drawing">
</center>
  <br>

⑷ characteristic

> ① the sum of independent probability variables following the Poisson distribution also follows the Poisson distribution

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FlSSOT%2Fbtq6649jyeF%2FVpU9Fjz0BS90OQ3Immosy1%2Fimg.png" alt="drawing">
</center>
  <br>

⑸ relationship with binomial distribution 

> ① conditional distribution of Poisson distribution**:** binominal distribution

> ② the limit of binomial distribution (n → ∞)**:** Poisson distribution 

⑹ example

> ① situation**:** got an average of 30 calls per hour

> ② question**:** the probability of getting 2 calls in 3 minutes.

> ③ as λ = 30, λ\* = 30 ÷ 20 = 1.5

> ④ calculation 

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fbpu7rR%2Fbtq650GjKKv%2FRSKFLcxZOiQxXPx3T5m4d1%2Fimg.png" alt="drawing">
</center>
  <br>

⑺ [Example problems for Poisson distribution](https://shorturl.at/TpmWp) 

<br>

---

_Input: 2019.06.18 23:48_
