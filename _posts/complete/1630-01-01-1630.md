## **Chapter 13. Statistical Estimation**

Higher category: 【Statistics】 [Statistics Overview](https://jb243.github.io/pages/1641)

---

**1.** [overview](#1-overview)

**2.** [point estimation](#2-point-estimation)

**3.** [interval estimation](#3-interval-estimation)

---

<br>

## **1. overview** 

⑴ statistical estimation**:** estimating the characteristics of a population through samples

⑵ state space**:** ℝn. a set of all the observed samples 

<br>

<br>

## **2. point estimation** (parametric approach, location type)

⑴ definition**:** estimating parameters from samples (x<sub>1</sub>, ···, x<sub>n</sub>)

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/oGKcy/btrt0NRKFX6/PKMex9Fhyk2Fq3t54SeGe0/img.png" alt="drawing" /></center><br>

> ① parameter: values showing the characteristics of the population. μ, σ, θ, λ, etc

> ② μ: mean of population

> ③ σ: standard deviation of population

> ④ θ: θ: probability of success in Bernoulli distribution or binomial distribution

> ⑤ λ: λ of Poisson distribution or exponential distribution

⑵ sampling distribution (empirical distribution)

⑶ point estimator**:** for parameter θ, 

> ① **definition 1.** point estimator is not a single number but a function

> ② **definition 2.** point estimator is a function for X<sub>1</sub>, ···, X<sub>n</sub> 

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/wbS2e/btrtU83KKum/sfxYcBake9Ah7fVjXcdMa1/img.png" alt="drawing" /></center><br>

> ③ **definition 3.** the probability of a point estimator is a function of θ

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/b9hv5W/btrtU9n1J0w/3pnEHP0NRitk9MxqaN8yoK/img.png" alt="drawing" /></center><br>

⑷ criteria for a good point estimator 

> ① expected error or mean squared error (MSE): also called model risk

>> ○ bias-variance decomposition 

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/bSeiUq/btrtX1iHlRG/O1YZpKS9Jd1nSTNbOY9Hz1/img.png" alt="drawing" /></center><br>

>> ○ as the covariance of bias and chance error is 0 intuitively, we can remove the intermediate term 

>> ○ the strategy to reduce the bias increases the model variance

>> ○ the strategy to reduce the model variance increases the bias

> ② **criterion 1.** bias**:** also called systemic error, non-random error, and model bias

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/cbFZTy/btrtXZ6cOQS/cn9wxEEPWA2BceRgFUdhm0/img.png" alt="drawing" /></center><br>

>> ○ small bias required

>>> ○ cause**:** **underfitting**, lack of domain knowledge

>>> ○ solutions**:** use of more complex models, use of models suitable for domain

>> ○ unbiased estimator**:** B = 0 ⇔ sample mean = population mean. if there's unbiasedness, it's a good estimator

>> ○ **example 1.** sample mean**:** unbiased estimator of population mean 

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/bKjQrS/btrtWuTvfLq/3SDKtAb20IfkBRzgYNKUMK/img.png" alt="drawing" /></center><br>

>> ○ **example 2.** [sample variance](https://jb243.github.io/pages/1595): unbiased estimator of population variance 

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/6kCgG/btrtVpEY0ep/Zmty2kcKR4KZSAeWFBEvJ1/img.png" alt="drawing" /></center><br>

>> ○ **example 3.** sample covariance

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/JlwpZ/btrtU9Bv9zv/3zLY4JLHMT56JWZL6uqRY1/img.png" alt="drawing" /></center><br>

>> ○ **example 4.** when X<sub>i</sub> ~ u[0, θ], either unbiased estimator or not 

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/7elfX/btrtOyPpBmU/1kWKH7klaemBdqAF2xKSQ1/img.png" alt="drawing" /></center><br>

> ③ **criterion 2.** efficiency**:** related to random chance error

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/edcohM/btrtRCxc62B/JF6aIGw8AgPTrnZ2uShBX1/img.png" alt="drawing" /></center><br>

>> ○ small variance is required based on the premise of unbiased estimator  

>> ○ **2-1.** noise variance**:** also called 1st chance error and observation variance. marked as σ<sup>2</sup>

>>> ○ example: error of the instrument itself, noise of the target itself

>>> ○ there are attempts to measure these information, but there are many difficulties

>> ○ **2-2.** model variance**:** also called 2nd chance error

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/k4290/btrtOyolfhO/uod2crqVmGLGIgdLZzGVe1/img.png" alt="drawing" /></center><br>

>>> ○ chance error due to the fact that the sample group is a randomly extracted set from the population

>>> ○ cause: **overfitting**

>>> ○ solution: using a simpler model

>> ○ bias-variance tradeoff: as model complexity increases, the bias decrease, but model variance increases, resulting in trade-off relationship and optimal complexity

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/8HspW/btrt1KOvJt5/Hx1y8lwKbXq767KbiFsLQ0/img.png" alt="drawing" /></center><br>

<center><b>Figure. 1.</b> bias-variance tradeoff</center>

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/Lpq5y/btrtVolQ6mg/IjsepwblXht4BvqKBmv5uK/img.png" alt="drawing" /></center><br>

<center><b>Figure. 2.</b> differences between simple and complex models</center>

<br>

>> ○ BLUE (best linear unbiased estimator): the estimator of the smallest variance among linear unbiased estimators

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/I46rb/btrt0Nj11Xf/KJWWfAjBL3eJKrm1cy1h71/img.png" alt="drawing" /></center><br>

>> ○ uniformly minimum variance unbiased estimator (UMVU)

>>> ○ definition**:** the estimator of the smallest variance among unbiased estimators including non-linear unbiased estimators 

>>> ○ the direct calculation of Fisher information <i>I</i>n 

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/bjrwxY/btrtVoF9Tg9/IKM8fwwHwBltKc4jwMKdck/img.png" alt="drawing" /></center><br>

>>> ○ the indirect calculation of Fisher information <i>I</i>n  

>>> ○ Cramér–Rao lower bound = 1 / <i>I</i>n

>>> ○ if an estimator is equal to the Cramér–Rao lower bound, it is UMVU 

>> ○ **Example 1.** Sample Mean and Sample Median

>>> ○ Asymptotic distribution of the sample mean when the population distribution is normal

<br>

<img width="259" alt="스크린샷 2025-04-18 오전 12 39 40" src="https://github.com/user-attachments/assets/b8957f7f-a53f-4670-a578-48896d9af194" />

<br>

>>> ○ Asymptotic distribution of the sample median when the population distribution is normal

<br>

<img width="300" alt="스크린샷 2025-04-18 오전 12 40 05" src="https://github.com/user-attachments/assets/a4039a9a-9f23-4b24-b14d-317174f57f31" />

<br>

>>> ○ When the population distribution is normal, the Hodges–Lehmann estimator, defined as median[(X<sub>i</sub> + X<sub>j</sub>) / 2 : i ≤ j], is more efficient than the sample mean.

>>> ○ If the random variable follows a double-exponential distribution, the sample mean has greater variance than the sample median.

>> ○ **Example 2.** Sample Standard Deviation (S<sub>n</sub>) and MAD (Median Absolute Deviation)

>>> ○ S<sub>n</sub> →<sup>d</sup> σ

>>> ○ MAD = median(|X<sub>1</sub> - μ|, ⋯, |X<sub>n</sub> - μ|) →<sup>d</sup> Φ<sup>-1</sup>(3/4) σ = 0.676 σ

> ④ **criterion 3.** consistency and consistent estimator 

>> ○ asymptotic property**:** characteristic of sample, of which size is approaching ∞

>> ○ asymptotic unbiasedness**:** the case in which the unbiasedness is established when n → ∞. it is related to the law of large numbers

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/KgxiJ/btrt0NdiVYV/hkh4EhO8eGkHtMtRCuUaWK/img.png" alt="drawing" /></center><br>

>> ○ asymptotic efficiency 

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/mt4uN/btrt1JhLyo4/uX9KiRGKWQ3BGIxk0tAK7K/img.png" alt="drawing" /></center><br>

>> ○ consistency: the property of the estimator converging into a parameter

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/0i93i/btrtX1b6rQO/pVgcgEPR4o90jF5oBdMXtk/img.png" alt="drawing" /></center><br>

>> ○ X is a random variable, but generally considered as a specific constant

>>> ○ example**:** the following is a poor random variable because it is inconsistent

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/cbJU8G/btrt0L7FH6n/1OTK6Adw8eL4YFKQCeY8m1/img.png" alt="drawing" /></center><br>

>> ○ ARE (Asymptotic Relative Efficiency): The ratio of the asymptotic variances of two random variables. A significant deviation from the theoretical value may suggest the presence of outliers.

> ⑤ **Criterion 4.** Robust Estimator: Investigates whether the estimator is more sensitive to outliers.

> ⑥ **Criterion 5.** Least Squared Estimator

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/be0toJ/btrtZberS2Y/TKCjN70eogaml7xrWpkCgK/img.png" alt="drawing" /></center><br>

⑸ **method 1.** discrete probability distribution and maximum probability

> ① it uses the definition of binomial coefficients

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/zIJ5X/btrt3ge9qon/23bqBs4JO3XIajMmRFBBU0/img.png" alt="drawing" /></center><br>

> ② example

>> ○ situation**:** number of members of a population are estimated through marking-and-recapture method

>> ○ given the number of members N, the number of firstly captured members m, the number of lastly captured members n, the number of lastly marked memebers x

>> ○ probability distribution**:** [hypergeometric distribution](https://jb243.github.io/pages/1626)

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/ToUHh/btrt4g7ft7L/g0yZ30ePRKKnJYiAVACaF0/img.png" alt="drawing" /></center><br>

>> ○ question**:** the most reasonable value of N 

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/bOsk0j/btrtYc55z7l/Xy4u872sWGxKzw3EKmR96k/img.png" alt="drawing" /></center><br>

⑹ **method 2.** method of moment estimator (MOM)**:** also called sample analog estimation

> ① definition: the method of calculating the estimator of θ in a way of θˆ =g<sup>-1</sup>((1/n) × ∑X<sub>i</sub><sup>k</sup>) based on the fact that E(X<sup>k</sup>) = g(θ) ⇔ θ = g<sup>-1</sup>(E(X<sup>k</sup>)) 

>> ○ E(X<sup>k</sup>): moment or population moment

>> ○ (1/n) × ∑X<sub>i</sub><sup>k</sup>: sample moment

>> ○ the moment is a constant, and the sample moment is a random variable with a constant distribution

>> ○ consistency: by the law of large numbers, the sample moment converges into the moment 

> ② sample moment

>> ○ k-th order sample moment for origin

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/SNufJ/btrt4gTHdUp/gOfde3ktMzZV1SUWMCpDA0/img.png" alt="drawing" /></center><br>

>> ○ k-th order sample moment for sample mean

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/dAhI8J/btrt3fU2PlD/Pvq4l30VkwOOEAFaLS5fj0/img.png" alt="drawing" /></center><br>

> ③ example

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/b1cPmg/btrt3RM6DYu/LmUP3al2oRxL2j3VIWzuSk/img.png" alt="drawing" /></center><br>

⑺ **method 3.** maximum likelihood method (ML) 

> ① definition

>> ○ θ: parameter

>> ○ θ\*: the estimator of the parameter θ

>> ○ θ<sub>ML</sub>: the maximum likelihood estimator of the parameter θ

> ② likelihood**:** the possibility of something happening

> ③ likelihood function

>> ○ the probability that a given sample will come out when θ\* is given.

>> ○ also known as product of likelihoods

>> ○ <span>that is, p(X | θ\*)</span>

>> ○ marked as ℒ

> ④ log likelihood function**:** taking a log into the likelihood function

>> ○ marked as ℓ = ln ℒ

> ⑤ <span>maximum likelihood estimation**:** examining θML that maximizes the likelihood function p(X | θ)</span>

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/WWCyK/btrt3f8nGyS/dvwefmPT8XSRH7OD67v6gk/img.jpg" alt="drawing" /></center><br>

>> ○ **assumption :** the closer the θ\* is to parameter θ, the greater the likelihood function will be

>> ○ 1<sup>st</sup>. differentiation of log likelihood function**:** acquire θ\* that makes the local maximum on a valid interval 

>> ○ 2<sup>nd</sup>. if the local maximum exists**:** it is assumed that the θ\* that makes the local maximum is θML 

>> ○ 3<sup>rd</sup>. if the local maximum doesn't exist: it is assumed that θ\* with higher likelihood of both ends of a valid interval is θML 

>> ○ maximum likelihood estimation and Hessian matrix**:** a useful method for obtaining estimators of all differentiable functions

>>> ○ **step 1.** get the second order approximation by obtaining the Taylor series for θk , and calculate the solution θ<sub>k+1</sub> = θ<sub>k</sub> + d<sub>k</sub> that maximize the approximated equation 

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/vTIgq/btrtYcyp4Vq/oReCNkAX2KRGpvsfvqYVcK/img.png" alt="drawing" /></center><br>

<center><b>Figure. 3.</b> relationship between maximum likelihood estimation and Taylor expansion</center>

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/Nwa5B/btrt1Ju8fCs/GBKkxs8cysgHbcrKTpKDQK/img.jpg" alt="drawing" /></center><br>

>>> ○ **step 2.** [Newton-Raphson method](https://jb243.github.io/pages/1773): updating θ<sub>k</sub> will eventually reach the global maximum

>>> ○ example: [logistic regression](https://jb243.github.io/pages/1633) 

> ⑥ Maximum likelihood estimator: when sample X is given, the function G corresponding to θML that maximizes the likelihood 

>> ○ θ<sub>ML</sub> = G<sub>ℓ</sub> (ℓ) = G<sub>ℒ</sub>(ℒ)

>> ○ limitation of the estimator**:** there are limits on the assumption of maximum likelihood estimation

>> ○ the estimator favored by statisticians.

> ⑦ **MAP**(maximum a posteriori) 

>> ○ The maximum likelihood estimation is a special example of [Bayes rule](https://jb243.github.io/pages/1623) 

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/cVqolu/btrt4gTIY5R/BTKMm3nNNheEovnbAFiesk/img.jpg" alt="drawing" />

<br>

>> ○ MLE assumes a uniform distribution over the parameters, but when a prior distribution is available, the MAP method is used.

<br>

<img width="401" alt="스크린샷 2025-03-28 오후 4 23 26" src="https://github.com/user-attachments/assets/08c7eccf-6cde-4584-9fbf-f27e8f862b91" />

<br>

> ⑧ **Example 1.**

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/bkwFvQ/btrtTNe5Xvl/WaIcWWGkbXUo6MQ5ROxt9k/img.png" alt="drawing" /></center><br>

> ⑨ **Example 2.**

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/cvp1og/btrt4fUPRO3/jVvzKKi3YrlaX5hgersW30/img.png" alt="drawing" /></center><br>

> ⑩ **Example 3.** 

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/rbGnG/btrtYVpvPGk/VgGkqIvxe9ixT2mAO6DZw1/img.png" alt="drawing" /></center><br>

> ⑪ **Example 4.** the maximum likelihood estimation might not be determined solely

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/Gpo3e/btrtVn8fb7k/F4wcFhFkQtHAuzyQWgA6L1/img.png" alt="drawing" /></center><br>

> ⑫ **Characteristic 1.** consistency

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/cd5p09/btrtWvkOY7D/OsKrQQmx5T9ZJLhnxYB7Mk/img.png" alt="drawing" /></center><br>

> ⑬ **Characteristic 2.** asymptotic normal distribution

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/tQNZl/btrt1IQKh4C/BtBI3tNme71RWxTa7ZnVlk/img.png" alt="drawing" /></center><br>

> ⑭ **Characteristic 3.** invariance: if θ<sub>ML</sub> is the maximum likelihood estimator of θ, g(θ<sub>ML</sub>) is the maximum likelihood estimator of g(θ)  

<br>

<br>

## **3. interval estimation** (scaling type) 

⑴ definition**:** estimating which interval the parameter is in through the samples 

> ① purpose of introduction**:** the probability that the point estimator exactly matches the actual parameter is zero

> ② confidence level (confidence coefficient)

>> ○ P(θ<sub>left</sub> ＜ θ ＜ θ<sub>right</sub>) = 1 - α, 0 ＜ α ＜ 1

>> ○ threshold**:** values that constitute the boundary of the confidence interval. θ<sub>left</sub>, , θ<sub>right</sub>, etc 

>> ○ 1 - α: confidence level (confidence coefficient)

>> ○ α: rejection probability or significance level

>> ○ confidence interval**:** the interval [θ<sub>left</sub>, θ<sub>right</sub>] , the probability of θ being on which is (1 - α) × 100%

> ③ **notes**

>> ○ P(Z ＞ 1.65) = 5% ⇔ P( Z ＞ 1.65) = 10%

>> ○ P(Z ＞ 1.96) = 2.5% ⇔ P( Z ＞ 1.96) = 5%

>> ○ P(Z ＞ 2.58) = 0.5% ⇔ P( Z ＞ 2.58) = 1%

> ④ 68 - 95 - 99.7 rule 

>> ○ μ ± 1 × σ: 68.27 %

>> ○ μ ± 2 × σ: 95.45 %

>> ○ μ ± 3 × σ: 99.73 %

⑵ **case 1.** when Xi ~ N(μ, σ<sup>2</sup>) and the population variance σ<sup>2</sup> is known 

> ① overview**:** a normal distribution is used

> ② method

>> ○ introduction**:** when μ is known, the probability of X<sub>avg</sub> (confidence level: α) is as follows

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/b9jETY/btrt4sfsB7q/J0R0LRW7RLoo5Fg29jCKP1/img.png" alt="drawing" /></center><br>

>> ○ change of ideas**:** X<sub>avg</sub> ∈ <i>I</i> (μ) ⇔ μ ∈<i>I</i> (X<sub>avg</sub>) (confidence level**:** α)

>>> ○ meaning: it means the probability distribution of μ when X<sub>avg</sub> is known 

>>> ○ it is noted that the probability distribution of μ follows the same conceptual framework of the probability distribution of X<sub>avg</sub> when μ is known 

>>> ○ draw your own picture to confirm

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/bQJKrE/btrt0LT8n3q/HuVtuy2m4UynPNmqHg9Kb1/img.png" alt="drawing" /></center><br>

<center><b>Figure. 4.</b> a figure showing the correspondence between the probability distribution of X<sub>avg</sub> when μ is known and the confidence interval of μ conjectured through X<sub>avg</sub> </center>

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/pIUEY/btrtRCjSHts/KwhNFrx0AXxaDnSWjvOK7K/img.png" alt="drawing" /></center><br>

>> ○ <span>pivotal estimation: for the shortest confidence interval, it should be established that |a| = |b|, _i.e._ a = -z<sub>α/2</sub>, b = z<sub>α/2</sub>. here, the proof is omitted</span>

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/kQlc2/btrtZafBrwo/cUZ6GnZqlOepiCuPaWFR51/img.png" alt="drawing" /></center><br>

> ③ if you know the distribution function

>> ○ **example 1.** F(x) = √x / θ, 0 ≤ x ≤ θ<sup>2</sup>: the 90% confidence interval is as follows

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/1vEsu/btrtX0dd2hy/GRSMOIp35a3fsFh57QKAlk/img.png" alt="drawing" /></center><br>

>> ○ **example 2.** F(x) = (x / θ)<sup>n</sup>: the 90% confidence interval is as follows

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/bLzHuz/btrtXrPFiiJ/BsHxTBFSDSVPQFKESLf0o0/img.png" alt="drawing" /></center><br>

⑶ **case 2.** when X<sub>i</sub> ~ N(μ, σ<sup>2</sup>) and the population variance σ<sup>2</sup> is unknown 

> ① overview 

>> ○ normal distribution needs to know the variance of the population 

>> ○ in reality, the sample variance is used because the population variance is unknown 

>> ○ the distribution of sample mean when using sample variance instead of population variance is exactly t-distribution 

> ② **example 1.** sample mean

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/TiURJ/btrt1JvncJx/Csn7czfilwOHteYZEOvEnK/img.png" alt="drawing" /></center><br>

>> ○ introduction**:** when μ is known, the probability of X<sub>avg</sub> (confidence level: α)

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/whwND/btrtXxvV9q0/CPVC53tiRXpqlXkXdbOkck/img.png" alt="drawing" /></center><br>

> ○ change of ideas: X<sub>avg</sub> ∈ _I\*_ (μ) ⇔ μ ∈ _I\*_ (X<sub>avg</sub>) (confidence level: α)

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/d2F2Pu/btrtYUqEgz7/pv0hYLxjE8KwOsIImPyuaK/img.png" alt="drawing" /></center><br>

> ○ <span>pivotal estimation**:** for the shortest confidence interval, it should be established that |a| = |b|, _i.e._ a = - t<sub>α/2</sub>, b = t<sub>α/2</sub>. here, the proof is omitted</span>

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/dLAihI/btrtZ7wvgni/xsEjPdVewneunp9whnCk81/img.png" alt="drawing" /></center><br>

> ③ **example 2\. (case 1)** when X<sub>i</sub> (μ<sub>X</sub>, σ<sup>2</sup>) (i = 1, ···, n) and Y<sub>j</sub> (μ<sub>Y</sub>, σ<sup>2</sup>) (i = 1, ···, n) are paired 

>> ○ also called paired estimation (matched sample estimation)

>> ○ in fact, there is only one variable**:** after defining W<sub>i</sub> = X<sub>i</sub> - Y<sub>i</sub>, manipulate **example 1** 

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/QjckY/btrtWuzttgo/gvG0rDqoH3ZcxFHi4PGST0/img.png" alt="drawing" /></center><br>

>> ○ an example of a paired sample 

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/wRC2A/btrtYcrAHqU/3HGkcuUKItI8byHYgSIqD0/img.jpg" alt="drawing" /></center><br>

<center><b>Table. 1.</b> an example of a paired sample</center>

<br>

>> ○ an example of an independent sample 

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/cWLaOR/btrtVprEwBD/NkyqhWV3A04WNmuKYrWuy1/img.jpg" alt="drawing" /></center><br>

<center><b>Table. 2.</b> an example of an independent sample</center>

> ④ **example 3. (case 2)** difference of two sample means: when X<sub>i</sub> (μ<sub>X</sub>, σ<sup>2</sup>) (i = 1, ···, n) and Y<sub>j</sub> (μ<sub>Y</sub>, σ<sub>2</sub>) (j = 1, ···, m) are **independent** 

>> ○ when the variances of two sample means are same in unpaired sample estimation (pooled sample estimation) 

>> ○ formula 

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/buRUP4/btrtXxJrl2j/P3wWTBDuQdcJtzsDJ11Vtk/img.png" alt="drawing" /></center><br>

>> ○ confidence interval for confidence level α

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/PcuoO/btrtZ65rIeB/rk1YFoDiXYSsQFpAKmE3g1/img.png" alt="drawing" /></center><br>

> ⑤ **example 4. (case 3)** difference of sample means**:** when X<sub>i</sub> (μ<sub>X</sub>, σ<sub>X</sub><sup>2</sup>) (i = 1, ···, n) and Y<sub>j</sub> (μ<sub>Y</sub>, σ<sub>Y</sub><sup>2</sup>) (j = 1, ···, m) are **independent** (assuming σ<sub>X</sub> ≠ σ<sub>Y</sub>)

>> ○ when the variances of two sample means are different in unpaired sample estimation (pooled sample estimation) 

>> ○ Welch approach is used

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/b0uGad/btrtVo0ChcL/Q2SnhiyTKNj4C9SBqPeTA1/img.png" alt="drawing" /></center><br>

>> ○ degree of freedom in **(case 3)** is lower than **(case 2)** → power of test decreases

>> ○ the formula of ν is very complex

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/XSKiX/btrt4g7iJq1/BovePwZ1CVUDkeKJwW8mhK/img.png" alt="drawing" /></center><br>

> ⑥ **example 5.** confidence interval of population variance 

>> ○ note that there is no obvious solution in minimizing the size of the confidence interval**:** numerical analysis should be used

>> ○ the given model

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/xm6a4/btrtXyO85gf/Xc4711uDk7smbdvn6mDtok/img.png" alt="drawing" /></center><br>

>> ○ confidence interval for confidence level α

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/wolt4/btrtU9BIQBy/Abm1AXuq5qOKpzKHIxC1Z0/img.png" alt="drawing" /></center><br>

> ⑦ **example 6.** ratio of population variance

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/bytbCu/btrtREhIC40/gdOZt9yBuch5Qk59h0WJj0/img.png" alt="drawing" /></center><br>

>> ○ confidence interval for confidence level α  

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/zVYAo/btrtZ6xDdUS/B2jCnxRGtAFiCoyaA7mhVK/img.png" alt="drawing" /></center><br>

⑷ **case 3.** when samples do not follow normal distribution, but there are many samples

> ① [central limit theorem](https://jb243.github.io/pages/1594#7-central-extreme-theorem): if n is large enough, the distribution of sample mean converges into normal distribution

>> ○ formula

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/pQkBq/btrtZa7I9pu/aSbscVVW5ZizpbmA5YyU91/img.png" alt="drawing" /></center><br>

>> ○ t distribution eventually converges into normal distribution

> ② number of samples

>> ○ normality is typically achieved with only 25 ~ 30 samples

>> ○ for a symmetric unimodal distribution (having one extreme value), n = 5 is sufficient

> ③ **example 1.** population ratio

>> ○ given model

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/burOj5/btrt3QOqoPj/kXN82ab5sk0PKYW5kv8xo1/img.png" alt="drawing" /></center><br>

>> ○ confidence interval for confidence level α 

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/dBpFXU/btrtZa7I6Hs/98r2FmYPQpmhr4l1vAHbfk/img.png" alt="drawing" /></center><br>

> ④ **example 2.** correlation coefficient 

>> ○ null hypothesis H<sub>0</sub>: correlation coefficient = 0

>> ○ alternative hypothesis H1: correlation ceofficient ≠ 0

>> ○ calculation of t statistics**:** for the correlation coefficient r obtained from the sample,

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/be8tZb/btrtRCEce1j/XRmxXQdS1A1cPyCaSmail1/img.jpg" alt="drawing" /></center><br>

>> ○ the above statistic follows the student t distribution with a degree of freedom of n - 2 (assuming the number of samples is n)

<br>

---

*Input : 2019.06.19 14:23*
