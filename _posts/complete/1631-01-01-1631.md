## **Chapter 14. Statistical Test**

Higher category: 【Statistics】 [Statistics Overview](https://jb243.github.io/pages/1641)

---

**1.** [terminology](#1-terminology)

**2.** [Neyman-Pearson lemma](#2-neyman-pearson-lemma)

**3.** [generalized likelihood ratio test](#3-generalized-likelihood-ratio-test)

**4.** [p value](#4-p-value)

**5.** [Types of Statistical Tests](#5-types-of-statistical-tests)

---

<br>

## **1. terminology** 

⑴ test

> ① definition **:** verifying if the hypothesis is statistically significant

>> ○ **application 1.** randomization check (balance test)**:** verifying that the random sampling goes well 

>> ○ **application 2.** causal effect**:** verifying that a particular treatment makes a significant change

> ② test statistic**:** summarizing the n-dimensional information of the state space in one dimension and using it for statistical test

>> ○ example**:** Z, T, χ<sup>2</sup>, F, etc 

>> ○ being able to be summarized in one dimension is important when the size of the critical region is constant

> ③ parametric test 

>> ○ definition**:** testing parameters based on test statistics

>> ○ in general, it is assumed that the distribution of the population is normal distribution**:** the central limit theorem is used for this assumption 

>> ○ in reality, using a parametric test on any sample without the above assumption is not a big problem

> ④ Non-parametric test

>> ○ Definition: A method of testing non-parametric characteristics through test statistics.

>> ○ Used when the population distribution cannot be specified (distribution-free method).

>> ○ Compared to parametric methods, the calculation of statistics is simpler and more intuitive to understand.

>> ○ Less affected by outliers.

>> ○ The reliability of the test statistics is often insufficient.

⑵ hypothesis

> ① null hypothesis (**H<sub>0</sub>**)**:** a hypothesis to be tested directly 

> ② alternative hypothesis (**H<sub>1</sub>**)**:** a hypothesis to be accepted when null hypothesis is rejected 

>> ○ Also known as the research hypothesis.

> ③ characteristics**:** for parameter θ,

>> ○ H<sub>0</sub> **:** Θ<sub>0</sub> =｛θ<sub>0</sub>, θ<sub>0</sub>', θ<sub>0</sub>'', ···｝ 

>> ○ H<sub>1</sub> **:** Θ<sub>1</sub> =｛θ<sub>1</sub>, θ<sub>1</sub>', θ<sub>1</sub>'', ···｝

>> ○ **characteristic 1.** p(θ ∈ Θ<sub>0</sub> or θ ∈ Θ<sub>1</sub>) = 1

>> ○ **characteristic 2.** p(θ ∈ Θ<sub>0</sub> and θ ∈ Θ<sub>1</sub>) = 0

> ④ classification

>> ○ simple hypothesis**:** in the cases of Θ =｛θ<sub>0</sub>｝, Θ =｛θ<sub>1</sub>｝, ···

>> ○ composite hypothesis**:** if the hypothesis is not a simple hypothesis

>> ○ example: the hypotheses that H<sub>0</sub> **:** θ ≤ θ0, H<sub>1</sub> **:** θ ＞ θ0 are composite hypotheses

⑶ introduction of critical region 

> ① state space = critical region + acceptable region

>> ○ Rejection Region: The range of test statistics that lead to the rejection of the null hypothesis.

>> ○ sample ∈ critical region**:** H0 is rejected 

>> ○ sample ∉ critical region**:** H0 is not rejected 

> ② power function πC(θ)**:** the probability of the samples being included in the critical region when the critical region is C and the parameter is θ

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/MGcAt/btruqEnkfR6/BTt3TI7bWYsOBHv9bZ29M1/img.png" alt="drawing" /></center><br>

> ③ an example of power function 

>> ○ p(x) = 4x<sup>3</sup> / θ<sup>4</sup> <i>I</i>｛0 ＜ x ＜ θ｝

>> ○ C =｛x | x ≤ 0.5, x ＞ 1｝

>> ○ θ ≤ 0.5 

<br>

<center>π<sub>C</sub>(θ) = 1</center>

<br>

>> ○ 0.5 ＜ θ ≤ 1 

<br>

<center><span>π<sub>C</sub>(θ) = ∫ p(x) dx (assuming x ∈ \[0, 0.5\] ) = 1 / 16θ<sup>4</sup></span></center>

<br>

>> ○ 1 ＜ θ

<br>

<center><span>π<sub>C</sub>(θ) = ∫ p(x) dx (assuming x ∈ \[0, 0.5\] ∪\[1, θ\] ) = 1 - 15 / 16θ<sup>4</sup></span></center>

<br>

> ④ size of critical region (size of test)**:** the maximum value of the probability of samples being included in the critical region when the null hypothesis is true 

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/bpt4Sn/btruuhLHi8g/OsyNIC4eK2nAo1A3j8yw9k/img.png" alt="drawing" /></center><br>

> ⑤ power **:** the probability of samples being included in the critical region when the alternative hypothesis is true. it is also the probability of null hypothesis to be rejected when the alternative hypothesis is true 

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/yXMrb/btrusNRBE2f/udiKL5Kj9ghbXeitbQeLOk/img.png" alt="drawing" /></center><br>

> ⑥ error **:** making a wrong statistical conclusion 

>> ○ ideal critical region 

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/eTr3j9/btruvtLSWRR/89zixGiAlqEEjCrGpksy1K/img.png" alt="drawing" /></center><br>

>> ○ type Ⅰ error 

>>> ○ definition **:** the error rejecting null hypothesis when null hypothesis is true

>>> ○ condition **:** defined when null hypothesis is **simple hypothesis**

>>> ○ the probability of type Ⅰ error (**α**) = the size of the critical region 

>>> ○ significance level**:** 10%, 5%, 1%, etc 

>>> ○ confidence level**:** 90%, 95%, 99%, etc 

>> ○ type Ⅱ error 

>>> ○ definition **:** the error adopting null hypothesis when alternative hypothesis is true

>>> ○ condition **:** defined when alternative hypothesis is **simple hypothesis**

>>> ○ the probability of type Ⅱ error (**β**) = 1 - power

>> ○ trade-off between α and β 

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/oxMlR/btrul4gdFD4/VDBEBXjZq3k7V1JPZFVg41/img.jpg" alt="drawing" /></center><br>

<center><b>Figure. 1.</b> trade-off between type Ⅰ error (α) and type Ⅱ error (β)</center>

<br>

>>> ○ the critical region appears as an interval larger than a specific value or less than a specific value (**∵** Neyman-Pearson lemma)

>>> ○ both α and β cannot be reduced

⑷ comparison of critical region 

> ① **criterion****:** power should be greater when the critical region is same in size

> ② more powerful testing**:** for a specific θ1 ∈ Θ1 and two critical regions C<sub>1</sub>, C<sub>2</sub>, 

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/blTVJQ/btrurUpRj0e/nc7hskO1frniHH0laGQCnK/img.png" alt="drawing" /></center><br>

> ③ most powerful testing **:** for a specific θ<sub>1</sub> ∈ Θ<sub>1</sub> and any critical region C,

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/TyCOd/btrusNKS8zZ/HWLdbA81oFU456Q4uEJhi1/img.png" alt="drawing" /></center><br>

> ④ uniformly most powerful testing**:** for any θ ∈ Θ<sub>1</sub> and any critical region C,

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/4J4N1/btruvvbQP1D/Lf1kgUmCoKYd7Er6ppSW4K/img.png" alt="drawing" /></center><br>

<br>

## **2. Neyman-Pearson lemma** 

⑴ idea

> ① premise**:** H0 **:** θ = θ<sub>0</sub>, H<sub>1</sub> **:** θ = θ<sub>1</sub> (simple hypothesis)

> ② question **:** finding a critical region that maximizes the power when the size of the critical region is constant

> ③ speculation**:** samples from a state space are included in the critical region C one by one 

>> ○ when a sample x is included C, both p(x, θ<sub>0</sub>) and p(x, θ<sub>1</sub>) increase

>> ○ p(x, θ<sub>0</sub>)**:** a kind of cost. the increase of p(x, θ<sub>0</sub>) increases the size of the critical region 

>> ○ p(x, θ<sub>1</sub>)**:** a kind of benefit. the increase of p(x, θ<sub>1</sub>) increases the power 

> ④ conclusion 

>> ○ line-up strategy**:** it is advantageous to include a sample x having more p(x, θ<sub>1</sub>) ÷ p(x, θ<sub>0</sub>) in the critical region in a faster order  

>> ○ <span>the critical region made by the line-up strategy, C =｛x | p(x, θ<sub>1</sub>) ÷ p(x, θ<sub>0</sub>) ≥ k｝, is a critical region for uniformly most powerful testing</span>

⑵ lemma 

> ① premise **:** H<sub>0</sub>, H<sub>1</sub> are simple hypotheses

> ② statement **:** for any k ∈ ℝ, if we take the following critical region it will be a critical region for uniformly most powerful testing 

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/bh4TLs/btruiRgy7M0/T8qgmmRmnLrlPMBeT9M1C0/img.png" alt="drawing" /></center><br>

>> ○ ℒ **:** likelihood function

>> ○ likelihood ratio test (LR test)**:** a test like λ(x) ≥ k

>> ○ determination of critical region**:** to know the exact form of critical region, the size of the critical region should be given 

>> ○ every x satisfying λ(x) ≥ k is included in critical region C\*

>> ○ every x satisfying λ(x) ＜ k is not included in critical region C\*

> ③ application

>> ○ as only the order of p(x, θ<sub>1</sub>) ÷ p(x, θ<sub>0</sub>) is important, the following conversion using a monotone increasing function f(·) is allowed

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/bnR4Aq/btruwI28Byg/wOLibmbnRueR5IQ07OVIw1/img.png" alt="drawing" /></center><br>

>> ○ terms related to θ<sub>0</sub>, θ<sub>1</sub>, n, etc are easily removed 

>> ○ point**:** the modification of critical region is allowed as far as the existence of k' is ensured  

>> ○ determination of critical region**:** to know the exact form of critical region the size of the critical region should be given 

⑶ proof 

> ① assumption **:** C\* and C are same in size

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/bZXSYn/btrusOpsMsv/JYeT6T2CyKoHWXFMBMcxC1/img.png" alt="drawing" /></center><br>

> ② definition of C\* 

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/uxHzY/btrul2W5cFs/FftkPBkkPGoAMsD8QSIMl1/img.png" alt="drawing" /></center><br>

> ③ conclusion**:** C\* is a critical region with the uniformly most powerful testing

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/5JiX1/btrul2W5dwC/e9zZBA2JrKejravuHrh7hk/img.png" alt="drawing" /></center><br>

⑷ **example** **1.**

> ① X<sub>1</sub>, ···, X<sub>n</sub> ~ Bernoulli(θ)

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/bv7d3d/btrul3In3cr/RXc34isb2c1loFxX4ERFck/img.png" alt="drawing" /></center><br>

> ② H<sub>0</sub> **:** θ = θ<sub>0</sub>, H<sub>1</sub> **:** θ = θ<sub>1</sub> ＞ θ<sub>0</sub>

> ③ likelihood ratio test

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/y9Ltr/btruuiKA1fP/4X7poKdUGCIx04AYTy7WIK/img.png" alt="drawing" /></center><br>

> ④ Z-test (confidence level **:** α) 

>> ○ θ<sub>1</sub> ＞ θ<sub>0</sub> **: one-tailed test**

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/K5FNs/btruuhLHs5r/JTH3pIIalHIQKfyBnwQqsk/img.png" alt="drawing" /></center><br>

>> ○ θ<sub>1</sub> ＜ θ<sub>0</sub> **:** **one-tailed test** 

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/vqdFS/btrurUco1HT/rb0xm0mjk6KUpv4F3xIuc1/img.png" alt="drawing" /></center><br>

>> ○ critical region with uniformly most powerful testing does not exist because the size of critical region of the most powerful testing depends on whether θ<sub>0</sub> is bigger than θ<sub>1</sub> or not  

⑸ **example 2.**

> ① X<sub>1</sub>, ···, X<sub>n</sub> ~ N(μ, 1<sup>2</sup>)

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/duVRtz/btruvucWhEV/0BrakXzb7k6XKhOnVHDuA0/img.png" alt="drawing" /></center><br>

> ② H<sub>0</sub> **:** μ = μ<sub>0</sub>, H<sub>1</sub> **:** μ = μ<sub>1</sub> ＞ μ<sub>0</sub>

> ③ likelihood ratio test

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/E8oOt/btruuiKA2g3/lvGRTgd64XKnlseOFlnV7K/img.png" alt="drawing" /></center><br>

> ④ Z-test **:** **one-tailed test** (confidence level **:** α)

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/wYUXZ/btruvvbQYNS/9pfIhTBMaWeGTiyZgA2L30/img.png" alt="drawing" /></center><br>

⑹ **generalization** **1.** the form of the critical region is constant no matter whether H<sub>1</sub> is a composite hypothesis or not, when the critical region does not depend on the specific values of θ<sub>1</sub>

> ① X<sub>1</sub>, ···, X<sub>n</sub> ~ Bernoulli(θ)

> ② H<sub>0</sub> **:** θ = θ<sub>0</sub>, H<sub>1</sub> **:** θ ＞ θ<sub>0</sub>

⑺ **generalization 2.** in the **generalization 1**, the form of the critical region is constant when the alternative hypothesis is a composite hypothesis including θ0 and α is the maximum if θ = θ<sub>0</sub> 

> ① X<sub>1</sub>, ···, X<sub>n</sub> ~ Bernoulli(θ)

> ② H<sub>0</sub> **:** θ ＜ θ<sub>0</sub>, H<sub>1</sub> **:** θ ＞ θ<sub>0</sub>

<br>

<br>

## **3\. generalized likelihood ratio test**  

⑴ definition

> ① the limit of Neyman-Pearson lemma **:** in general, the null hypothesis and the alternative hypothesis should be simple hypotheses

> ② GLR test (generalized likelihood ratio test) 

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/cVkOXA/btruqFT1KnN/A70CjLxb2NLNk6KVbtlqCk/img.png" alt="drawing" /></center><br>

> ③ max p(x, θ) utilizes the [maximum likelihood method](https://jb243.github.io/pages/1630) (ML) 

> ④ this method has been proven to set a statistically not bad critical regions 

⑵ **example 1.** X<sub>i</sub> ~ N(μ, σ<sup>2</sup>), σ<sup>2</sup> is known 

> ① H<sub>0</sub> **:** μ = μ<sub>0</sub>, H<sub>1</sub> **:** μ ≠ μ<sub>0</sub>

> ② generalized likelihood ratio test  

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/dgZKRl/btruokCUpwF/UfuRVwXe0uAteENDIaPls0/img.png" alt="drawing" /></center><br>

> ③ τ\-test**:** one-tailed test (confidence level: α)

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/bDDAfy/btrupHq3Iyg/1CYEStN4LQYUaBOI73dWTk/img.png" alt="drawing" /></center><br>

> ④ Z-test **:** **two-tailed test** (confidence level: α)

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/KDvZn/btruol2WMUd/N1Ndxe9Nk1KIVJk96wTYs0/img.png" alt="drawing" /></center><br>

> ⑤ it is proven that even if Xi does not follow the normal distribution, the above method can be applied approximately 

⑶ **example 2.** X<sub>i</sub> ~ N(μ, σ<sup>2</sup>), σ<sup>2</sup> is unknown 

> ① H<sub>0</sub> **:** μ = μ<sub>0</sub>, H<sub>1</sub> **:** μ ≠ μ<sub>0</sub>

> ② generalized likelihood ratio test  

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/erunTt/btrupIwLVy3/tmwBwlI8FOAGRTtRs5wRaK/img.png" alt="drawing" /></center><br>

> ③ F-test**:** one-tailed test (confidence level: α)

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/8doRe/btruolIAeAM/v59p6BkuMTU2uhiS6KD0Kk/img.png" alt="drawing" /></center><br>

> ④ T-test**:** **two****\-tailed test** (confidence level**:** α) 

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/H1XCb/btrul3n8L6N/kwbPtRAu8KmqjRs4lJZt4K/img.png" alt="drawing" /></center><br>

⑷ **example 3.** X<sub>i</sub> ~ N(μ, σ<sup>2</sup>), σ<sup>2</sup> is unknown 

> ① H<sub>0</sub> **:** μ = μ<sub>0</sub>, H<sub>1</sub> **:** μ ＞ μ<sub>0</sub>

> ② generalized likelihood ratio test 

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/cdyZHX/btruvuYkawU/uXk35g6zi7IEtT2ItYoMk0/img.png" alt="drawing" /></center><br>

> ③ key assumptions 

>> ○ X<sub>avg</sub> ≥ μ<sub>0</sub> has a higher likelihood ratio than X<sub>avg</sub> ＜ μ<sub>0</sub>, so the former has a higher priority in the line-up strategy than the latter 

>> ○ as the significance level is only 0.025, 0.05, and 0.10 at most, it is sufficient to consider Xavg ≥ μ0 having the half of the full probable cases 

> ④ T-test **:** **one-tailed test** (confidence level: α)

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/bUubMv/btrunPC8Nkq/2Beo87w6MdGEiqTNoukNqK/img.png" alt="drawing" /></center><br>

> ⑤ H<sub>1</sub> **:** the same logic is applied even if μ ＜ μ<sub>0</sub> 

⑸ **example 4.** X<sub>i</sub> ~ N(μ, σ<sup>2</sup>), μ is unknown 

> ① H<sub>0</sub> **:** σ<sup>2</sup> = σ<sub>0</sub><sup>2</sup>, H<sub>1</sub> **:** σ<sup>2</sup> ≠ σ<sub>0</sub><sup>2</sup>

> ② generalized likelihood ratio test

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/FQwBV/btrurTEzixX/cZWbZmNIEXhj1Z9vrY8so0/img.png" alt="drawing" /></center><br>

> ③ setting the critical region 

>> ○ f(τ) is a function that is convex downwards with a minimal value at  τ = n

>> ○ <span><b>condition 1.</b> P(τ ≥ k' | H<sub>0</sub>) + P(τ ≤ k'' | H<sub>0</sub>) = α </span>

>> ○ **condition 2.** f(k') = f(k'')

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/c0EYxE/btruvuqtPhP/vak9EDxSIRPTJ7C8LKzWe1/img.png" alt="drawing" /></center><br>

> ④ τ\-test**:** two-tailed test (confidence level**:** α)

>> ○ numerical analysis is required to set an ideal critical region

>> ○ in practice, simpler critical regions are used

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/m6OQg/btruqFzLAZe/KKEpmQXUXUJoM80n8CDzRk/img.png" alt="drawing" /></center><br>

⑹ **example 5.** special likelihood ratio test

> ① definition

>> ○ in the case that X<sub>i</sub> ~ N(μ, σ<sup>2</sup>) and σ<sup>2</sup> is known, 2 ln λ ~ χ<sup>2</sup>(1) 

>> ○ if the sample size n is large enough, the following is mathematically demonstrated for the number of parameters, _i.e._ k 

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/bh8oSG/btruokiB7sB/MRaPedCoUfHqKo8nWuNrok/img.png" alt="drawing" /></center><br>

> ② τ\-test**:** one-tailed test (confidence level **:** α)

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/cbA7jy/btruologTSw/aEkHZmIUVM4tTRo6SqMEa1/img.png" alt="drawing" /></center><br>

> ③ supplements

>> ○ some statisticians only refer to these tests as the likelihood ratio test (LR test)  

>> ○ some statisticians define \-2 ln λ = 2 ln ℒ(H<sub>1</sub>) - 2 ln ℒ(H<sub>0</sub>)) 

<br>

<br>

## **4\. p value** 

⑴ definition: probability of more extreme values than a given sample when null hypothesis is true

> ① **Another definition:** probability of null hypothesis being true

> ② Rejecting only if the test statistic is included in the critical region and rejecting only if the p value is less than α are necessary and sufficient conditions

> ③ A strict definition

<br>

<img width="255" alt="스크린샷 2025-03-31 오후 11 06 20" src="https://github.com/user-attachments/assets/cbd01212-53dc-4bb6-bd71-65b8f2b2560c" />

<br>

⑵ calculation **:** θ\* is a measured value

> ① right-sided test**:** p value = P(θ ≥ θ\*)

> ② left-sided test**:** p value = P(θ ≤ θ\*)

> ③ <span>symmetric distribution about μ**:** p value = P(|θ - μ| ≥ |θ* - μ|)</span>

> ④ chi-squared distribution**:** if θ\* is bigger than the median, p value = P(θ ≥ θ\*). if θ\* is smaller than the median, p value = P(θ ≤ θ\*)

⑶ power and p value 

> ① the main issues in classical statistics are finding distribution and increasing power

> ② strict meaning**:** high power means that if the alternative hypothesis is true when α is constant, the probability of rejecting the null hypothesis is high

> ③ meaning that α is constant

>> ○ meaning to define a constant Maginot line for each distribution obtained from various statistical techniques

>> ○ meaning that many other cases other than a given sample are regarded as the null hypothesis is true even if they are not necessarily indicating the true null hypothesis

> ④ meaning of increasing 1-β**:** meaning of making the Maginot Line more extreme in various statistical techniques

> ⑤ intuitive meaning**:** higher power means that **we will use statistical techniques that show a smaller p value** when α is constant

> ⑥ **example 1.** for the same sample, using the F statistic than the t statistic has a smaller pvalue → higher power

> ⑦ **example 2.** the t-distribution becomes narrower as the degree of freedom increases → the power increases

> ⑧ different statistical techniques have different power: meaning that statistical conclusions may differ for the same statistical data

⑷ **Example :** correlation coefficient and p value.

> ① H<sub>0</sub> **:** X and Y are not correlated 

> ② meaning of p value **:** probability that the correlation coefficient of the sample group taken from the uncorrelated population is greater than the given correlation coefficient

> ③ assumtion of the calculation of the value through the normal distribution

>> ○ random sampling data

>> ○ two-variate normal distribution: two variables X and Y follow normal distribution

>> ○ linear relationship**:** the relationship of two-order or three-order is not suitable 

>> ○ failure to meet three conditions above, p value must be calculated by non-parametric test 

⑸ Multiple testing problem 

> ① Overview

>> ○ Assume that the p-value follows a uniform distribution under the null hypothesis H<sub>0</sub>.

>>> ○ Proof: Under the null hypothesis, let the CDF of S be F<sub>0</sub>. If F<sub>0</sub> is a non-decreasing function, then...

<br>

<img width="505" alt="스크린샷 2025-04-15 오전 1 06 37" src="https://github.com/user-attachments/assets/3811ba26-60fb-4f84-aeb7-8b3070527f5e" />

<br>

>> ○ Problem Definition: Suppose we test 1,000 hypotheses and reject the null hypothesis for each hypothesis with a p-value less than α = 0.05. In this case, how many null hypotheses would we expect to be incorrectly rejected? The answer is approximately 50 (∵ 1000 × 0.05 = 50). Thus, we cannot assume that all rejected hypotheses are significant.

<br>

<img width="432" alt="스크린샷 2025-04-17 오전 8 40 31" src="https://github.com/user-attachments/assets/098876a2-ff84-469b-a152-5708af927d06" />

<br>

>> ○ Key Issue: Conducting multiple statistical tests inherently increases the likelihood of inaccurate conclusions.

>> ○ Example: This problem is particularly relevant when identifying differentially expressed genes (DEGs) from sequencing data consisting of multiple genes.

> ② **Solution 1:** Controlling the Family-Wise Error Rate (FWER)

>> ○ Definition: The probability of making at least one incorrect conclusion among all hypotheses.

<br>

<img width="285" alt="스크린샷 2025-04-17 오전 8 46 55" src="https://github.com/user-attachments/assets/accb0314-5691-4e3d-8f19-a8bc9a6e959d" />

<br>

>>> ○ For instance, a 5% FWER means that the probability of making even a single incorrect conclusion is less than or equal to 5%. This approach is very conservative and minimizes false positives.

>>> ○ FWER is sometimes criticized as inducing low power in the sense of leading to making many type II errors.

>> ○ **Methods 1.** Sidak Correction: Adjusts the alpha threshold instead of the p-values. Used when p-values are independent.

<br>

<img width="306" alt="스크린샷 2024-11-26 오후 3 38 43" src="https://github.com/user-attachments/assets/10885ad4-bed0-4d3b-9ec6-81324e7c70e4">

<br>

>>> ○ d: Number of statistical tests

>> ○ **Method 2.** Bonferroni Correction: Adjusts individual p-values directly. Can be applied even if p-values are not independent. Very conservative.

<br>

<img width="256" alt="스크린샷 2024-11-26 오후 3 39 44" src="https://github.com/user-attachments/assets/de74410c-9263-4c02-9d1f-c4f739ed7bf1">

<br>

>>> ○ d: Number of statistical tests

>>> ○ Note: If the adjusted p-value exceeds 1, it is forcibly set to 1.

>>> ○ FWER proof at α 

>>>> ○ Let the number of statistical tests be m, and assume that each test is independent (this assumption is required for the union bound condition).

>>>> ○ I<sub>0</sub> is a fixed but unknown set, which is presumed to be primarily composed of null hypotheses with high p-values.

<br>

<img width="502" alt="스크린샷 2025-04-17 오전 8 59 27" src="https://github.com/user-attachments/assets/30e713dc-bb0f-4fbd-af8c-c0a6177f3e77" />

<br>

>> ○ **Method 3.** Holm (step-down) procedure

>>> ○ **Step 1.** Order the p-values, obtaining P<sub>(1)</sub> ≤ ··· ≤ P<sub>(m)</sub>.

>>> ○ **Step 2.** Let R denote the smallest r ≥ 0 such that P<sub>(r+1)</sub> > α / (m-r).

>>> ○ **Step 3.** If R > 0, reject H<sup>(1)</sup>, ···, H<sup>(R)</sup>, where H<sup>(i)</sup> is associated with P<sub>(i)</sub>.

<br>

<img width="301" alt="스크린샷 2025-04-17 오전 9 12 39" src="https://github.com/user-attachments/assets/b94aeedf-df09-4807-8327-e6bfa81c5465" />

<br>

>>> ○ FWER proof at α

>>>> ○ Let the number of statistical tests be m, and assume that each test is independent (this assumption is required for the union bound condition).

>>>> ○ I<sub>0</sub> is a fixed but unknown set, which is presumed to be primarily composed of null hypotheses with high p-values.

<br>

<img width="602" alt="스크린샷 2025-04-17 오후 9 07 05" src="https://github.com/user-attachments/assets/06368dc0-b096-48cc-8a02-289399532b7e" />

<br>

>>> ○ At the same α, Holm is more powerful than Bonferroni.

<br>

<img width="458" alt="스크린샷 2025-04-17 오후 9 16 21" src="https://github.com/user-attachments/assets/b81a4ecc-ed34-493c-a89d-4cc5cf496e76" />

<br>

>> ○ **Method 4.**  Hochberg (step-up) procedure

>>> ○ **Step 1.** Order the p-values, obtaining P<sub>(1)</sub> ≤ ⋯ ≤ P<sub>(m)</sub>.

>>> ○ **Step 2.** Let R denote the largest r ≥ 0 such that P<sub>(r)</sub> ≤ α / (m + 1 - r).

>>> ○ **Step 3.** If R > 0, reject H<sup>(1)</sup>, ⋯, H<sup>(R)</sup>, where H<sup>(i)</sup> is associated with P<sub>(i)</sub>.

<br>

<img width="303" alt="스크린샷 2025-04-17 오후 11 02 00" src="https://github.com/user-attachments/assets/c97d9b3b-359c-4538-8114-1a2ad5ef132b" />

<br>

>>> ○ Intuitive understanding of FWER control at significance level α

>>>> ○ Assume the number of statistical tests is m, and that each test is independent (required for the independence condition)

>>>> ○ Let I<sub>0</sub> be a fixed but unknown set, assumed to primarily include null hypotheses with large p-values

>>>> ○ Note: The inequality <span style="color: red;">m - j<sub>0</sub> + 1 ≥ m<sub>0</sub></span> does not necessarily hold, so the following derivation is for reference only

<br>

<img width="600" alt="스크린샷 2025-04-17 오후 11 05 14" src="https://github.com/user-attachments/assets/174b165d-7ffb-40bb-ac29-2666205ec526" />

<br>

>>> ○ When using the same significance level α, Hochberg is more powerful than Holm

>>>> ○ Intuition: Holm uses a "for all" condition, while Hochberg uses a "for some" condition

>> ○ **Method 5.** Tukey-Kramer honest significant difference (range test)

>>> ○ This procedure applies for performing all pairwise comparisons in a multiple sample situation. 

>>> ○ Null hypothesis: H<sup>jk</sup> : μ<sub>j</sub> = μ<sub>k</sub>

>>>> ○ J samples (Y<sub>ij</sub> : i = 1, ···, n<sub>j</sub>), j = 1, ···, J

>>>> ○ N = n<sub>1</sub> + ··· + n<sub>J</sub>

>>>> ○ μ<sub>j</sub>: Population mean of the group j

>>> ○ Statistic: At level α, Tukey-Kramer rejects H<sup>jk</sup> if

<br>

<img width="400" alt="스크린샷 2025-04-17 오후 11 33 17" src="https://github.com/user-attachments/assets/6c9ecd09-fe82-4373-97c5-7021388c8919" />

<br>

>>> ○ Theory: When the samples are independent, normal and with same variance, and when the sample sizes are equal, Tukey-Kramer controls the FWER exactly at level α.

> ③ **Solution 2:** Controlling the False Discovery Rate (FDR)

>> ○ Overview

>>> ○ Definition: Limits the proportion of incorrect conclusions (false discoveries) among hypotheses where the null hypothesis is rejected to a certain level.

<br>

<img width="203" alt="스크린샷 2025-04-18 오전 12 04 12" src="https://github.com/user-attachments/assets/17abcdbd-4512-49db-b784-3d783d9acdef" />

<br>

>>> ○ FWER control implies FDR control (at the same level α).

<br>

<img width="344" alt="스크린샷 2025-04-18 오전 12 04 55" src="https://github.com/user-attachments/assets/e772faee-42e0-44de-b77c-6e73d3ea28dc" />

<br>

>>> ○ By considering the p-value distributions under both H<sub>0</sub> and H<sub>1</sub>, a less conservative statistical test can be performed.

<br>

<img width="452" alt="스크린샷 2025-02-02 오후 8 41 51" src="https://github.com/user-attachments/assets/1acb26ef-825d-4d2d-b77e-cc13a37ddab8" />

<br>

>> ○ **Method 1.** Benjamini–Hochberg (B&H): Suitable when the correlations among tests are simple.

>>> ○ **Step 1.** Order the p-values, obtaining P<sub>(1)</sub> ≤ ⋯ ≤ P<sub>(m)</sub>.

>>> ○ **Step 2.** Let R denote the largest r such that P<sub>(r)</sub> ≤ rα / m.

>>> ○ **Step 3.** If R > 0, reject H<sup>(1)</sup>, ⋯, H<sup>(R)</sup>, where H<sup>(i)</sup> is associated with P<sub>(i)</sub>.

<br>

<img width="254" alt="스크린샷 2025-04-18 오전 12 10 09" src="https://github.com/user-attachments/assets/fb8e9384-ce63-4b74-9cb5-344a25c4d539" />

<br>

>>>> ○ Like the Hotchberg procedure, this is a step-up procedure (starts from the least significant p-value) but the thresholds are much different.

>>>> ○ Hotchberg compares P<sub>(j)</sub> with α / (m - j + 1).

>>>> ○ Benjamini-Hochberg compares P<sub>(j)</sub> with jα / m.

>>> ○ An Intuitive Understanding of FDR Proof at Significance Level α

>>>> ○ Assuming Independence Among Statistical Tests

<br>

<img width="452" alt="스크린샷 2025-04-18 오전 12 13 01" src="https://github.com/user-attachments/assets/0c58c0b0-d653-4465-ab53-3c4aa475dba5" />

<br>

>>> ○ Adjusted p value

<br>

<img width="277" alt="스크린샷 2025-02-02 오후 8 42 20" src="https://github.com/user-attachments/assets/32d0f259-df41-4b72-9dbc-579621e4f433" />

<br>

>>>> ○ d: Number of statistical tests

>>>> ○ rank: Sorting order of p-values

>>>> ○ Note: If the adjusted p-value exceeds 1, it is forcibly set to 1.

>>>> ○ The lower the rank (e.g., rank = 1), the lower the p-value should be. If this condition is not met, there is a step to adjust it.

>>> ○ Example: For significance level α, total tests m, and the i-th smallest p-value p<sub>(i)</sub>

<br>

<img width="261" alt="스크린샷 2024-11-26 오후 3 42 05" src="https://github.com/user-attachments/assets/79e92443-7d9b-4b48-b514-1780d13c14c2">

<br>

| Gene | p-val | Rank | Initial Adj p-val         | Final Adj p-val |
|------|-------|------|---------------------------|-----------------|
| A    | 0.039 | 3    | 0.039 × (25/3) = 0.325    | 0.21            |
| B    | 0.001 | 1    | 0.001 × (25/1) = 0.025    | 0.025           |
| C    | 0.041 | 4    | 0.041 × (25/4) = 0.256    | 0.21            |
| D    | 0.042 | 5    | 0.042 × (25/5) = 0.21     | 0.21            |
| E    | 0.008 | 2    | 0.008 × (25/2) = 0.1      | 0.1             |
| ...  | ...   | ...  | ...                       | ...             |

<br>

**Table 1.** Example of B&H Test with 25 Genes

<br>

>> ○ **Method 2.** Benjamini–Yekutieli (B&Y): Suitable for cases with complex correlations among tests.

>>> ○ Whether the tests are independent or not, Benjamini-Yekutieli controls the FDR at α.

>>> ○ Adjusted p value 

<br>

<img width="354" alt="스크린샷 2024-11-26 오후 3 43 44" src="https://github.com/user-attachments/assets/07c860a5-f7f4-429f-9bae-87f792cd3120">

<br>

>>>> ○ d: Number of statistical tests

>>>> ○ rank: Sorting order of p-values

>>>> ○ ∑<sub>i=1</sub><sup>d</sup> i/1: Adjusted constant to more conservatively control FDR by accounting for test correlations.

>>>> ○ Note: If the adjusted p-value exceeds 1, it is forcibly set to 1.

> ④ Adjusted p-value: Introduced to apply the same significance level α across different correction methods.

<br>

<img width="602" alt="스크린샷 2025-04-18 오전 12 15 17" src="https://github.com/user-attachments/assets/b95987f3-d55c-43b5-9399-f39043a5b669" />

<br>

<br>

## 5. Types of Statistical Tests

⑴ Overview 

> ① [Summary for Statistical Test](https://jb243.github.io/pages/1662)

> ② [Simple Testing](https://jb243.github.io/pages/1634) 

⑵ **Type 1.** One sample categorical 

> ① Summary Statistic: table

> ② Visualization: Bar chart (= bar plot), Pie chart

> ③ **1-1.** [Chi-squared goodnees-of-fit test](https://jb243.github.io/pages/1727#1-chi-square-goodness-of-fit-test) 

> ④ **1-2.** [Likelihood Ratio Test](https://jb243.github.io/pages/614) 

> ⑤ **1-3.** [Run Test](https://jb243.github.io/pages/1689) 

> ⑥ **1-4.** Simulation: Monte Carlo simulation (e.g., Permutation) 

⑶ **Type 2.** Multi sample categorical 

> ① Summary Statistic: Contingency table

> ② Visualization: Segmented bar plot, Side-by-side barplot 

> ③ **2-1.** [Chi-squared goodnees-of-fit test](https://jb243.github.io/pages/1727#1-chi-square-goodness-of-fit-test) 

> ④ **2-2.** [Chi-squared independence test](https://jb243.github.io/pages/1727#2-chi-square-test-of-independence) 

> ⑤ **2-3.** [Fisher's exact test (hypergeometric test)](https://jb243.github.io/pages/1690)

> ⑥ **2-4.** Simulation: Monte Carlo simulation, Parametric bootstrap

⑷ **Type 3.** One sample numerical

> ① Summary Statistic: Location, Scale

>> ○ Location: Mean, Median, Quantile, etc.

> ○ Scale: Standard deviation, Median absolute deviation, etc.

> ② Visualization: Boxplot, Histogram, Q-Q plot (normality check)

> ③ **3-1.** T test

> ④ **3-2.** [Chi-squared goodnees-of-fit test](https://jb243.github.io/pages/1727#1-chi-square-goodness-of-fit-test): Using data binning

> ⑤ **3-3.** [Kolmogorov-Smirnov test](https://jb243.github.io/pages/2310) 

> ⑥ **3-4.** Simulation: Monte Carlo simulation, Nonparametric bootstrap, Parametric bootstrap 

⑸ **Type 4.** Two sample numerical

> ① Visualization: Side-by-side box plot, Q-Q plot (normality check)

> ② **4-1.** Paired t-test: One-sample. Parametric 

> ③ **4-2.** Unpaired t-test with equal variance: Two-sample. Parametric

> ④ **4-3.** Unpaired t-test with unequal variance (Welch t-test): Two-sample. Parametric

> ⑤ **4-4.** [Wilcoxon signed rank test](https://jb243.github.io/pages/2099#3-wilcoxon-signed-rank-test): One-sample. Non-parametric

> ⑥ **4-5.** [Wilcoxon rank-sum test](https://jb243.github.io/pages/2099#2-wilcoxon-rank-sum-test): Two-sample. Non-parametric

> ⑦ **4-6.** [Kolmogorov-Smirnov two-sample test](https://jb243.github.io/pages/2310#4-kolmogorov-smirnov-two-sample-test): Two-sample. Non-parametric

> ⑧ **4-7.** Simulation: Monte Carlo simulation (e.g., Permutation), Bootstrap

⑹ **Type 5.** Multiple sample numerical

> ① Visualization: Side-by-side box plot 

> ② **5-1.** [One-way ANOVA](https://jb243.github.io/pages/1635#2-one-way-anova): Parametric 

>> ○ Assumption: iid, normality, homoscedasticity (but it is not applied to Welch ANOVA F-test)

>> ○ Visualization: residual plot (homoscedasticity check), Q-Q plot (normality check)

> ③ **5-2.** Tukey's honest significant difference 

>> ○ Assumption: Normality, Homoscedasticity (but it is not applied to Welch ANOVA F-test)

> ④ **5-3.** [Kruskal-Wallis test](https://jb243.github.io/pages/1688): Non-parametric 

> ⑤ **5-4.** [Friedman test](https://jb243.github.io/pages/1688#3-friedman-test) 

> ⑥ **5-5.** [Two-way ANOVA](https://jb243.github.io/pages/1635#3-two-way-anova)

>> ○ Visualization: Side-by-side boxplot, Residual plot (homoscedasticity check), Interaction plot

> ⑦ **5-6.** Permutation test 

⑺ **Type 6.** Bivariate paired numerical

> ① Summary Statistic: Correlation coefficient

> ② Visualization: Scatter plot

> ③ **6-1.** Pearson correlation

> ④ **6-2.** Spearman correlation

> ⑤ **6-3.** Kendall's tau correlation

> ⑥ **6-4.** [Cochran-Mantel-Haenszel (CMH) test](https://jb243.github.io/pages/2423)

> ⑦ **6-5.** Kolmogorov-Smirnov independence test

> ⑧ **6-6.** Monte Carlo simulation (e.g., Permutation)

⑻ **Type 7.** Simple regression

> ① Visualization: Scatter plot

> ② **7-1.**T test

> ③ **7-2.** Simulation: Nonparametric bootstrap, Parametric bootstrap 

<br>

--- 

*Input: 2019.06.19 14:52*
