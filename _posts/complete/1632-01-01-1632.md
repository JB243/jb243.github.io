## **Chapter 16. Linear Regression Analysis**

Higher category: 【Statistics】 [Statistics Overview](https://jb243.github.io/pages/1641)

---

**1.** [regression analysis](#1-regression-analysis)

**2.** [simple linear regression model](#2-simple-linear-regression-model)

**3.** [multiple linear regression model](#3-multiple-linear-regression-model)

---

<br>

## **1. regression analysis** 

⑴ regression analysis**:** representing a particular variable as a dependency of one or multiple other variables

> ① more precisely, y ~ X (assuming y ∈ ℝ) 

>> ○ included in a supervised algorithm

>> ○<span> (note) classification **:** y ~ X (assyming | { y } | ＜ ∞ ) </span>

> ② Specific variables: The name dependent variable is representative, but there are several names

>> ○ Response variable 

>> ○ Outcome variable

>> ○ Target variable

>> ○ Output variable

>> ○ Predicted variable

> ③ Other variables: The name independent variable is representative, but there are several names

>> ○ Experimental variables

>> ○ Explanatory variable

>> ○ Predictor variable
 
>> ○ Regressor

>> ○ Covariate

>> ○ Controlled variable

>> ○ Manipulated variable

>> ○ Exposure variable

>> ○ Risk factor

>> ○ Input variable

>> ○ Feature

⑵ (comparison) cross-analysis and analysis of variance 

> ① regression analysis**:** independent variables are measurable variables. dependent variables are measurable variables

>> ○ regression analysis shows the causal relationship of an independent variable to a dependent variable

>> ○ the proof of actual causality is not required because the purpose is the prediction itself 

>> ○ example **:** the length of an uncalcified bone for a 6-year-old child predicts additional height to grow, but is not causal

> ② cross analysis**:** independent variables are categorical (classified) variables. dependent variables are categorical (classified) variables

>> ○ cross analysis is simply a representation of the correlation between variables

> ③ analysis of variance**:** independent variables are categorical (classified) variables. dependent variables are measurable variables

⑶ simple regression analysis and multiple regression analysis

> ① simple regression analysis**:** regression with one independent variable

> ② multiple regression analysis**:** regression with more than one independent variables

⑷ Variable Selection Methods

> ① Forward Selection

>> ○ Step 1. Start with a constant model that only includes the intercept

>> ○ Step 2. Sequentially add independent variables considered important to the model

> ② Backward Elimination

>> ○ Step 1. Start with a model that includes all candidate independent variables

>> ○ Step 2. Remove variables one by one, starting with the one that has the least impact based on the sum of squares

>> ○ Step 3. Continue removing independent variables until there are no more statistically insignificant variables

>> ○ Step 4. Select the model at this stage

> ③ Stepwise Method

>> ○ Step-by-Step Addition: If the importance of existing variables weakens due to the addition of a new variable, remove the affected variable

>> ○ Stepwise Elimination: Review which variables are removed and stop when there are no more to remove

⑸ Model Selection Criteria

> ① Overview

>> ○ Method of penalizing the complexity of the model

>> ○ Calculate AIC and BIC for all candidate models and select the model with the minimum value

> ② AIC (Akaike Information Criterion)

>> ○ AIC = -2 ln (L) + 2p (where ln (L) is model fit, L is the likelihood function, p is the number of parameters)

<br>

<img width="268" alt="스크린샷 2025-06-07 오후 1 48 18" src="https://github.com/user-attachments/assets/590aeb3d-685d-499f-b43e-8fd21d3e1e50" />

<br>

>> ○ Purpose: Since models with many parameters tend to overfit, penalize in proportion to the number of parameters.

>> ○ An indicator showing the difference between the actual data distribution and the distribution predicted by the model

>> ○ Lower values indicate better model fit

>> ○ Becomes less accurate as the sample size increases

> ③ BIC (Bayesian Information Criterion)

>> ○ BIC = -2 ln (L) + p ln n (where ln (L) is model fit, L is the likelihood function, p is the number of parameters, n is the number of data points)

>> ○ Compensates for the inaccuracy of AIC as sample size increases

>> ○ Penalizes more complex models more strongly as sample size increases

> ④ AIC<sub>c</sub> 

>> ○ AIC<sub>c</sub> = AIC + 2K(K+1) / (N-K-1), where N is the number of samples 

>> ○ Purpose: To address the issue that AIC becomes less accurate as the sample size increases.

<br>

<br>

## **2. simple linear regression model** 

⑴ definition**:** the case of a simple regression analysis in which the dependency is shown as a linear function 

⑵ representation of data

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/sZKZB/btruiQPFzBX/qyTEL1NQhe15R2fP8ICOs1/img.jpg" alt="drawing" /></center><br>

<center><b>Figure. 1.</b> simple linear regression model</center>

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/cijkyq/btruiQhRJoS/OmH44OLyVoN1hf8WkQZ4p0/img.png" alt="drawing" /></center><br>

> ① β<sub>0</sub> **:** y intercept 

> ② β<sub>1</sub> **:** slope or coefficient on X 

>> ○ also called parameter, regression coefficient, weight, etc 

>> ○ intuitively, elasticity means the degree to which the absolute value of the slope is large

>> ○ in microeconomics, elasticity means the slope multiplied by (-1).

> ③ types of regression lines

>> ○ population regression line**:** regression line obtained from the characteristics of the population

>> ○ fitted regression line**:** regression line obtained from the characteristics of the sample

> ④ u<sub>i</sub> **:** residual

> ⑤ difference between residual and error

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/Tk79h/btruyNcrapj/rCn8ERjyjozHJ2sf5Tc6a0/img.png" alt="drawing" /></center><br>

>> ○ the errors mentioned in the future actually mean residuals

> ⑥ characteristic of variance 

>> ○ **homoscedasticity****:** <span>VAR(u<sub>i</sub> | X<sub>i</sub>) and Xi are independent. an impractical assumption. default setting for many statistical programs</span>

>> ○ **heteroscedasticity **:** <span>VAR(u<sub>i</sub> | X<sub>i</sub>) depends on X<sub>i</sub></span>

>> ○ a model with homoscedasticity is a good model 

⑶ assumptions 

> ① **assumption** **1\.** X<sub>i</sub> does not provide any information about the error

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/c1sKBQ/btruqFUtWvo/kwnhIsfmKvZ3hRbKNI6emK/img.png" alt="drawing" /></center><br>

>> ○ if there is a pattern on a residual plot, the model is not a good model 

> ② **assumption 2.** (X<sub>i</sub>, y<sub>i</sub>) is i.i.d. 

> ③ **assumption 3.** the existence of 4<sup>th</sup> order moment

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/cgTx9Q/btrurx86IZI/diOuVnjYlxtoK80BSgdTF0/img.png" alt="drawing" /></center><br>

⑷ induction of the fitted regression line  

> ① **method 1.** method of [moment](https://jb243.github.io/pages/1630) estimator (MOM) or sample analog estimation 

>> ○ calculation process

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/dgmehi/btruhvSyyvu/Z88P2NfW3kbOXpjaRGmND1/img.png" alt="drawing" /></center><br>

> ② **method** **2.** method of least squares or ordinary least squares (OLS)

>> ○ definition**:** calculate the minimum value of sum of squares of the errors (SSE)

>> ○ provided by all statistical softwares 

>> ○ calculation process **:** if X<sub>i</sub> is one-dimensional 

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/cynYnX/btruoK2bqqB/TaR0SHeoqqlZSM4hMNPn01/img.png" alt="drawing" /></center><br>

>> ○ method of least squares is based on [maximum likelihood estimation](https://jb243.github.io/pages/1630) (assuming the residual has homoscedasticity and normality)

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/YluVP/btruxjbBa01/YtLh6dmTvD6go9KjKJMwsK/img.jpg" alt="drawing" /></center><br>

>> ○ the regression of X to Y and the regression of Y to X are generally not the same

>>> ○ E(X<sub>2</sub>), E(XY), E(X), ect are involved in the regression of X to Y

>>> ○ E(Y<sub>2</sub>), E(XY), E(Y), etc are involved in the regression of Y to X

>>> ○ E(X<sub>2</sub>), E(Y<sub>2</sub>), ect make the asymmetry

> ③ **Method 3.** [Cross entropy](https://jb243.github.io/pages/2145) 

>> ○ general definition 

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/cd8zj0/btruvtS8yXW/dJLzSxR8kX1qww0X38Xilk/img.png" alt="drawing" /></center><br>

>> ○ binary classification

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/bQXDyL/btrukfhuLA9/isB6TM156Clmbq3pxgUZ71/img.png" alt="drawing" /></center><br>

>> ○ if y is represented as one-hot vector \[0, ···, 1, ···, 0\], the following is established 

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/cvIT9l/btruyMLmmaX/c9Kd54kuIjstZWHKSaPjm0/img.png" alt="drawing" />

<br>

⑸ [Characteristics](https://jb243.github.io/pages/1630) of regression line 

> ① Overview

<br>

![image](https://github.com/user-attachments/assets/f097999c-65c8-4b0e-a1d2-7ae6ca63bd1e)

<br>

> ② Unbiasedness

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/boCSdb/btruwHXFQgG/SKj7yGvQtDqVV8fYbr3Jbk/img.png" alt="drawing" />

<br>

> ③ Efficiency

>> ○ Gauss-Markov theorem**:** OLS is efficient when homoscedasticity is satisfied  

> ④ Consistency

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/bb3UA4/btrusNYQdme/dzsahz21F4tGqTTQjne700/img.png" alt="drawing" />

<br>

> ⑤ Asymptotic normality

>> ○ slope

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/bWn9P7/btrup8IvvHc/mLpVbL9mRsLE8CZwA7rMd0/img.png" alt="drawing" /></center><br>

>> ○ y intercept - heteroscedasticity-robust standard error

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/3QBdc/btrutlN5E0O/SbyAV8GfYvLlaNwPpnGtI0/img.png" alt="drawing" /></center><br>

>> ○ y intercept - homoscedasticity-robust standard error

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/okdsC/btruoKHThbc/v42FgCjckaB7NxNskSNN0k/img.png" alt="drawing" /></center><br>

⑹ evaluation of the regression line

> ① **criterion 1.** linearity 

> ② **criterion 2.** homoscedasticity **:** residual terms having equal variances

> ③ **criterion 3.** normality **:** residual terms following normal distribution

>> ○ Box-Cox **:** In cases where it is difficult to assume normality in linear regression models, this method transforms the dependent variable to be closer to a normal distribution.

⑺ coefficient of determination **:** also called R-squared

> ① coefficient of determination R2

>> ○ definition

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/ACb4x/btrusOKbHXb/H8LZyavZaMPrS6Ip70KkzK/img.jpg" alt="drawing" /></center><br>

>>> ○ SST **:** total variation

>>> ○ SSR **:** variation for regression equation

>>> ○ SSE **:** variation due to error

>>> ○ SSE is also called residual sum of squares (RSS), sum of squared residuals (SSR)

>>> ○ reason why the term <span style="color=purple">■</span> is 0**:** because covariance of bias and chance error is intuitively 0

>> ○ meaning

>>> ○ **meaning 1.** the proportion of the variance of Y that X can describe (no units)

>>> ○ **meaning 2.** the sum of squares described by the regression line ÷ the total sum of squares

> ② the coefficient of determination is the same as **the square of the correlation coefficient**

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/bz5sFZ/btruiQCbsu8/NyVU33vYXocMdR0MA5lwO1/img.png" alt="drawing" /></center><br>

> ③ fraction of the variance unexplained (FVR)

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/cJxPjI/btruomgRtgm/JpHYHPheUpJuZZGWYl8C3k/img.png" alt="drawing" /></center><br>

> ④ characteristics 

>> ○ 0 ≤ R<sup>2</sup> ≤ 1

>> ○ the closer R<sup>2</sup> is to 1, the better the goodness of fit of regression line is

>> ○ estimator of β<sub>1</sub> = 0 ⇒ R<sup>2</sup> = 0

>> ○ R<sup>2</sup> = 0 ⇒ estimator of β<sub>1</sub> = 0 or X<sub>i</sub> = constant

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/cxNYEg/btrukgHm3yR/Hhb8H1b1CNvO0hlkBr2JZ1/img.png" alt="drawing" /></center><br>

⑻ average error regression

> ① formula for SSE 

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/l6oEv/btruiQCbsAP/foXthIxhuPg0ndDr8ryKhK/img.png" alt="drawing" /></center><br>

> ② the expected value of SSE

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/b9DQad/btruxjWWPAn/kTU4ty0G0RZ9CrnmwH78OK/img.png" alt="drawing" /></center><br>

>> ○ total degree of freedom = degree of freedom of residual + degree of freedom of regression line

>> ○ total degree of freenom = n-1

>> ○ degree of freedom of regression line = 1 (**∵** there is only one regression variable)

>> ○ degree of freedom of residual = n-2 

> ③ Mean squared error (MSE)

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/wgFGy/btrutlm10Cr/tgtKUEvQTD15upeZcsF561/img.png" alt="drawing" />

<br>

> ④ Standard error regression (SER)

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/bOvs2Y/btruuhrPcIc/8k7U8eC7qwqKQRESH1dU10/img.png" alt="drawing" />

<br>

> ⑤ SSE and unbiased estimator of variance

<br>

<img width="506" alt="스크린샷 2025-04-11 오후 4 49 28" src="https://github.com/user-attachments/assets/01eac8d9-7eba-4d96-aeac-250a6c41bebd" />

<br>

⑼ **Example 1.** the etymology of regression

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/czRTat/btruiRA0W4x/7hTyaqj5ksYA0hBH6iK41K/img.jpg" alt="drawing" />

<br>

<b>Figure 2.</b> the etymology of regression

<br>

> ① X **:** a father's height

> ② Y **:** a son's height

> ③ E(X) = 67.7, E(Y) = 68.7, σ<sub>X</sub> = 2.7, σ<sub>Y</sub> = 2.7, ρ<sub>XY</sub> = 0.5 

> ④ <span>E(Y | X = 80) = 74.85</span>

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/ceGnse/btrupHZptl4/uaHcIGIXL7eLCgq8nPkdJ1/img.jpg" alt="drawing" /></center><br>

> ⑤ <span>E(Y | X = 60) = 64.85</span>

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/85zQc/btrulR8qm9H/oi0DZnUhi88ZqgsvmWo9a1/img.jpg" alt="drawing" /></center><br>

> ⑥ conclusion

>> ○ the son of a tall father tends to get shorter

>> ○ the son of a short father tends to grow taller

>> ○ finally, the son's height tends to return to the average

>> ○ however, since the above tendency is only based on expected value, the variance of sons' generation's height is not necessarily lower than the variance of fathers' generation's height 

⑽ **example 2.** predicting Y-values outside the range of independent variables: also called extrapolation

> ① generally, it is unadvisable to use extrapolation 

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/m6Vst/btruokQWyjX/oD2NnqfFQHdrIrOPxwOlH1/img.png" alt="drawing" />

<b>Figure 3.</b> problems of extrapolation

<br>

> ② extrapolation methodology is not always wrong

>> ○ example**:** research on biological evolution

⑾ [Example problems for linear regression and bivariate normal distribution](https://blog.kakaocdn.net/dn/JMcdz/btsLJABUa7u/kWPry4j0RkcYiqBLKr2vM0/%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%2016%E1%84%8C%E1%85%A6.pdf?attach=1&knm=tfile.pdf)

⑿ Python code

<br>

<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #008800; font-weight: bold">from</span> <span style="color: #0e84b5; font-weight: bold">sklearn</span> <span style="color: #008800; font-weight: bold">import</span> linear_model 
reg <span style="color: #333333">=</span> linear_model<span style="color: #333333">.</span>LinearRegression() 
reg<span style="color: #333333">.</span>fit([[<span style="color: #0000DD; font-weight: bold">0</span>, <span style="color: #0000DD; font-weight: bold">0</span>], [<span style="color: #0000DD; font-weight: bold">1</span>, <span style="color: #0000DD; font-weight: bold">1</span>], [<span style="color: #0000DD; font-weight: bold">2</span>, <span style="color: #0000DD; font-weight: bold">2</span>]], [<span style="color: #0000DD; font-weight: bold">0</span>, <span style="color: #0000DD; font-weight: bold">1</span>, <span style="color: #0000DD; font-weight: bold">2</span>])
<span style="color: #888888"># LinearRegression() </span>
reg<span style="color: #333333">.</span>coef_ 
<span style="color: #888888"># array([0.5, 0.5])</span>
</pre></div>

<br>

<br>

## **3. multiple linear regression model**  

⑴ definition**:** the case of a multiple regression analysis in which the dependency is shown as a linear function  

⑵ omitted variable bias 

> ① definition **:** a phenomenon in which the expected value of the error is not zero due to omitted variables

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/bBctTY/btruuiK3jMl/zXH1DzE8uFq8rJwuERzbp0/img.png" alt="drawing" /></center><br>

>> ○ endogenous variable**:** variables correlated with ui  

>> ○ exogenous variable**:** variables uncorrelated with ui  

> ② **condition 1.** omitted variables and regressor (_e.g._, X<sub>i</sub>) should have correlation

> ③ **condition** **2.** omitted variables should be determinators of Y

> ④ the convergent value of the slope

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/SiVap/btruuiYxFfD/FHRKEqtrR7bJSkJTwRFLjK/img.png" alt="drawing" /></center><br>

>> ○ ρ<sub>Xu</sub> ＞ 0 **:** upward bias

>> ○ ρ<sub>Xu</sub> ＜ 0 **:** downward bias

> ⑤ if the value of the coefficient changes significantly when a new variable is added, it can be said that there is omitted variable basis

⑶ representation of data

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/ZrchP/btrusOXJbPy/AvQEhfc07dkWtuXXb1OL6k/img.png" alt="drawing" /></center><br>

> ① unbiasedness, consistency, and asymptotically jointly normal are observed in the above estimators 

> ② robustness **:** a characteristic that adding a new regressor does not significantly change any slope value of a regressor 

> ③ sensitivity **:** a characteristic that adding a new regressor significantly changes the slope value of a particular regressor 

⑷ assumptions

> ① **assumption** **1.** error is not explained by X<sub>1i</sub>,, ···, X<sub>ki</sub>

> ② **assumption** 2.** (X<sub>1i</sub>, ···, X<sub>ki</sub>,Y<sub>i</sub>) is i.i.d.  

> ③ **assumption 3.** existence of 4th order moment

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/3yOF5/btruuhZFdBj/oA3RnsKBuDd2ZuOjjfBDS0/img.png" alt="drawing" /></center><br>

> ④ **assumption 4.** no perfect multicollinearity 

>> ○ multicollinearity **:** a characteristic that the linear combination of one independent variable and another independent variable is highly correlated

>>> ○ (note) multiple linear regression model expects independent variables to be truly independent 

>> ○ perfect multicollinearity**:** if one regressor has perfect linearity with the other regressors. the determinant value = 0

>>> ○ perfect multicollinearity is not the nature of a variable, but the nature of a data set

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/6qSEL/btruvtS8BtR/TTSE8dWjZYzdxdZWpRQn4K/img.png" alt="drawing" /></center><br>

>>> ○ when you attempt a regression analysis on perfect multicollinear data, the number of possible coefficients is infinite**:** **impossible** to perform regression analysis 

>> ○ imperfect multicollinearity **:** two or more regressors are just highly correlated

>>> ○ not a problem at once

>>> ○ the variance of the slope estimator is quite large → difficult to trust the slope estimator

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/chFfWK/btrupHLP76O/o8dD1PvS7hRcARONjFuoH0/img.png" alt="drawing" /></center><br>

<center><b>Figure. 4.</b> the reason why multi-collinearity increases the variance of the slope estimator</center>

<center>⒝ a variety of planes may exist within the significance interval</center>

<br>

>>> ○ in general, a pair of variables should not have a correlation of more than 0.9

>> ○ solution

>>> ○ draw pairwise plots of all combinations and remove highly correlated variables

>>> ○ PCA, weighted sum, etc. may be attempted, but each has its own shortcomings

>> ○ (note) R Studio randomly ignores the last of the problematic terms when analyzing perfect multicollinearity data

⑸ OLS estimator**:** determine the coefficient by calculating the following simultaneous equations 

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/nIKy5/btruxkn3Btx/zky6kzyIRiKrhEY0Puekjk/img.png" alt="drawing" /></center><br>

⑹ characteristics of the regression line

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/c8J2a5/btruxjbBkgI/eXKOiB63Kf7RVPGbITlfw0/img.png" alt="drawing" /></center><br>

> ① unbiasedness

> ② consistency 

> ③ asymptotically jointly normality

> ④ Frisch-Waugh theorem 

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/RR4jG/btruxkVQ5eU/8R4xFd02dQFB4vjbndrDLK/img.png" alt="drawing" /></center><br>

⑺ adjusted R<sup>2</sup>

> ① drawbacks of R<sup>2</sup> **:** the degree of fitting is not well reflected in multiple regression model

>> ○ **drawback 1.** R<sup>2</sup> always increases whenever a new regressor is added because the minimum value of SSE is reduced

>> ○ **drawback 2.** high R<sup>2</sup> does not verify the absence of omitted variable bias  

>> ○ **drawback 3****.** high R<sup>2</sup> does not verify the current regressors are optimal 

>> ○ to resolve **drawback 1**, adjusted R2 is introducted

> ② formula

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/nu9lX/btrupHrtUBG/e8JzC4Q1MKPW5Gg9zcehlk/img.png" alt="drawing" /></center><br>

> ③ characteristic 

>> ○ adjusted R<sup>2</sup> ≤ R<sup>2</sup>

>> ○ adjusted R<sup>2</sup> can be negative

>> ○ The value decreases as inappropriate variables are added

⑻ standard error regression (SER)**:** k is the number of independent variables in the regression equation

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/nNrO6/btrupJv74dZ/nxSkMFzIp2d2q44KpKURfK/img.png" alt="drawing" /></center><br>

⑼ joint hypothesis**:** hypothesis when there are more than or equal to 2 constraints

> ① **idea 1.** t1 and t2 are independent

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/b8TqH8/btruokDlbtu/EVJEOJutd409fRTJHBVQk0/img.png" alt="drawing" /></center><br>

② **idea 2.** t<sub>1</sub> and t<sub>2</sub> have multicollinearity

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/PosAK/btrupIcQh5s/vanaA3w9fAd1Vxy30Pb5FK/img.png" alt="drawing" /></center><br>

> ③ general case

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/c31GQE/btrusPbgpS1/3UvyXatamtsMreKe5jybC0/img.png" alt="drawing" /></center><br>

>> ○ in general, heteroscedastic-robust F-statistics is used

>> ○ many statistical programs take homoscedastic-robust F-statistics as default setting

> ④ null hypothesis 

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/btXHw7/btruok4pD21/HTJgHjUpJVZR4yFZjgxxU1/img.png" alt="drawing" /></center><br>

⑽ redefinition of multiple linear regression model 

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/oULUo/btruolWxF5k/0lZgvKViebgkeqjPangrDk/img.png" alt="drawing" /></center><br>

> ① H<sub>0</sub> **:** if you want to test β<sub>1</sub> = β<sub>2</sub>,  

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/bl3OeN/btruvt6BCUh/2TVTBePtmAPsCWBKmpA5C0/img.png" alt="drawing" /></center><br>

> ② H<sub>0</sub> **:** if you want to test β<sub>1</sub> + β<sub>2</sub> = 1, 

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/6pKSd/btruyOCqHsl/Fid7PrAZp0raeSZ4Xj5pV1/img.png" alt="drawing" /></center><br>

⑾ conditional mean independence

> ① definition

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/dWcbsF/btruuic9o6t/B7yySIothueoHTy43rT7d1/img.png" alt="drawing" /></center><br>

> ② X<sub>1i</sub> is not correlated with ui for given X<sub>2i</sub> 

> ③ β<sub>2</sub> may not have consistency**:** but it is not important 

⑿ matrix notation

> ① linear regression model

>> ○ for a scalar Y, column vector X, and β, 

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/chIJYG/btruyOblLQS/KhkkJUMLg0OiBlYo5C2wk1/img.png" alt="drawing" /></center><br>

>> ○ generalization

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/crQLrc/btrusN5CvIS/ravPQE4gCf7YXvYDaaJMIk/img.png" alt="drawing" /></center><br>

> ② assumption

>> ○ **assumption 1.** E(u<sub>i</sub> | X<sub>i</sub>) = 0 

>> ○ **assumption 2.** (X<sub>i</sub>, Y<sub>i</sub>), i = 1, ···, n is i.i.d. 

>> ○ **assumption 3.** X<sub>i</sub> and u<sub>i</sub> have nonzero finite fourth moment 

>> ○ **assumption** **4.** 0 ＜ E(X<sub>i</sub>X<sub>i</sub><sup>t</sup>) ＜ ∞, no perfect multicollinearity 

> ③ OLS modeling - a simple version

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/ciffIj/btrusPh1yPr/8TDkB6RWHzhLGBgMK2FABk/img.jpg" alt="drawing" /></center><br>

> ④ OLS modeling

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/di1fx6/btrusNEuzrH/p3ct8xHDmN7NcOd7H9BFa0/img.jpg" alt="drawing" /></center><br>

> ⑤ consistency

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/qlY2T/btrusNYQpz5/DJLqmTK8jEkYXWUscuwcRK/img.png" alt="drawing" /></center><br>

> ⑥ multivariate central limit theorem

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/yHscL/btruyNwJT03/b0wQKJPZCozRGnBAClwdSK/img.png" alt="drawing" /></center><br>

> ⑦ asymptotic normality

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/PgJrN/btrupIYia6G/pKsF33ydYfSCbKzXsRemsk/img.png" alt="drawing" /></center><br>

> ⑧ robust standard error (Eicker-Huber-White standard error)

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/bQa8mM/btrusPbgtk0/pqXIwzWx9L52769DX9RgI1/img.png" alt="drawing" /></center><br>

> ⑨ robust F

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/b6cnEv/btruolPLQli/GJdHH19uK7KB8RePsRk3k0/img.png" alt="drawing" /></center><br>

---

<br>

*Input: 2019.06.20 23:26*
