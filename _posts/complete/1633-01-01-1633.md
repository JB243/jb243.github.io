## **Chapter 17. Non-linear Regression Analysis** 

Higher category**:** 【Statistics】 [Statistics Overview](https://jb243.github.io/pages/1641)

---

**1.** [quadratic regression model](#1-quadratic-regression-model)

**2.** [polynomial regression model](#2-polynomial-regression-model)

**3.** [logarithm regression model](#3-logarithm-regression-model)

**4.** [probability model](#4-probability-model-the-case-of-dependent-variable-being-a-binary-variable)

**5.** [interaction](#5-interaction)

---

<br>

## **1. quadratic regression model** 

⑴ formula 

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/bwg7vV/btruBbqC3Rt/eIwvk2GCPy4OuGQcabqbg1/img.png" alt="drawing" /></center><br>

⑵ determination of coefficients 

> ① [multiple linear regression model](https://jb243.github.io/pages/1632#3-multiple-linear-regression-model) is used **:** consider X<sub>i</sub> and X<sub>i</sub><sup>2</sup> as different variables and interpret them 

> ② it is possible because Xi and Xi2 do not have perfect multi-collinearity

⑶ linearity test 

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/dA2F5C/btruxjXetVJ/dLALbce9aNOWKuVDBL9CPk/img.png" alt="drawing" /></center><br>

⑷ confidence interval of the amount of change 

> ① effect **:** the effect of Y by a unit change of X is as follows 

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/wQ1tl/btruxjv8pRL/LMlpCVABkMdWs2ock9WoQk/img.png" alt="drawing" /></center><br>

> ② marginal effect

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/cFGEPu/btruBcQBG6C/Uk9cFRRgJMH3KBVTkRwbw0/img.png" alt="drawing" /></center><br>

> ③ standard deviation of the amount of change

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/9gZzX/btrup7XbuUJ/GG5aUByafLZdTqgjqg1vRK/img.png" alt="drawing" /></center><br>

> ④ confidence interval of the amount of change

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/cu0g0W/btrurw3xiyC/i8u7fr3nDWRB2KVXFTjOq0/img.png" alt="drawing" /></center><br>

<br>

<br>

## **2. polynomial regression model** 

⑴ general equation

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/bjRNiH/btruBa6kDKW/Mfmg5cEcBreqZsRoBl1f50/img.png" alt="drawing" /></center><br>

⑵ determination of coefficients **:** [multiple linear regression model](https://jb243.github.io/pages/1632#3-multiple-linear-regression-model) is used

⑶ linearity test 

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/lqxOp/btruujjajc3/nXgohWGubSKzkO5nfILl4k/img.png" alt="drawing" /></center><br>

⑷ **decision of degree (order) of polynomial 1.** top-down method 

> ① the most commonly adopted method

> ② 1<sup>st</sup>. set the maximum value of r

> ③ 2<sup>nd</sup>. test H0 **:** βr = 0 

> ④ 3<sup>rd</sup>. if H0 is rejected, r is the degree of regression line 

> ⑤ 4<sup>th</sup>. if H0 is not rejected, eliminate Xir and repeat 2nd step for βr-1, ··· 

⑸ **decision of degree (order) of polynomial** **2**. bottom-top method 

> ① a way to see if there is a significant effect on explaining a given sample when a term with a higher order of one level is added

> ② procedure

>> ○ 1<sup>st</sup>. assume that the coefficient for all terms is significant up to the (r-1)-order polynomial of in the botom-top manner  

>> ○ 2<sup>nd</sup>. add a r-order term 

>> ○ 3<sup>rd</sup>. calculate the sum of squares by the r-order regression line (degree of freedom**:** r)

>> ○ 4<sup>th</sup>. subtract the sum of squares by the (r-1)-order regression line (degree of freedom**:** r-1) from the value obtained from 3rd step 

>> ○ 5<sup>th</sup>. calculate the sum of squares by the residual of the r-order regression line (degree of freedom: n-1-r)

>> ○ 6<sup>th</sup>. calculate the mean square by dividing the value obtained from 5th step by (n-1-r)

>> ○ 7<sup>th</sup>. divide the difference of the sum of squares obtained from 4th step by the mean square obtained from 6th step: the degree of freedom of the difference of the sum of squares is 1

>> ○ 8<sup>th</sup>. calculate p value by substituting F statistic obtained in 7th step from F(1, n-1-r)

> ③ example 

>> ○ problem situation

<br>

| model | sum of squares | df | mean square |
| --- | --- | --- | --- |
| linear | 3971.46 | 1 | 3971.46 |
| error | 372515.09 | 18 | 20695.28 |
| quadratic | 367833.58 | 2 | 183916.79 |
| error | 8652.97 | 17 | 509.10 |
| cubic | 369211.71 | 3 | 123070.57 |
| error | 7274.84 | 16 | 454.68 |

<center><b>Table. 1.</b> problem situation</center>

<br>

>> ○ table of results 

<br>

| model | difference of sum of squares | df | sum of squares of residuals | df | mean of sum of squares of residuals | F ratio |
| --- | --- | --- | --- | --- | --- | --- |
| quadratic | 367833.58 | 2 | 8652.97 | 17 | 509.10 | F1,17 \= 714.72 |
| linear | 3971.46 | 1 |   |   |   | p < 0.001 |
| difference | 363862.12 | 1 |   |   |   |   |
| cubic | 369211.71 | 3 | 7274.84 | 16 | 454.68 | F1,16 \= 3.03 |
| quadratic | 367833.58 | 2 |   |   |   | NS |
| difference | 1378.13 | 1 |   |   |   |   |

<center><b>Table. 2.</b> table of results</center>

<br>

> ④ drawbacks**:** sequential type Ⅰ error accumulation is problematic 

>> ○ the study of statistics is to analyze them with a phenomenon that occurs at once

>> ○ it's very difficult to analyze a phenomenon of a certain probability by applying another phenomenon that has already appeared with a different probability

>> ○ difficulty means that the statistic may not follow the F distribution

>> ○ bottom-top method of degree determination is to analyze a phenomenon of a different probability in a phenomenon of a particular probability

>> ○ the phenomenon of a particular probability refers to the (r-1)-order regression equation

>> ○ the phenomenon of a different probability refers to the r-order regression equation 

>> ○ (note) it is impossible to clearly show the difference of sum of squares follows the F distribution 

<br>

<br>

## **3. logarithm regression model** 

⑴ (note) logarithmic approximation

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/8OwV1/btruAr1wYep/hDLK1Md2qdkG3QjO3bsW3K/img.png" alt="drawing" /></center><br>

⑵ **class** **1.** linear-log model

> ① formula

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/EuBhh/btruvuLuo1s/MToq4U02zMFhIVyoHwou11/img.png" alt="drawing" /></center><br>

> ② if X<sub>i</sub> increases by 1%, Y<sub>i</sub> increases by 0.01β<sub>1</sub> 

⑶ **class 2.** log-linear model

> ① formula

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/THmcs/btruArHe8NI/UAlqRksXIhAnsXufLvQdjK/img.png" alt="drawing" /></center><br>

> ② if X<sub>i</sub> increases by 1,Y<sub>i</sub increases by 100β1% 

⑷ **class** ****3**.** log-log model

> ① formula

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/dLL6uB/btruqFN0Zin/tt4j8kCEDsMmlEzNS5KvQK/img.png" alt="drawing" /></center><br>

> ② if X<sub>i</sub> increases by 1%, Y<sub>i</sub> increases by β<sub>1</sub>% 

⑸ we can select the more appropriate model by compairing adjusted R<sup>2</sup> between log-linear model and log-log model

⑹ it's pointless to compare linear-log model with other two models, as the dependent variable of linear-log model differs

<br>

<br>

## **4. probability model****:** the case of dependent variable being a binary variable

⑴ linear probability model (LPM)

> ① formula

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/tSsuj/btrusOcDTTB/KOTEkz1utgRPijKnizzuz0/img.jpg" alt="drawing" /></center><br>

<center><b>Figure. 1.</b> linear probability model</center>

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/niLmi/btruqGlXHSK/B8gnWZQeVqifJ9OCjzvPxk/img.png" alt="drawing" /></center><br>

> ② issue **:** the dependent variable does not always show values between 0 and 1

⑵ **probit regression model** 

> ① overview **:** the most frequently used probability model

> ② formula

>> ○ simple model

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/dczZAk/btrulS0BnDV/iSZr5INjDkOL32hYBsM7KK/img.jpg" alt="drawing" /></center><br>

<center><b>Figure. 2.</b> probit regression model</center>

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/bSkOzb/btrurUqAKa8/RVo9LGlZWBFkNk7OuiHy91/img.png" alt="drawing" /></center><br>

>> ○ multiple model

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/das6JF/btrusODLCxD/hOX9SaZnZWXJEaG8uLCzf0/img.png" alt="drawing" /></center><br>

> ③ effect 

>> ○ formula

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/0LXRR/btrurTL1UGz/bR34QeJ8mt6hyWWlMds7G0/img.png" alt="drawing" /></center><br>

>> ○ marginal effect

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/byG0vm/btruuhFGHZu/13al2y18DMeUICQW9OxHDk/img.png" alt="drawing" /></center><br>

> ④ statistical estimation

>> ○ there is no exact form of function of the estimator of each coefficient **:** find the maximum likelihood estimator through numerical analysis

>> ○ once obtained, the maximum likelihood estimator satisfies consistency and normality

⑶ **logistic regression model**

> ① formula

>> ○ logistic function

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/y85sN/btrurURGNtI/yjlHS7FnSIFNsZzK9hjvv1/img.png" alt="drawing" /></center><br>

>> ○ modeling**:** put the linear regression form of βx + β<sub>0</sub> to logistic function, which is a kind of a linking function

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/9aDE1/btruvuxYAnx/bBNhofpQfxcqB2uvv3xJ60/img.png" alt="drawing" /></center><br>

>> ○ log-odd (logarithmic of odds ratio) **:** also called logit. A logarithmic conversion of the odds ratio. having a value from negative to positive infinity.

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/coXLOw/btruuhljsEK/xzy9HdO24SEyUkKuJ2koak/img.png" alt="drawing" /></center><br>

<center><b>Figure. 3.</b> logit function</center>

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/cu1MkG/btruxkhx2NU/RyHatKYy2Wx9790ZK9IWT1/img.png" alt="drawing" /></center><br>

>> ○ The logistic function is the inverse function of the logit function.

>> ○ The logistic function transforms an input variable that takes values from negative infinity to positive infinity into an output variable that ranges from 0 to 1.

> ② [maximum likelihood estimation](https://jb243.github.io/pages/1630) 

>> ○ assume the independent variable is not ome-dimensional variable of x<sub>i</sub> but multidimensional variable of x<sub>i</sub>, and use the [Bernoulli function](https://jb243.github.io/pages/1626#2-bernoulli-distribution) 

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/0KYZO/btruvtFQ4G4/TIVPmOz6qsKwKeNWNxgOXK/img.png" alt="drawing" /></center><br>

>> ○ definition of likelihood function of L(θ) and log likelihood function of ℓ(θ) **:** here, L(θ) is defined as a [cross-entropy](https://jb243.github.io/pages/2145) 

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/YUpS2/btruxkolAg9/hxHKOBg80BlrSDRhTk2VJ1/img.png" alt="drawing" /></center><br>

>> ○ **lemma :** L(θ) and ℓ (θ) are convex function. the minimal solution is not local solution but global solution. proof is complicated

>> ○ **step 1.** definition of gradient 

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/bf2V0Y/btruqEn17YE/HVEHIRrcBYKKOTtP8EoKwk/img.png" alt="drawing" /></center><br>

>> ○ **step 2.** definition of [Hessian matrix](https://jb243.github.io/pages/1813) 

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/SsWD2/btrurTk1HiW/EEvEEoGhckzzu77Ne5O6oK/img.png" alt="drawing" /></center><br>

>> ○ **step 3.** get the Taylor series for θ<sub>k</sub> for the secondary approximated equation and obtain the maximum solution θ<sub>k+1</sub> = θ<sub>k</sub> + d<sub>k</sub> of the approximated equation

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/0IYW8/btruBcQB0lX/A100XlVnwpxsuMBtnB2Ruk/img.png" alt="drawing" /></center><br>

<center><b>Figure. 4.</b> relationship between maximum likelihood estimation and Taylor series</center>

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/bd7dXg/btruqFUK024/kjVbaNwtZV4RcMJC9FgHa0/img.png" alt="drawing" /></center><br>

>> ○ **step 4.** updating θ<sub>k</sub> in a way of [Newton-Raphson method](https://jb243.github.io/pages/1773) reaches the global maximum 

>>> ○ this is obtained by numerical analysis, but does not have the exact function form of the estimator of each coefficient.

> ③ idea for the proof of consistency (assuming symbols may differ from the above)

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/cDUMKi/btruBciL2NE/b6Gph9kkeAEUomfiQ7eKpk/img.png" alt="drawing" /></center><br>

> ④ idea for the proof of asymptotic normality (assuming symbols may differ from the above) 

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/HFVKL/btruxjv8E10/NUtkkpfqKOpidR7oWe0SFk/img.png" alt="drawing" /></center><br>

> ⑤ application **:** multiclass classification 

>> ○ intoduction**:** as logistic regression is a binary classification, it cannot be applied directly to multiclass classification 

>> ○ **method 1.** performing 1 vs {2, 3} at first, and 2 vs 3 afterward 

>> ○ **method 2. softmax function**

>>> ○ definition

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/W2j7E/btruBcQB1ic/BIgDTZ3JDBrH1xEqHNAPo0/img.png" alt="drawing" /></center><br>

>>> ○ softmax function in multiclass classification

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/bqdjKK/btruvvwSrgO/qqRmyBisvwK0Xip2TbsUD0/img.png" alt="drawing" /></center><br>

>>> ○ proof **:** logistic regression is a special example of softmax function

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/DMEJ3/btruxkaM5vt/Rs1mVN9Z9LfjAXW6pkW841/img.png" alt="drawing" /></center><br>

⑷ Comparison of LPM, probit, and logistic function 

> ① unable to compare coefficients between LPM, probit, and logistic because models are different

>> ○ **example 1.** comparison between probit regression model and logistic regression model

>>> ○ showing very similar plots

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/bQMajB/btruBbc6aoF/5Xku7WPKwrJxVO7iB233N1/img.png" alt="drawing" /></center><br>

<center><b>Figure. 5.</b> comparison between probit regression model and logistic regression model</center>

<br>

>>> ○ the difference in coefficients is very significant**:** there is no mathematical meaning to this difference

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/bikDuu/btruqE2EFzy/23nIMqoyy4JY1gf39XFVx1/img.png" alt="drawing" /></center><br>

⑸ Dirichlet regression model:

> ① Overview **:** Used for regression analysis while considering the topology (simplex) of the data.

<br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcoOMjC%2FbtrUurQV3R4%2FdJB6GulWXRRUvoOfU6HuG1%2Fimg.png" alt="drawing" />
</center></br>

<center><b>Figure 6.</b> Situations where the Dirichlet regression model is applied</center>

<br>

> ② Sample space: A multi-dimensional vector representing the proportions or probabilities of each item

<br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcIfmrC%2FbtrUuxi9haq%2FlJajvp8honqvFRY3YZuI20%2Fimg.png" alt="drawing" />
</center></br>

>> ○ Non-negative data

>> ○ Unit-sum

>> ○ D: The number of components, i.e., the size of the dimension

>> ○ d = D - 1

> ③ Dirichlet distribution: Gains attention because it can analyze the simplex.

> ④ Estimation of Dirichlet regression:

>> ○ Aitchison (2003) first introduced the log-ratio transformation.
   
>> ○ Log-likelihood function: Given n data points,

<br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdguFz6%2FbtrUujSOwtU%2FirK1CmMk7i5dWzN7P5qukK%2Fimg.png" alt="drawing" />
</center></br>

>> ○ Zero data can be a problem in the log-likelihood function.
   
>>> ○ Solution 1: Replace zero data with very small nonzero values, as proposed by Palarea-Albaladejo, Martín-Fernández, and others.

>>> ○ Solution 2: Use a dual model that handles zero data separately, as proposed by Zadora, Scealy, Welsh, Stewart, Field, Bear, Billheimer, and others.

>>> ○ Solution 3: Use an improved regression model that robustly applies to zero data, as proposed by Tsagris, Stewart, and others.

<br>

## 5. interaction

⑴ modeling

> ① interaction regressor or interaction term is introduced 

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/crZefT/btruyMLFz6Z/s3xhcd6ixX2tTBRYksBHkK/img.png" alt="drawing" /></center><br>

> ② unable to compare coefficients with models without interaction terms

> ③ 3 or more multiple interactions can also be defined

⑵ effect**:** the change of Yi by a unit change of Xi is as follows

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/Mmp48/btrurT6nh7P/zwfuvswPjizZ8tBVb39he0/img.png" alt="drawing" /></center><br>

⑶ elasticity 

> ① intuitively, it means the degree to which the absolute value of the slope is large

> ② in microeconomics, elasticity means the slope multiplied by (-1)

⑷ applicatio n**:** interaction of binary variables

> ① modeling

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/DSYea/btruuhyUHzY/yIPkRX5EolgPZkv6BBurd0/img.png" alt="drawing" /></center><br>

> ② effect **:** the effect of Y by a unic change of X is as follows

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/d8dhff/btruxjprbq5/xRUaw8mcM2K70vaFxbxQO1/img.png" alt="drawing" /></center><br>

> ③ H<sub>0</sub> **:** the proposition that Y is not affected by D can be tested by F statistic concerning β<sub>2</sub> = β<sub>3</sub> = 0. determinant check 

> ④ H<sub>0</sub> **:** the proposition that the effect of Y by a unit change of X is not affected by D can be tested by t statistic concerning β<sub>3</sub> = 0 

> ⑤ the entire regression line can be obtained by using the regression line of D = 0 and the regression line of D = 1

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/blm3Dj/btruxj31cDF/aNEaYusfLl2IDwy4BTx40k/img.png" alt="drawing" /></center><br>

⑸ application**:** interaction of two binary variables (dummy variables)

> ① modeling

<br><center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/bRpvXC/btrusNLB4FS/ykOfv4aHFuVun7vi7nIGP0/img.png" alt="drawing" /></center><br>

> ② knowing 2 × 2 table for D<sub>1</sub>, D<sub>2</sub> can lead to regression line equation

---

<br>

*Input : 2019.06.21 12:10*
