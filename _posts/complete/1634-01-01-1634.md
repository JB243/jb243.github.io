## **Chapter 14-2. Simple Testing**

Recommended Article: 【Statistics】 Chapter 14. [Statistical Testing](https://jb243.github.io/pages/1631)

---

**1.** [Sign Test](#1-sign-test)

**2.** [ROC Analysis](#2-roc-analysis-receiver-operator-characteristic)

---

<br>

## **1\. Sign Test** 

 ⑴ Overview

> ① A test method that uses only the sign of the difference, ignoring the magnitude of the difference, to test the position of the median

> ② Convert the data into signs of + and - based on the median, and then test based on the number of these signs

> ③ Assumes that the data distribution is continuous and independent

 ⑵ Procedure

> ① **Step 1.** Sample extraction

>> ○ Extract a continuous sample from the population

>> ○ Define the remaining samples as X1, X2, ..., Xn when the number of samples remaining after excluding samples equal to the assumed median θ0 is n

> ② **Step 2.** Test statistic

<br>

<img width="314" height="109" alt="스크린샷 2025-07-24 오전 9 01 51" src="https://github.com/user-attachments/assets/95fe7ca7-ed5c-4d21-a784-adad0a92e30e" />

<br>

> ③ **Step 3.** Rejection region for significance level α

>> ○ Null hypothesis: θ = θ<sub>0</sub>

>> ○ If the alternative hypothesis is θ ＞ θ<sub>0</sub>, then the rejection region is B ≥ b(α, n, 1/2)

>> ○ If the alternative hypothesis is θ ＜ θ<sub>0</sub>, then the rejection region is B ≤ b(α, n, 1/2)

>> ○ If the alternative hypothesis is θ ≠ θ<sub>0</sub>, then the rejection region is B ≥ b(α/2, n, 1/2) or B ＜ b(1 - α/2, b, 1/2)

<br>

<br>

## **2\. ROC Analysis** (receiver operator characteristic)

 ⑴ Parameter Definition

<br>

<img width="685" height="484" alt="image" src="https://github.com/user-attachments/assets/42063d3b-b30d-4a6a-b8c0-6960d9ce0b43" />

**Table 1.** Confusioin Matrix

<br>

> ① TP (true positive): The case where the actual value is true and the measured value is true. (Note) Means real positive

> ② FN (false negative): The case where the actual value is true and the measured value is false. (Note) Means fake negative

> ③ FP (false positive): The case where the actual value is false and the measured value is true. (Note) Means fake positive

> ④ TN (true negative): The case where the actual value is false and the measured value is false. (Note) Means real negative

> ⑤ **Sensitivity** (true positive rate, TPR) or Recall: TP / (TP + FN)

> ⑥ **Specificity**: TN / (TN + FP)

> ⑦ Accuracy: (TP + TN) / (TP + FN + FP + TN)

> ⑧ Error rate: 1 - Accuracy

> ⑨ **Precision** or Positive Predictive Value (PPV): TP / (TP + FP)

> ⑨ Negative Predictive Value (NPV): TN / (TN + FN)

> ⑩ **False Discovery Rate** (FDR, false positive rate): FP / (TN + FP)

> ⑪ F1 Score: 2 × precision × recall / (precision + recall)

>> ○ A performance evaluation indicator that combines precision and sensitivity

>> ○ Ranges from 0 to 1

>> ○ The higher the precision and sensitivity, the higher the F1 Score

> ⑫ Kappa Statistic

>> ○ K = (Pr(a) - Pr(e)) / (1 - Pr(e))

>> ○ K: Kappa coefficient

>> ○ Pr(a): Probability of prediction being accurate

>> ○ Pr(e): Probability of prediction being coincidentally accurate

>> ○ A method to measure the agreement of categorical values measured by two observers

>> ○ Ranges from 0 to 1, with closer to 1 indicating better agreement between model predictions and actual values, and closer to 0 indicating disagreement

>> ○ In addition to accuracy, the kappa statistic is used to demonstrate that the evaluation results of the model are not coincidental

> ⑬ Matthew correlation coefficient (MCC)

<br>

<img width="491" height="151" alt="스크린샷 2025-07-24 오전 9 03 23" src="https://github.com/user-attachments/assets/a6063f8e-779d-40c4-b6a3-62d7f78bfb18" />

<br>

 ⑵ Concordance Index

> ① Generally, adjusting the threshold causes sensitivity and specificity to show opposite trends

<br>

<img width="630" height="187" alt="스크린샷 2025-07-24 오전 9 09 22" src="https://github.com/user-attachments/assets/352529ef-86f3-4ce2-b726-c611edb102cb" />

 **Figure 1.** Trend of sensitivity and specificity with respect to the threshold

<br>

> ② ROC curve: A graph visualized with 1 - specificity (= FDR) on the x-axis and sensitivity on the y-axis

<br>

<img width="503" height="375" alt="스크린샷 2025-07-24 오전 9 09 40" src="https://github.com/user-attachments/assets/784e37d2-b57a-479a-9526-7ba55bd65925" />

 **Figure 2.** AOC curve

<br>

>> ○ The ideal case is when both sensitivity and specificity are 1

>> ○ AUC (area under curve; area under AURC, ROC curve): Values range from 0 to 1. The closer to 1, the better the response.

<br>

![image](https://blog.kakaocdn.net/dna/PRoUd/btsKVTolYf1/AAAAAAAAAAAAAAAAAAAAAHZbAGwt33o0vg_IeQRgvTZHYMNmWtwj5tBmRnkDzt-j/img.gif?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1753973999&allow_ip=&allow_referer=&signature=bVxoGfL2517kcAyhWjW%2BzZSM0u8%3D)

**Figure 3.** AUC calculation process

<br>

![image](https://blog.kakaocdn.net/dna/bogpvf/btsKWWR54kv/AAAAAAAAAAAAAAAAAAAAAJtSC-CfxsYt4obd9sO2W6GMfiYLrcJuuhEcWfXpWO0h/img.gif?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1753973999&allow_ip=&allow_referer=&signature=QyLuQZSbWBj2eIHUk7BF0Eex5l4%3D)

**Figure 4.** AUC calculation process

<br>

> ③ Concordance Index: Refers to the area under the AOC curve

> ④ If the ROC is random, the concordance index = 0.5

> ⑤ The concordance cannot exceed 1

> ⑥ AUPRC

>> ○ Using precision and recall instead of sensitivity and specificity when calculating AUROC

>> ○ If the balance between the number of positive (class 1) examples and negative (class 2) examples is skewed towards one of them, AUPRC is the preferred metric compared to AUC.

<br>

---

_Input: 2021.04.13 15:22_
