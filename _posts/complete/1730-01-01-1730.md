## **Chapter 7-1. SNE, symmetric-SNE, tSNE**

Recommended post: 【Algorithm】 Chapter 7. [Dimensionality Reduction Algorithms](https://jb243.github.io/pages/2158)

---

**1.** [Overview](#1-overview)

**2.** **Type 1.** [SNE](#2-type-1-sne)

**3.** **Type 2.** [Symmetric-SNE](#3-type-2-symmetric-sne)

**4.** **Type 3.** [t-SNE](#4-type-3-t-sne)

---

<br>

## **1. Overview**

⑴ Terminology

> ① Manifold: A shape in a high-dimensional space that can, in practice, be represented in a lower dimension

> ② Data point: Also called a high-dimensional data point

<br>

<img width="76" height="27" alt="스크린샷 2026-03-01 오전 11 08 56" src="https://github.com/user-attachments/assets/e5f49f3d-d30f-4a01-af04-17e7be924543" />

<br>

> ③ Map point: Also called a low-dimensional data point

<br>

<img width="129" height="30" alt="스크린샷 2026-03-01 오전 11 09 18" src="https://github.com/user-attachments/assets/792b91aa-4b4d-4155-aa41-5fb036f98dbc" />

<br>

⑵ Characteristics

> ① Introduces the concept of probability to solve the drawbacks of MDS (multidimensional scaling)

> ② **Characteristic 1.** A dimensionality reduction algorithm that tries to preserve local neighborhoods in data: A local metric that does not consider extremely far data points

> ③ **Characteristic 2.** Nonlinear and non-deterministic

<br>

<br>

## **2. Type 1. SNE** 

⑴ Definition: A method that converts Euclidean distances in high-dimensional space into conditional probabilities that represent similarity between data points

> ① Euclidean distance: Distance using the Pythagorean theorem

> ② Mapping: A mapping that corresponds high-dimensional data points to low-dimensional data points

<br>

<img width="79" height="70" alt="스크린샷 2026-03-01 오전 11 10 04" src="https://github.com/user-attachments/assets/4698cc2c-f62e-486c-aa9f-4ca5ca3f6863" />

<br>

⑵ Conditional probabilities expressing similarity of high-dimensional data

<br>

<img width="384" height="117" alt="스크린샷 2026-03-01 오전 11 10 40" src="https://github.com/user-attachments/assets/dfac747e-0692-41f7-be17-ccb499db9879" />

<br>

> ① $x_i$: High-dimensional data point

> ② $x_j$: High-dimensional data point

> ③ $\sigma_i$: Standard deviation of the Gaussian distribution centered at xi

⑶ Conditional probabilities expressing similarity of low-dimensional data

<br>

<img width="336" height="168" alt="스크린샷 2026-03-01 오전 11 10 59" src="https://github.com/user-attachments/assets/cff734cf-6bd3-435c-bc3f-f504c5ae6b95" />

<br>

> ① Means clustering so that $\sigma_y = 0.707106781$

⑷ **Main assumption**: If the mapping is good, it is as follows

<br>

<img width="99" height="25" alt="스크린샷 2026-03-01 오전 11 11 28" src="https://github.com/user-attachments/assets/eb2dfe62-4a9b-455d-8952-7cd098c0b0b9" />

<br>

> ① [KL divergence](https://jb243.github.io/pages/2145)

<br>

<img width="500" height="32" alt="스크린샷 2026-03-01 오전 11 11 51" src="https://github.com/user-attachments/assets/98270c50-83d2-4e38-b996-65a6875c7147" />

<br>

> ② Cost function: Sum of KL divergences over all data points

<br>

<img width="464" height="46" alt="스크린샷 2026-03-01 오전 11 12 12" src="https://github.com/user-attachments/assets/f69b7ba3-19c1-4cc4-9266-d338ad6fc557" />

<br>

> ③ Strategy: Update Q in the direction that minimizes the cost function

⑸ Perplexity

<br>

<img width="372" height="47" alt="스크린샷 2026-03-01 오전 11 12 50" src="https://github.com/user-attachments/assets/82f0c121-1022-4a77-991e-833fa47a21be" />

<br>

> ① Perp(p<sub>i</sub>): A measure of the effective number of neighbors of data point x<sub>i</sub>

<br>

<img width="853" height="475" alt="스크린샷 2026-03-01 오전 11 13 19" src="https://github.com/user-attachments/assets/025e1128-d69c-4f68-8c79-e1afbbf631b9" />

**Figure 1.** SNE shape according to Perp]

(Note that the above example is t-SNE)

<br>

> ② In general, perplexity is set between 5 and 50

> ③ Experimentally observed to have a range of 1 ≤ perplexity ≤ 99

> ④ Set $\sigma_i$ differently for each $i$ so as to have a **constant** perplexity

> > ○ As σ increases, perplexity decreases (a decreasing function)

> > ○ Define σi using the bisection method

> > ○ 1<sup>st</sup>. Set LEFT and RIGHT values

> > ○ 2<sup>nd</sup>. MID ← (LEFT + RIGHT) / 2

> > ○ 3<sup>rd</sup>. $\sigma_i$ ← MID

> > ○ 4<sup>th</sup>. If Perp($\sigma_i$) ＜ Perp, then LEFT ← MID; move to 2<sup>nd</sup>

> > ○ 5<sup>th</sup>. If Perp($\sigma_i$) ＞ Perp, then RIGHT ← MID; move to 2<sup>nd</sup>

> > ○ 6<sup>th</sup>. After repeating N times, define the MID value as $\sigma_i$

⑹ Rate of change of the cost function

<br>

<img width="396" height="58" alt="스크린샷 2026-03-01 오전 11 15 05" src="https://github.com/user-attachments/assets/445a6947-a9d1-4a01-a35b-32fc8170e9c6" />

<br>

> ① $p_{j \mid i} - q_{j \mid i} + p_{i \mid j} - q_{i \mid j}$: Mismatch between similarity of data points and similarity of map points

> ② $y_i - y_j$: Distance on the map

⑺ Updating algorithm

<br>

<img width="332" height="163" alt="스크린샷 2026-03-01 오전 11 16 26" src="https://github.com/user-attachments/assets/675d1802-5b85-4599-97e2-916ceeffdc2a" />

<br>

> ① Learning rate η: Generally around 100

> ② Momentum α: Introduce momentum to avoid falling into a local minimum. Around 0.5 in the early stage, around 0.8 in the later stage

> ③ Iteration number t: Generally around 1000

⑻ **Application:** Setting $\sigma_i$ so that Perp(p<sub>i</sub>) is constant in SNE

> ① Definition

<br>

<img width="463" height="437" alt="스크린샷 2026-03-01 오전 11 17 08" src="https://github.com/user-attachments/assets/f7210822-3bb2-498a-b748-c526df595ed5" />

<br>

> ② Updating algorithm

<br>

<img width="400" height="488" alt="스크린샷 2026-03-01 오전 11 17 59" src="https://github.com/user-attachments/assets/fddd0f1c-e9ea-4c71-905b-645ad1388614" />

<br>

> ③ Conclusion: Since s differs on a log scale, a constant learning rate should not be applied

<br>

<img width="443" height="261" alt="스크린샷 2026-03-01 오전 11 18 22" src="https://github.com/user-attachments/assets/226479bd-57b1-4973-98de-c17e0dfb9c2e" />

<br>

<br>

## **3. Type 2. Symmetric-SNE**

⑴ Overview

> ① Problems of SNE: Optimization is difficult; the crowding problem occurs

> ② t-SNE: When computing similarity between two points, uses the Student t-distribution rather than a normal distribution

> ③ symmetric-SNE: An improved version of t-SNE

⑵ Joint probability expressing similarity of high-dimensional data

<br>

<img width="322" height="79" alt="스크린샷 2026-03-01 오전 11 18 44" src="https://github.com/user-attachments/assets/df341e0d-e719-47e0-9014-e4861ba80790" />

<br>

⑶ Joint probability expressing similarity of low-dimensional data

<br>

<img width="270" height="79" alt="스크린샷 2026-03-01 오전 11 18 58" src="https://github.com/user-attachments/assets/1fdbab7f-24fc-4400-b0cc-bcdb638c66c0" />

<br>

⑷ Cost function

<br>

<img width="407" height="50" alt="스크린샷 2026-03-01 오전 11 19 25" src="https://github.com/user-attachments/assets/74f01c8c-ccea-44b8-bf3b-9b584c527229" />

<br>

> ① SNE strategy: Minimize the KL divergence between conditional probability distributions p j | i and q j | i

> ② Symmetric-SNE strategy: Minimize the KL divergence between joint probability distributions p ji and q ji

⑸ Rate of change of the cost function

<br>

<img width="252" height="57" alt="스크린샷 2026-03-01 오전 11 19 47" src="https://github.com/user-attachments/assets/aba1f3bb-63c1-4802-b580-1ffbf28bfd04" />

<br>

## **4. Type 3. t-SNE**

⑴ Overview

> ① t-SNE has better expressive power than SNE ([ref](https://www.youtube.com/watch?v=wUcKUv-xmTY&t=1066s))

> ② References: ([ref1](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf), [ref2](https://arxiv.org/abs/1301.3342), [ref3](https://github.com/jkrijthe/Rtsne))

⑵ Joint probability expressing similarity of high-dimensional data: Convert to Gaussian-distribution similarity

<br>

<img width="155" height="48" alt="스크린샷 2026-03-01 오전 11 20 27" src="https://github.com/user-attachments/assets/4a9c000d-27a1-4bbb-8792-819104b63c12" />

<br>

⑶ Joint probability expressing similarity of low-dimensional data: Convert to t-distribution similarity with 1 degree of freedom

<br>

<img width="268" height="102" alt="스크린샷 2026-03-01 오전 11 20 48" src="https://github.com/user-attachments/assets/19db9e8c-4f89-431d-be1d-1ee9bfdc2081" />

<br>

⑷ Cost function

<br>

<img width="406" height="49" alt="스크린샷 2026-03-01 오전 11 21 10" src="https://github.com/user-attachments/assets/fef2b402-10e5-43d6-8a74-847ce827364f" />

<br>

⑸ Rate of change of the cost function

<br>

<img width="585" height="750" alt="스크린샷 2026-03-01 오전 11 21 46" src="https://github.com/user-attachments/assets/6c5bcbe9-1be5-4cea-ac60-1a1c23c96107" />

<br>

⑹ Updating algorithm

<br>

<img width="339" height="55" alt="스크린샷 2026-03-01 오전 11 22 15" src="https://github.com/user-attachments/assets/884e003e-80dd-457b-b309-9c07d2bcd6b9" />

<br>

> ① Learning rate η: Generally around 100

> ② Momentum α: Introduce momentum to avoid falling into a local minimum. Around 0.5 in the early stage, around 0.8 in the later stage

> ③ Iteration number t: Generally around 1000

> ④ Note: In the paper, there is a (+) in front of η, but I think it should be (-) when η ＞ 0

⑺ **R programming**

<br>

```r
install.packages("Rtsne")
library(Rtsne)

# data consists of only numbers; no rownames, no colnames
data <- read.csv("C:/Users/data.csv")

tsne <- Rtsne(data, dims = 2, perplexity = 30, verbose = TRUE, max_iter = 500)
# Performing PCA
# Read the 14893 x 50 data matrix successfully!
# OpenMP is working. 1 threads.
# Using no_dims = 2, perplexity = 30.000000, and theta = 0.500000
# Computing input similarities...
# Building tree...
# - point 10000 of 14893
# Done in 5.06 seconds (sparsity = 0.007533)!
# Learning embedding...
# Iteration 50: error is 103.272821 (50 iterations in 4.11 seconds)
# Iteration 100: error is 88.450651 (50 iterations in 4.85 seconds)
# Iteration 150: error is 79.372908 (50 iterations in 3.18 seconds)
# Iteration 200: error is 76.927413 (50 iterations in 3.10 seconds)
# Iteration 250: error is 75.783877 (50 iterations in 3.02 seconds)
# Iteration 300: error is 2.946416 (50 iterations in 2.96 seconds)
# Iteration 350: error is 2.515109 (50 iterations in 2.84 seconds)
# Iteration 400: error is 2.244128 (50 iterations in 2.88 seconds)
# Iteration 450: error is 2.053382 (50 iterations in 2.86 seconds)
# Iteration 500: error is 1.912981 (50 iterations in 2.90 seconds)
# Fitting performed in 32.67 seconds.

exeTimeTsne<- system.time(Rtsne(data, dims = 2, perplexity=30, verbose=TRUE, max_iter = 500))

plot(tsne$Y, t = 'n', xlab = "Y1", ylab = "Y2", main = "tsne")
text(tsne$Y, labels = "○", col = "light blue")

write.csv(tsne$Y, "C:/Users/map.csv")
```

<br>

> ① Rtsne cannot allocate vectors larger than 2.2 Gb (e.g. a 10000 × 10000 matrix)

> ② Processing a 1000 × 10000 matrix takes 1 minute 30 seconds

> ③ ERROR message: "Remove duplicates before running TSNE"

> > ○ Solution: `Rtsne(data, dims = 2, perplexity = 30, verbose = TRUE, max_iter = 500, check_duplicates = FALSE)`

<br>

---

_Input: 2019.10.05 17:32_
