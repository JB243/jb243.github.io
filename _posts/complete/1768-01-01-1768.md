## **Lecture 18. Regularization in Regression Analysis** (regularization, penalization)

Recommended post: 【Statistics】 [Statistics Table of Contents](https://jb243.github.io/pages/1641)

---

**1.** [Overview](#1-overview)

**2.** [MSPE](#2-mspe)

**3.** **Technique 1.** [Ridge regression](#3-technique-1-ridge-regression)

**4. Technique 2.** [LASSO regression](#4-technique-2-lasso-regression)

**5. Technique 3.** [Elastic net](#5-technique-3-elastic-net)

**6. Technique 4.** [SelectFromModel](#6-technique-4-selecfrommodel)

---

<br>

## **1. Overview **

 ⑴ Problems in regression analysis: Mainly prominent when there are many regression variables

> ① [Multicollinearity](https://jb243.github.io/pages/1632)

> ② **Underfitting**: The model lacks flexibility and cannot properly learn the given data

> ③ **Overfitting**

>> ○ In standard regression like OLS estimation, the model learns the noise in the sample, reducing predictive power

>> ○ Learning the bias during training can actually improve predictive power

 ⑵ Regularization (penalization)

> ① To solve regression problems, a penalty term is added to the parameters

> ② Note that not applying regularization causes overfitting, while too much causes underfitting

> ③ Standardization of data must be performed

>> ○ Features with large values have large coefficients, which may be overly penalized and shrink too much

>> ○ Conversely, features with small values have small coefficients and may be under-penalized

> ④ Sometimes includes a process of optimizing parameters (e.g. penalty term weight) using a validation set

> ⑤ Expected results of regularization

<br>

![image](https://github.com/user-attachments/assets/d2dfb20f-a53c-48eb-be5f-4c9863b11647)

**Figure 1.** Expected results of regularization

<br>

<br>

##  **2. MSPE** 

⑴ Overview

> ① Error: Assuming **e** is squared error, h is hypothesis, and f is true function

> ② **Type 1.** In-sample error: Also called training error. Similar to bias

<br>

![image](https://github.com/user-attachments/assets/d4154696-e199-4aac-acba-0ef2ca6d7b92)

<br>

> ③ **Type 2.** Out-of-sample error: Also called generalization error, MSPE. Similar to variance

<br>

![image](https://github.com/user-attachments/assets/fd5278f3-fc41-42c7-8c76-20785f3166d2)

<br>

>> ○ **Step 1.** Build prediction model using given sample

>> ○ **Step 2.** Compare predicted and actual values using data outside the sample (XOOS, YOOS)

>> ○ Note: ŷ refers to prediction obtained using in-sample data

> ④ (Reference) [bias-variance tradeoff](https://jb243.github.io/pages/1630)

> ⑤ Best prediction quantity: Called oracle predictor. E(YOOS | XOOS)

>> ○ Prediction error in MSPE is as follows

>> ○ Fundamental error: Cannot be improved. YOOS \- E(YOOS | XOOS)

>> ○ Estimation error: Ŷ(XOOS) - E(YOOS | XOOS)

⑵ MSPE estimator

<br>

<img width="523" alt="스크린샷 2025-06-08 오전 11 24 06" src="https://github.com/user-attachments/assets/773d49ed-b2d8-429a-b5c7-f29551edaf29" />

<br>

> ① If β is known, MSPE = σu2 holds

> ② k/n may be large

 ⑶ Assumptions

> ① **Assumption 1.** No multicollinearity

> ② **Assumption 2.** (XOOS, YOOS) are randomly drawn from the same population

 ⑷ Transformations

> ① Standardization

>> ○ (Xi1*, ···, Xki*, Yi*) are values extracted from original sample

>> ○ Define Xji as (Xji* - μXj*) / σXj*

>> ○ Dependent variable is transformed as Yσj ← Yσj \- μY*

> ② Principle of shrinkage

<br>

<img width="214" alt="스크린샷 2025-06-08 오전 11 24 44" src="https://github.com/user-attachments/assets/b3899cc4-59fb-4fdb-9fbb-a427fbd78c47" />

<br>

>> ○ Can reduce MSPE

>> ○ Bias occurs instead: Tradeoff

>> ○ Most famous example is James-Stein estimator

 ⑸ In-sample MSPE calculation: m-fold cross validation is commonly used

> ① 1<sup>st</sup>. Split given sample into m parts

> ② 2<sup>nd</sup>. Use m-1 parts to estimate parameters: Training data

> ③ 3<sup>rd</sup>. Use the remaining part to evaluate performance: Testing data

> ④ 4<sup>th</sup>. Repeat m times with different combinations

> ⑤ 5<sup>th</sup>. Take average to determine final estimator

<br>

<img width="311" alt="스크린샷 2025-06-08 오전 11 25 30" src="https://github.com/user-attachments/assets/0d35062e-4ecc-4255-a50c-be1565ba472e" />

<br>

> ⑥ Typically 10-fold cross validation is used

 ⑹ Out-of-sample root MSPE calculation

> ① Use model trained on in-sample data to evaluate performance on different sample

> ② This different sample is called validation set

<br>

<br>

## **3. Technique 1.** Ridge regression

 ⑴ Overview

> ① Definition: Penalizes squared values to control model complexity. Penalty is a function of weights

> ② Also called L2 regularization

> ③ Introduced in 1962 by A. E. Hoerl to solve non-invertibility of the regression matrix

<br>

<img width="135" alt="스크린샷 2025-06-08 오전 11 26 03" src="https://github.com/user-attachments/assets/740a913f-d8ab-49dc-a023-6b9cb908f1c8" />

<br>

> ④ [MAP learning for Gaussian distribution](https://jb243.github.io/pages/1768)

 ⑵ Objective function

> ① Simple form: Ridge estimator minimizes

<br>

<img width="500" height="68" alt="스크린샷 2025-12-16 오후 2 01 34" src="https://github.com/user-attachments/assets/b27bb210-4de8-4ec6-b21e-71a77817b4ff" />

<br>

> ② PRSS (penalized residual sum of squares)

<br>

<img width="321" alt="스크린샷 2025-06-08 오전 11 26 54" src="https://github.com/user-attachments/assets/607e0942-18a3-41c3-bb16-83f08ab13493" />

<br>

 ⑶ **Case 1.** Regression variables are uncorrelated

> ① Simple form: Can express relative to β̂j found when λ = 0

<br>

<img width="286" alt="스크린샷 2025-06-08 오전 11 27 30" src="https://github.com/user-attachments/assets/048072fe-dad3-447f-a62e-1671a172dc06" />

<br>

> ② Matrix form: Ridge objective function is convex, allowing easy solution via differentiation

<br>

<img width="294" alt="스크린샷 2025-06-08 오전 11 27 51" src="https://github.com/user-attachments/assets/ee8e4970-6e63-4dae-8de0-11493aaa12ea" />

<br>

 ⑷ **Case 2.** Regression variables are correlated: Must examine MSPE with respect to λ<sub>Ridge</sub>

> ① Bias-variance trade-off

<br>

![image](https://github.com/user-attachments/assets/53ed187b-d017-4f20-a458-59adc5d9bc61)

 **Figure 2.** General bias-variance trade-off

<br>

> ② λ<sub>Ridge</sub> is calculated via cross validation

> ③ λ<sub>Ridge</sub> = 0 fits best in-sample but not out-of-sample

<br>

![image](https://github.com/user-attachments/assets/5af18b40-6f1b-4389-b6bb-374bed7662ef)

**Figure 3.** Square root of MSPE according to λ<sub>Ridge</sub>

<br>

 ⑸ Characteristics of Ridge regression solution

> ① Even without invertible XtX, λ allows calculation of inverse

> ② Each λ gives one estimator

> ③ λ → 0: **Overfitting**. Reaches linear regression (OLS) solution

> ④ λ → ∞: **Underfitting**. Coefficients w approach 0 (**∵** penalty on large coefficients)

 ⑹ **Application 1.** Soft order constraints: Eventually turns inequality constraint like ||w|| ≤ C into equality constraint

<br>

![image](https://github.com/user-attachments/assets/0ff9777c-5b4e-49b2-920c-6b298cdbaacf)

<br>

 ⑺ **Application 2.** Weight decay: Treat ||**w**|| like error-term and apply standard neural net update methods

> ① Standard gradient descent: **w** t \- η∇Ein(**w** t)

<br>

![image](https://github.com/user-attachments/assets/ebbbe495-b9a3-4835-9573-d7401bd5bcd6)

<br>

 ⑻ **Application 3.** MAP (maximum a posteriori)

> ① Bayes rule

<br>

![image](https://github.com/user-attachments/assets/b2f1afcf-f18a-4b2d-bfbf-6627e4b0514a)

<br>

> ② General MAP learning: Recall case where "P(D) = constant" in Bayes rule

<br>

![image](https://github.com/user-attachments/assets/ec6239db-c8b6-4432-8a43-948fc1badaca)

<br>

>> ○ Assuming normal distribution: Assuming **w** is unrelated to prior except w0 and is small

<br>

![image](https://github.com/user-attachments/assets/b4b31271-b909-4682-9879-d971047ddd2f)

<br>

> ③ MAP learning in Ridge regression

<br>

![image](https://github.com/user-attachments/assets/fdad8c06-e8c5-4117-90f1-297aa9920b9a)

<br>

 ⑼ **Application 4.** Comparison with other methods

<br>

![image](https://github.com/user-attachments/assets/9ef36e8c-a75d-4a44-bbc7-9f624020d496)

**Figure 4.** Comparison of predictive performance

<br>

<br>

 ## **4. Technique 2.** LASSO regression 

 ⑴ Overview

> ① Definition: Penalizes absolute values to control model complexity. Penalty is a function of weights

> ② Also called L1 regularization

> ③ [MAP learning for Laplacian prior](https://jb243.github.io/pages/1768)

<br>

![image](https://github.com/user-attachments/assets/8248fe8a-8958-4f9e-b6f6-0b6813f36b91)

**Figure 5.** Laplace probability density function

<br>

 ⑵ Objective function

> ① Simple form: LASSO estimator minimizes 

<br>

<img width="502" height="66" alt="스크린샷 2025-12-16 오후 2 04 17" src="https://github.com/user-attachments/assets/17472bce-0c4b-4ab8-9d0c-7e386b9d757f" />

<br>

> ② Matrix form

<br>

![image](https://github.com/user-attachments/assets/b76b1c97-f9ec-4380-91f0-e087e0ddb534)

<br>

 ⑶ Solution of objective function: Compute MSPE with respect to λ<sub>LASSO</sub>

<br>

![image](https://github.com/user-attachments/assets/91b145b1-8a25-4945-a1e6-eb3637992d13)

**Figure 6.** Square root of MSPE according to λ<sub>LASSO</sub>

<br>

> ① λ<sub>LASSO</sub> is calculated via cross validation

> ② Unlike Ridge, no general closed-form solution

 ⑷ Characteristics

> ① Useful when model has sparsity property: i.e., many coefficients are 0

> ② λ → 0: Reaches linear regression (OLS) solution. Best in-sample but poor out-of-sample

> ③ λ → ∞: Coefficients w approach 0 (**∵** penalty on large coefficients)

 ⑸ **Application 1.** Principle of **sparsity**

> ① Laplace prior sets unimportant variables exactly to 0: Effectively removes unimportant **variables**

<br>

![image](https://github.com/user-attachments/assets/99b44618-cc1c-4a9a-a9dc-148e0ef8fc00)

 **Figure 7.** Changes in coefficients with shrinkage factor

<br>

> ② **Diagram of sparsity principle**

<br>

![image](https://github.com/user-attachments/assets/555d350d-241a-43e0-b578-b7f71e0338ce)

 **Figure 8.** Intuitive understanding of LASSO regression's sparsity

<br>

>> ○ Red ellipses connect points of equal MSE (mean squared error)

>> ○ Blue region connects points with equal penalty

>> ○ As λ increases, penalty increases, both LASSO and Ridge shrink

>> ○ In Ridge, optimum occurs at point where red ellipse touches circular blue region: If not, solution lies nearer to origin with smaller penalty

>> ○ In LASSO, if blue region is small, optimal solution occurs at point where some coefficients are 0: At this sharp point, movement along edge exits red ellipse (→ higher MSE)

>> ○ Unlike Ridge, LASSO induces sparsity

 ⑹ **Application 2.** Comparison with other methods

<br>

![image](https://github.com/user-attachments/assets/643d52a4-36e7-4ed8-8ba6-92c677fea17b)

**Figure 9.** Comparison of predictive performance

<br>

<br>

##  **5. Technique 3.** Elastic Net

 ⑴ A linear combination of LASSO and Ridge. Adds both sum of absolute values and squared values of weights as penalty terms

<br>

<img width="545" alt="스크린샷 2025-06-08 오전 11 32 22" src="https://github.com/user-attachments/assets/3aee984e-b49e-4c29-bb7e-36ebc8d8919a" />

<br>

 ⑵ **Parameter 1.** Alpha (α): Controls mix ratio of L1 and L2 penalties. α = 1 is LASSO, α = 0 is Ridge

 ⑶ **Parameter 2.** Lambda (λ): Controls strength of penalty. Multiplies the entire regularization term

<br>

<br>

## **6. Technique 4.** SelectFromModel

 ⑴ A method for selecting variables based on [decision tree](https://jb243.github.io/pages/2161) algorithms

<br>

---

_Input: 2019.12.08 12:35_

_Edit: 2024.09.27 08:47_
