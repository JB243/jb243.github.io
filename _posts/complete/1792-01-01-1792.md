## **Chapter 18. Advanced Regression Analysis** 

Higher category **:** 【Statistics】 [Statistics Overview](https://jb243.github.io/pages/1641)

---

**1.** [validity](#1-validity)

**2.** [panel data](#2-panel-data)

**3.** [instrumental variable](#3-instrumental-variable)

**4.** [randomized controlled experiment](#4-radomized-controlled-experiment)

**5.** [quasi-experiment](#5-quasi-experiment)

**6.** [heterogeneous population](#6-heterogeneous-population)

---

<br>

## **1.** **validity** 

⑴ internal validity

> ① definition **:** qualitative evaluation of whether each coefficient obtained as a result of regression analysis is reasonably calculated

> ② **threat 1.** omitted variable bias 

>> ○ definition**:** if there are variables that satisfy the following two conditions, the expected value of the residual is not zero

>>> ○ **condition 1.** the omitted variable correlates with one or several existing variables  

>>> ○ **condition 2.** the omitted variable must be the determinator of Y

>>> ○ example of the expected value of the residual

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/BIc8c/btrulROfMAN/p5IknsljHuAR3hwB1eOmtK/img.png" alt="drawing" />

<br>

>> ○ solution 

>>> ○ include omitted variables in regression analysis

>>> ○ if there is no data related to the omitted variable, the following three methods exist:

>>> ○ **method 1.** panel data**:** remove properties that do not change over time

>>> ○ **method 2.** instrumental variable regression**:** only essence information can be extracted through instrumental variables  

>>> ○ **method 3.** collecting new information under randomized controlled experiment

> ③ **threat 2.** wrong functional form bias 

>> ○ definition**:** bias arising from linear regression analysis in nonlinear relationships

>> ○ a kind of omitted variable bias 

> ④ **threat 3.** errors-in-variable bias or measurement error in the regressors 

>> ○ definition **:** an independent variable with measurement error, X̃<sub>i</sub>, can be correlated with error v<sub>i</sub> 

>> ○ formula

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/zgnnM/btruAkupIzA/TzCSSWZkEQ5sNdwqWKF7Ik/img.png" alt="drawing" />

<br>

>> ○ **issue 1.** the iron law of econometrics**:** the OLS estimator of slope tends to be lower than the true value

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/GgF2g/btrukgtSoFG/enQxkkhJMhfqmP9rx8Gg9k/img.png" alt="drawing" />

<br>

>> ○ **issue 2.** OLS estimator doesn't have consistency 

>> ○ **issue 3.** statistical estimation is inaccurate

>> ○ solution 

>>> ○ **method 1.** improvement of accuracy of measuring instruments

>>> ○ **method 2.** instrumental variable regression**:** only essence information can be extracted through instrumental variables

>>> ○ **method 3.** error correction**:** correction is possible if there is a pattern of error

>> ○ (note) if there is a measurement error in the dependent variable

>>> ○ formula

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/bjXMMd/btruoMlv2w7/Ucph8cSzvn9Yxy8UNIvtek/img.png" alt="drawing" />

<br>

>>> ○ the estimator of the slope does not change

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/pk8yj/btruyNw4LqD/wMgArKCIkK7ggNz3DHcjSK/img.png" alt="drawing" />

<br>

>>> ○ satisfying the **three** **major assumptions** of a simple linear regression model

>>> ○ **assumption 1.** X<sub>i</sub> does not provide any information on v<sub>i</sub> 

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/cjSlBV/btruhu7ktiX/51pARSyLYpKUfOLE7enx7k/img.png" alt="drawing" />

<br>

>>> ○ **assumption 2.** X<sub>i</sub> and Ỹ<sub>i</sub> are i.i.d. 

>>>> ○ as Y<sub>i</sub> and wi are i.i.d. and mutually independent, Ỹ<sub>i</sub> is i.i.d. 

>>>> ○ as X<sub>i</sub> is independent with Y<sub>j</sub> or w<sub>j</sub> for i ≠ j, X<sub>i</sub> and Ỹ<sub>i</sub> are independent 

>>>> ○ therefore, the **assumption 2** is satisfied 

>>> ○ **assumption 3.** existence of 4<sup>th</sup> order moment 

>>>> ○ because u<sub>i</sub> and w<sub>i</sub> have finite 4th order moments and mutually independent, v<sub>i</sub> = u<sub>i</sub> + w<sub>i</sub> has a finitie 4<sup>th</sup> order moment 

>>>> ○ thus, (X<sub>i</sub>, v<sub>i</sub>) has non-zero finite 4-th moment 

>>> ○ there are **three differences** between errors-in-variable bias

>>> ○ **difference 1.** OLS estimator has consistency 

>>> ○ **difference 2.** statistical estimation is accurate 

>>> ○ **difference 3.** increases variance of regression errors → increases the variance of OLS estimator

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/v2TUi/btruoKBfGkq/nUmFPvgt5kBykgF405h53K/img.png" alt="drawing" />

<br>

>> ⑤ **threat 4.** sample selection bias

>>> ○ when bias occurs in the data selection process

>>> ○ in other words, bias is generated by deducing the characteristics of the entire group from a part 

>>> ○ **example 1.** recruitment rate for factor A and factor B

>>>> ○ assume that the recruitment rate increases as A and B increase

>>>> ○ people with low A factor do not want to apply

>>>> ○ among those with low A factor, those with high B factor apply

>>>> ○ as a result, the employment rate regression curve for A factor measures the effect on A factor lower than the actual one

> ⑥ **threat 5.** simultaneous causality bias 

>> ○ it is natural that there is a casual link from the independent variable to the dependent variable

>> ○ if there is a causal link from the dependent variable to the independent variable, bias occurs in the coefficient of the independent variable

>> ○ it is as if the **feedback circuit** is expressed in a complex formula

>>> ○ positive feedback circuit **:** increases the absolute value of the coefficients

>>> ○ negative feedback circuit **:** decreases the absolute value of the coefficients

>>> ○ example **:** birth rate and mortality rate have a mutual causal relationship. similar to a positive feedback circuit

>> ○ solution 

>>> ○ **method 1.** instrumental variable regression**:** extracts only the essence information that has been removed from the causal link

>>> ○ **method 2.** randomized controlled experiment**:** eliminate causality of dependent variables by randomly performing the treatment

⑵ external validity 

> ① definition **:** a qualitative evaluation of whether the coefficients for each independent variable obtained from regression analysis are applicable to other populations

> ② **threat 1.** non-representative sample**:** difference in populations themselves

> ③ **threat 2.** non-representative program or policy**:** difference in system 

>> ○ different systems can violate external validity even if the population is the same

>> ○ example **:** difference in educational environment, difference in laws and institutions, difference in physical environment, etc

> ④ **threat 3.** general equilibrium effect 

>> ○ definition**:** treatment changes the overall environment, which can amplify or suppress the effectiveness of treatment

>> ○ similar to simultaneous causality bias

>> ○ [example](https://economics.stackexchange.com/questions/26892/what-are-general-equilibrium-effects) **:** effect of the existence of oil fields on income

>>> ○ existence of oil fields → increase in workers' income

>>> ○ increase in workers' income → increase in the inflow of new workers

>>> ○ increase in home purchase → increase in housing prices due to lack of housing → decrease in income

>>> ○ increased car congestion → factors of income reduction

>>> ○ increased demand for increased restaurant quality due to increased income → increase in dining out costs → factor of decreasing income

> ⑤ solutions

>>> ○ method for adjusting the conclusions of the regression relationship according to population and setting

>>> ○ meta-analysis**:** comparing conclusions of similar but not identical populations

<br>

<br>

## **2. panel data**

⑴ overview 

> ① referring to the following data

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/bdQtLa/btrurT6qoYx/xigKO4CtGfTgk2Oz0MpoOK/img.png" alt="drawing" />

<br>

> ② balanced panel data **:** all entities are equipped in all time intervals

> ③ unbalanced panel data **:** if it is not balanced panel data

> ④ (comparison) repeated cross-sectional data 

>> ○ panel data is data tracked for each individual

>> ○ repeated cross-sectional data is data obtained over time 

>> ○ even repeated cross-sectional data can include the same person in the before and after data and is cheap

⑵ before and after regression model

> ① formula

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/wbpED/btrurURJL3N/wHEtkWX4SORpRKZS6jRO00/img.png" alt="drawing" />

<br>

>> ○ this model can remove constant elements over time 

>> ○ Z is different from intercept because it has different values depending on i

> ② a kind of fixed effect regression model 

⑶ fixed effect regression model

>> ① major assumptions

>>> ○ **assumption 1.** <span>E(u<sub>it</sub> | X<sub>i1</sub>, ···, X<sub>iT</sub>, α<sub>i</sub>)</span> \= 0**:** it is not sufficient that E(u<sub>it</sub> | X<sub>it</sub>, α<sub>i</sub>) = 0 (**∵** information of all time is used for the average value of y and u)

>>> ○ **assumption 2.** (X<sub>i1</sub>, ···, X<sub>iT</sub>, u<sub>i1</sub>, ···, u<sub>iT</sub>) is i.i.d. under joint distribution **:** in other words, it does not mean that cov(u<sub>it</sub>, u<sub>is</sub>) = 0 (assuming t ≠ s) 

>>> ○ **assumption 3.** existence of 4<sup>th</sup> order moment 

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/kFMy0/btrupI5pifA/rboPlIHF1lRa97ULOpFAHK/img.png" alt="drawing" />

<br>

>>> ○ **assumption 4.** no perfect multicollinearity **:** X<sub>it</sub> must depend on t

>>> ○ under the majore assumptions, fixed effect estimator satisfies consistency and asymptotic normality

>>> ○ even if n increases to infinite, the average of Y on time does not satisfy consistency and normality (**∵** n and T are irrelevant)

> ② formula 

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/dq3ZCa/btrupHMbzoQ/aqBEr5VAdpkJxJGnkBafKk/img.png" alt="drawing" />

<br>

>> ○ data should be comprehended as a table represented on the axes of i and t

>> ○ in the case of T = 2, the situation is the same with before and after regression model

>> ○ the standard deviation of slope = clustered standard error = heteroscedasticity & autocorrelation consistent standard error (HAC)

>> ○ there are not a total of T regression lines up to t = 1, · · · and T. it's just one regression line

>> ○ it is not for β<sub>1, t</sub> but β<sub>1</sub> 

> ③ [an example of algorithm](http://www.pearsonglobaleditions.com/) 

<br>

<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">data <span style="color: #333333">&lt;-</span> read<span style="color: #333333">.</span>csv(<span style="background-color: #fff0f0">&quot;C:/Users/sun/Desktop/Guns.csv&quot;</span>, header <span style="color: #333333">=</span> T)
attach(data)

y <span style="color: #333333">&lt;-</span> data[, <span style="color: #0000DD; font-weight: bold">2</span>]
y <span style="color: #333333">&lt;-</span> log(y)
x1 <span style="color: #333333">&lt;-</span> data[, <span style="color: #0000DD; font-weight: bold">13</span>]
x2 <span style="color: #333333">&lt;-</span> data[, <span style="color: #0000DD; font-weight: bold">5</span>]
x3 <span style="color: #333333">&lt;-</span> data[, <span style="color: #0000DD; font-weight: bold">11</span>]
x4 <span style="color: #333333">&lt;-</span> data[, <span style="color: #0000DD; font-weight: bold">10</span>]
x5 <span style="color: #333333">&lt;-</span> data[, <span style="color: #0000DD; font-weight: bold">9</span>]
x6 <span style="color: #333333">&lt;-</span> data[, <span style="color: #0000DD; font-weight: bold">6</span>]
x7 <span style="color: #333333">&lt;-</span> data[, <span style="color: #0000DD; font-weight: bold">7</span>]
x8 <span style="color: #333333">&lt;-</span> data[, <span style="color: #0000DD; font-weight: bold">8</span>]

state_y <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">56</span>)
state_x1 <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">56</span>)
state_x2 <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">56</span>)
state_x3 <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">56</span>)
state_x4 <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">56</span>)
state_x5 <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">56</span>)
state_x6 <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">56</span>)
state_x7 <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">56</span>)
state_x8 <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">56</span>)

<span style="color: #008800; font-weight: bold">for</span>(i <span style="color: #000000; font-weight: bold">in</span> <span style="color: #0000DD; font-weight: bold">1</span>:<span style="color: #0000DD; font-weight: bold">56</span>){
    <span style="color: #008800; font-weight: bold">if</span>(i <span style="color: #333333">!=</span> <span style="color: #0000DD; font-weight: bold">3</span> <span style="color: #333333">&amp;&amp;</span> i <span style="color: #333333">!=</span> <span style="color: #0000DD; font-weight: bold">7</span> <span style="color: #333333">&amp;&amp;</span> i <span style="color: #333333">!=</span> <span style="color: #0000DD; font-weight: bold">14</span> <span style="color: #333333">&amp;&amp;</span> i <span style="color: #333333">!=</span> <span style="color: #0000DD; font-weight: bold">43</span> <span style="color: #333333">&amp;&amp;</span> i <span style="color: #333333">!=</span> <span style="color: #0000DD; font-weight: bold">52</span>){
        data_sub <span style="color: #333333">&lt;-</span> data[stateid <span style="color: #333333">==</span> i, ]
        state_y[i] <span style="color: #333333">&lt;-</span> mean(data_sub[, <span style="color: #0000DD; font-weight: bold">2</span>])
        state_x1[i] <span style="color: #333333">&lt;-</span> mean(data_sub[, <span style="color: #0000DD; font-weight: bold">13</span>])
        state_x2[i] <span style="color: #333333">&lt;-</span> mean(data_sub[, <span style="color: #0000DD; font-weight: bold">5</span>])
        state_x3[i] <span style="color: #333333">&lt;-</span> mean(data_sub[, <span style="color: #0000DD; font-weight: bold">11</span>])
        state_x4[i] <span style="color: #333333">&lt;-</span> mean(data_sub[, <span style="color: #0000DD; font-weight: bold">10</span>])
        state_x5[i] <span style="color: #333333">&lt;-</span> mean(data_sub[, <span style="color: #0000DD; font-weight: bold">9</span>])
        state_x6[i] <span style="color: #333333">&lt;-</span> mean(data_sub[, <span style="color: #0000DD; font-weight: bold">6</span>])
        state_x7[i] <span style="color: #333333">&lt;-</span> mean(data_sub[, <span style="color: #0000DD; font-weight: bold">7</span>])
        state_x8[i] <span style="color: #333333">&lt;-</span> mean(data_sub[, <span style="color: #0000DD; font-weight: bold">8</span>])
    }
}

Y <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">1173</span>)
X1 <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">1173</span>)
X2 <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">1173</span>)
X3 <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">1173</span>)
X4 <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">1173</span>)
X5 <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">1173</span>)
X6 <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">1173</span>)
X7 <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">1173</span>)
X8 <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">1173</span>)

<span style="color: #008800; font-weight: bold">for</span>(i <span style="color: #000000; font-weight: bold">in</span> <span style="color: #0000DD; font-weight: bold">1</span> : dim(data)[<span style="color: #0000DD; font-weight: bold">1</span>]){
    j <span style="color: #333333">&lt;-</span> data[i, <span style="color: #0000DD; font-weight: bold">12</span>]
    Y[i] <span style="color: #333333">&lt;-</span> y[i] <span style="color: #333333">-</span> state_y[j]
    X1[i] <span style="color: #333333">&lt;-</span> x1[i] <span style="color: #333333">-</span> state_x1[j]
    X2[i] <span style="color: #333333">&lt;-</span> x2[i] <span style="color: #333333">-</span> state_x2[j]
    X3[i] <span style="color: #333333">&lt;-</span> x3[i] <span style="color: #333333">-</span> state_x3[j]
    X4[i] <span style="color: #333333">&lt;-</span> x4[i] <span style="color: #333333">-</span> state_x4[j]
    X5[i] <span style="color: #333333">&lt;-</span> x5[i] <span style="color: #333333">-</span> state_x5[j]
    X6[i] <span style="color: #333333">&lt;-</span> x6[i] <span style="color: #333333">-</span> state_x6[j]
    X7[i] <span style="color: #333333">&lt;-</span> x7[i] <span style="color: #333333">-</span> state_x7[j]
    X8[i] <span style="color: #333333">&lt;-</span> x8[i] <span style="color: #333333">-</span> state_x8[j]
}



RELATION <span style="color: #333333">&lt;-</span> lm(Y <span style="color: #333333">~</span> X1 <span style="color: #333333">+</span> X2 <span style="color: #333333">+</span> X3 <span style="color: #333333">+</span> X4 <span style="color: #333333">+</span> X5 <span style="color: #333333">+</span> X6 <span style="color: #333333">+</span> X7 <span style="color: #333333">+</span> X8)

summary(RELATION)
</pre></div>

<br>

> ④ when a result of applying the fixed effect regression model shows a significantly different conclusion from the original result

>> ○ strong implication that there was an omitted variable bias in the original model

> ⑤ even if the main assumptions are satisfied, there can be autocorrelation

>> ○ autocorrelation**:** uit and uit\* (t ≠ t\*) also have serial correlation. associated with HAC

>> ○ the case without autocorrelation 

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F996F474B5DDC77CC29" alt="drawing"/>

<b>Figure 1.</b> the case without autocorrelation

<br>

>> ○ the case with autocorrelation 

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/bbQqm5/btrupHSWz3Z/8GPC2NJmZDocQVkLZgmgY1/img.jpg" alt="drawing" />

<b>Figure 2.</b> the case with autocorrelation 

<br>

>> ○ the proof of cov(v<sub>it</sub>, v<sub>is</sub>) = 0 (assuming k ≠ s) in the case without autocorrelation 

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/bAqz2l/btruBbEd3nR/kqmfkKCTAu6PqikvUDJop0/img.png" alt="drawing" />

<br>

⑷ matrix notation of fixed effect regression model 

> ① modeling

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/erBfrj/btruvvqaTQJ/ue6VVsKtofojRD3xEO7ZV0/img.png" alt="drawing" />

<br>

> ② assumption

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/ox2A1/btrukf2QYYL/QKhiAnR1zBEncSeNKVG7K0/img.png" alt="drawing" />

<br>

> ③ fixed effects estimator

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/zkUuq/btruBbRKvTM/JeQWWUl9CzEpsoH1MRYmpk/img.png" alt="drawing" />

<br>

> ④ consistency

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/cALSZ6/btruvudKkX2/4lutkvFoQYrb2BeTpYYll0/img.png" alt="drawing" />

<br>

> ⑤ asymptotic normality

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/JBvYn/btruuiR9ANJ/Ek7qNGyIMMkLK6GSXfpwQ1/img.png" alt="drawing" />

<br>

⑸ least squares dummy variables model (LSDV)

> ① formula

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/l0WTh/btruqEImP2L/RPwMU5uGVwDQQbCc07aAok/img.png" alt="drawing" />

<br>

> ② the reason why D1<sub>i</sub> is not included**:** to avoid perfect multi-collinearity

>> ○ formula

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/bs4Swg/btrutkIwu8U/kTgwupX0RfCreyiEWE6101/img.png" alt="drawing" />

<br>

>> ○ if the coefficient exists, γ<sub>1</sub> cannot be specified

>> ○ perfect multicollinearity caused by dummy variables is also called dummy variable trap

> ③ cannot perform the regression analysis if range of i (_i.e._, n) is too large**:** because there are too many regression variables 

⑹ time effect 

> ① time effect term**:** marked as λ<sub>t</sub>  

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/btzgFg/btrutkhts4k/9u5gYDoVA3VpuCJtJIe1C1/img.png" alt="drawing" />

<br>

> ② modeling

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/be8sxr/btrukfBNReC/N7Clm2vlnx80XLri4sDlh0/img.png" alt="drawing" />

<br>

> ③ [an example of algorithm](http://www.pearsonglobaleditions.com/)

<br>

<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">data <span style="color: #333333">&lt;-</span> read<span style="color: #333333">.</span>csv(<span style="background-color: #fff0f0">&quot;C:/Users/sun/Desktop/Guns.csv&quot;</span>, header <span style="color: #333333">=</span> T)
attach(data)

<span style="color: #888888"># definition</span>
y <span style="color: #333333">&lt;-</span> data[, <span style="color: #0000DD; font-weight: bold">2</span>]
y <span style="color: #333333">&lt;-</span> log(y)
x1 <span style="color: #333333">&lt;-</span> data[, <span style="color: #0000DD; font-weight: bold">13</span>]
x2 <span style="color: #333333">&lt;-</span> data[, <span style="color: #0000DD; font-weight: bold">5</span>]
x3 <span style="color: #333333">&lt;-</span> data[, <span style="color: #0000DD; font-weight: bold">11</span>]
x4 <span style="color: #333333">&lt;-</span> data[, <span style="color: #0000DD; font-weight: bold">10</span>]
x5 <span style="color: #333333">&lt;-</span> data[, <span style="color: #0000DD; font-weight: bold">9</span>]
x6 <span style="color: #333333">&lt;-</span> data[, <span style="color: #0000DD; font-weight: bold">6</span>]
x7 <span style="color: #333333">&lt;-</span> data[, <span style="color: #0000DD; font-weight: bold">7</span>]
x8 <span style="color: #333333">&lt;-</span> data[, <span style="color: #0000DD; font-weight: bold">8</span>]

<span style="color: #888888"># elimination of fixed state effects</span>
state_y <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">56</span>)
state_x1 <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">56</span>)
state_x2 <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">56</span>)
state_x3 <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">56</span>)
state_x4 <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">56</span>)
state_x5 <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">56</span>)
state_x6 <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">56</span>)
state_x7 <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">56</span>)
state_x8 <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">56</span>)

<span style="color: #008800; font-weight: bold">for</span>(i <span style="color: #000000; font-weight: bold">in</span> <span style="color: #0000DD; font-weight: bold">1</span>:<span style="color: #0000DD; font-weight: bold">56</span>){
    <span style="color: #008800; font-weight: bold">if</span>(i <span style="color: #333333">!=</span> <span style="color: #0000DD; font-weight: bold">3</span> <span style="color: #333333">&amp;&amp;</span> i <span style="color: #333333">!=</span> <span style="color: #0000DD; font-weight: bold">7</span> <span style="color: #333333">&amp;&amp;</span> i <span style="color: #333333">!=</span> <span style="color: #0000DD; font-weight: bold">14</span> <span style="color: #333333">&amp;&amp;</span> i <span style="color: #333333">!=</span> <span style="color: #0000DD; font-weight: bold">43</span> <span style="color: #333333">&amp;&amp;</span> i <span style="color: #333333">!=</span> <span style="color: #0000DD; font-weight: bold">52</span>){
        data_sub <span style="color: #333333">&lt;-</span> data[stateid <span style="color: #333333">==</span> i, ]
        state_y[i] <span style="color: #333333">&lt;-</span> mean(data_sub[, <span style="color: #0000DD; font-weight: bold">2</span>])
        state_x1[i] <span style="color: #333333">&lt;-</span> mean(data_sub[, <span style="color: #0000DD; font-weight: bold">13</span>])
        state_x2[i] <span style="color: #333333">&lt;-</span> mean(data_sub[, <span style="color: #0000DD; font-weight: bold">5</span>])
        state_x3[i] <span style="color: #333333">&lt;-</span> mean(data_sub[, <span style="color: #0000DD; font-weight: bold">11</span>])
        state_x4[i] <span style="color: #333333">&lt;-</span> mean(data_sub[, <span style="color: #0000DD; font-weight: bold">10</span>])
        state_x5[i] <span style="color: #333333">&lt;-</span> mean(data_sub[, <span style="color: #0000DD; font-weight: bold">9</span>])
        state_x6[i] <span style="color: #333333">&lt;-</span> mean(data_sub[, <span style="color: #0000DD; font-weight: bold">6</span>])
        state_x7[i] <span style="color: #333333">&lt;-</span> mean(data_sub[, <span style="color: #0000DD; font-weight: bold">7</span>])
        state_x8[i] <span style="color: #333333">&lt;-</span> mean(data_sub[, <span style="color: #0000DD; font-weight: bold">8</span>])
    }
}

Y <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">1173</span>)
X1 <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">1173</span>)
X2 <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">1173</span>)
X3 <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">1173</span>)
X4 <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">1173</span>)
X5 <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">1173</span>)
X6 <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">1173</span>)
X7 <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">1173</span>)
X8 <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">1173</span>)

<span style="color: #008800; font-weight: bold">for</span>(i <span style="color: #000000; font-weight: bold">in</span> <span style="color: #0000DD; font-weight: bold">1</span> : dim(data)[<span style="color: #0000DD; font-weight: bold">1</span>]){
    j <span style="color: #333333">&lt;-</span> data[i, <span style="color: #0000DD; font-weight: bold">12</span>]
    Y[i] <span style="color: #333333">&lt;-</span> y[i] <span style="color: #333333">-</span> state_y[j]
    X1[i] <span style="color: #333333">&lt;-</span> x1[i] <span style="color: #333333">-</span> state_x1[j]
    X2[i] <span style="color: #333333">&lt;-</span> x2[i] <span style="color: #333333">-</span> state_x2[j]
    X3[i] <span style="color: #333333">&lt;-</span> x3[i] <span style="color: #333333">-</span> state_x3[j]
    X4[i] <span style="color: #333333">&lt;-</span> x4[i] <span style="color: #333333">-</span> state_x4[j]
    X5[i] <span style="color: #333333">&lt;-</span> x5[i] <span style="color: #333333">-</span> state_x5[j]
    X6[i] <span style="color: #333333">&lt;-</span> x6[i] <span style="color: #333333">-</span> state_x6[j]
    X7[i] <span style="color: #333333">&lt;-</span> x7[i] <span style="color: #333333">-</span> state_x7[j]
    X8[i] <span style="color: #333333">&lt;-</span> x8[i] <span style="color: #333333">-</span> state_x8[j]
}

<span style="color: #888888"># elimination of fixed time effects</span>
time_Y <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">23</span>)
time_X1 <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">23</span>)
time_X2 <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">23</span>)
time_X3 <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">23</span>)
time_X4 <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">23</span>)
time_X5 <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">23</span>)
time_X6 <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">23</span>)
time_X7 <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">23</span>)
time_X8 <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">23</span>)

<span style="color: #008800; font-weight: bold">for</span>(t <span style="color: #000000; font-weight: bold">in</span> <span style="color: #0000DD; font-weight: bold">77</span>:<span style="color: #0000DD; font-weight: bold">99</span>){
    data_sub2 <span style="color: #333333">&lt;-</span> data[year <span style="color: #333333">==</span> t, ]
    time_Y[t <span style="color: #333333">-</span> <span style="color: #0000DD; font-weight: bold">76</span>] <span style="color: #333333">&lt;-</span> mean(data_sub2[, <span style="color: #0000DD; font-weight: bold">2</span>]) <span style="color: #333333">-</span> mean(state_y, na<span style="color: #333333">.</span>rm <span style="color: #333333">=</span> TRUE)
    time_X1[t <span style="color: #333333">-</span> <span style="color: #0000DD; font-weight: bold">76</span>] <span style="color: #333333">&lt;-</span> mean(data_sub2[, <span style="color: #0000DD; font-weight: bold">13</span>]) <span style="color: #333333">-</span> mean(state_x1, na<span style="color: #333333">.</span>rm <span style="color: #333333">=</span> TRUE)
    time_X2[t <span style="color: #333333">-</span> <span style="color: #0000DD; font-weight: bold">76</span>] <span style="color: #333333">&lt;-</span> mean(data_sub2[, <span style="color: #0000DD; font-weight: bold">5</span>]) <span style="color: #333333">-</span> mean(state_x2, na<span style="color: #333333">.</span>rm <span style="color: #333333">=</span> TRUE)
    time_X3[t <span style="color: #333333">-</span> <span style="color: #0000DD; font-weight: bold">76</span>] <span style="color: #333333">&lt;-</span> mean(data_sub2[, <span style="color: #0000DD; font-weight: bold">11</span>]) <span style="color: #333333">-</span> mean(state_x3, na<span style="color: #333333">.</span>rm <span style="color: #333333">=</span> TRUE)
    time_X4[t <span style="color: #333333">-</span> <span style="color: #0000DD; font-weight: bold">76</span>] <span style="color: #333333">&lt;-</span> mean(data_sub2[, <span style="color: #0000DD; font-weight: bold">10</span>]) <span style="color: #333333">-</span> mean(state_x4, na<span style="color: #333333">.</span>rm <span style="color: #333333">=</span> TRUE)
    time_X5[t <span style="color: #333333">-</span> <span style="color: #0000DD; font-weight: bold">76</span>] <span style="color: #333333">&lt;-</span> mean(data_sub2[, <span style="color: #0000DD; font-weight: bold">9</span>]) <span style="color: #333333">-</span> mean(state_x5, na<span style="color: #333333">.</span>rm <span style="color: #333333">=</span> TRUE)
    time_X6[t <span style="color: #333333">-</span> <span style="color: #0000DD; font-weight: bold">76</span>] <span style="color: #333333">&lt;-</span> mean(data_sub2[, <span style="color: #0000DD; font-weight: bold">6</span>]) <span style="color: #333333">-</span> mean(state_x6, na<span style="color: #333333">.</span>rm <span style="color: #333333">=</span> TRUE)
    time_X7[t <span style="color: #333333">-</span> <span style="color: #0000DD; font-weight: bold">76</span>] <span style="color: #333333">&lt;-</span> mean(data_sub2[, <span style="color: #0000DD; font-weight: bold">7</span>]) <span style="color: #333333">-</span> mean(state_x7, na<span style="color: #333333">.</span>rm <span style="color: #333333">=</span> TRUE)
    time_X8[t <span style="color: #333333">-</span> <span style="color: #0000DD; font-weight: bold">76</span>] <span style="color: #333333">&lt;-</span> mean(data_sub2[, <span style="color: #0000DD; font-weight: bold">8</span>]) <span style="color: #333333">-</span> mean(state_x8, na<span style="color: #333333">.</span>rm <span style="color: #333333">=</span> TRUE)
}

YY <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">1173</span>)
XX1 <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">1173</span>)
XX2 <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">1173</span>)
XX3 <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">1173</span>)
XX4 <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">1173</span>)
XX5 <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">1173</span>)
XX6 <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">1173</span>)
XX7 <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">1173</span>)
XX8 <span style="color: #333333">&lt;-</span> array(dim <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">1173</span>)

<span style="color: #008800; font-weight: bold">for</span>(i <span style="color: #000000; font-weight: bold">in</span> <span style="color: #0000DD; font-weight: bold">1</span> : dim(data)[<span style="color: #0000DD; font-weight: bold">1</span>]){
    j <span style="color: #333333">&lt;-</span> data[i, <span style="color: #0000DD; font-weight: bold">1</span>]
    YY[i] <span style="color: #333333">&lt;-</span> Y[i] <span style="color: #333333">-</span> time_Y[j <span style="color: #333333">-</span> <span style="color: #0000DD; font-weight: bold">76</span>]
    XX1[i] <span style="color: #333333">&lt;-</span> X1[i] <span style="color: #333333">-</span> time_X1[j <span style="color: #333333">-</span> <span style="color: #0000DD; font-weight: bold">76</span>]
    XX2[i] <span style="color: #333333">&lt;-</span> X2[i] <span style="color: #333333">-</span> time_X2[j <span style="color: #333333">-</span> <span style="color: #0000DD; font-weight: bold">76</span>]
    XX3[i] <span style="color: #333333">&lt;-</span> X3[i] <span style="color: #333333">-</span> time_X3[j <span style="color: #333333">-</span> <span style="color: #0000DD; font-weight: bold">76</span>]
    XX4[i] <span style="color: #333333">&lt;-</span> X4[i] <span style="color: #333333">-</span> time_X4[j <span style="color: #333333">-</span> <span style="color: #0000DD; font-weight: bold">76</span>]
    XX5[i] <span style="color: #333333">&lt;-</span> X5[i] <span style="color: #333333">-</span> time_X5[j <span style="color: #333333">-</span> <span style="color: #0000DD; font-weight: bold">76</span>]
    XX6[i] <span style="color: #333333">&lt;-</span> X6[i] <span style="color: #333333">-</span> time_X6[j <span style="color: #333333">-</span> <span style="color: #0000DD; font-weight: bold">76</span>]
    XX7[i] <span style="color: #333333">&lt;-</span> X7[i] <span style="color: #333333">-</span> time_X7[j <span style="color: #333333">-</span> <span style="color: #0000DD; font-weight: bold">76</span>]
    XX8[i] <span style="color: #333333">&lt;-</span> X8[i] <span style="color: #333333">-</span> time_X8[j <span style="color: #333333">-</span> <span style="color: #0000DD; font-weight: bold">76</span>]
}

RELATION <span style="color: #333333">&lt;-</span> lm(YY <span style="color: #333333">~</span> XX1 <span style="color: #333333">+</span> XX2 <span style="color: #333333">+</span> XX3 <span style="color: #333333">+</span> XX4 <span style="color: #333333">+</span> XX5 <span style="color: #333333">+</span> XX6 <span style="color: #333333">+</span> XX7 <span style="color: #333333">+</span> XX8)

summary(RELATION)
</pre></div>

<br>

⑺ time effect regression using dummy variables

> ① formula

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/bbUx5e/btruoL7WIOs/TQ2lXuj6WzWhDOapGXU3Qk/img.png" alt="drawing" />

<br>

> ② the reason why B1<sub>t</sub> is not included **:** to avoid perfect multi-collinearity

>> ○ formula

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/b4KumB/btruqEVWfL1/kW2QhdRqYva2QVlx9b5lR1/img.png" alt="drawing" />

<br>

>> ○ if the coefficient exists, δ<sub>1</sub> cannot be specified

>> ○ perfect multi-collinearity caused by dummy variables is also called dummy variable trap

<br>

<br>

## **3. instrumental variable** 

⑴ definition**:** a method of separating only the essence information of the regression variable using the third variable

⑵ simple expression

> ① modeling

>> ○ if there is one regression variable

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/BRB4H/btruvu5Q4T2/GNwao9wZJekoEIzYEsMBOk/img.png" alt="drawing" />

<br>

>> ○ if there are multiple regression variables 

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/bDE2WC/btruyNDQKFQ/kvYEjMXhdK3uPiHDvPq83k/img.png" alt="drawing" />

<br>

>> ○ endogenous variable **:** a variable correlated with u<sub>i</sub>  

>> ○ exogenous variable **:** a variable that is not correlated with u<sub>i</sub>

>> ○ exactly identified **:** m = k

>> ○ over-identified **:** m ＞ k

>> ○ under-identified **:** m ＜ k 

>> ○ unable to perform modeling in under-identified **:** it means that there should be a lot of instrumental variables

>> ○ the reason why W is included **:** it is useful when it is difficult to find a Z that meets the criteria

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/crHsEP/btrurTrRS1v/FjkTAChIlpswwxeVl8nKMk/img.png" alt="drawing" />

<br>

> ② assumptions for using instrumetnal variables 

>> ○ **assumption 1.** <span>E(u<sub>i</sub> | W<sub>1i</sub>, ···, W<sub>ri</sub>) = 0 </span>

>> ○ **assumption 2.** (X<sub>1i</sub>, ···, X<sub>ki</sub>, W<sub>1i</sub>, ···, W<sub>ri</sub>, Z<sub>1i</sub>, ···, Z<sub>mi</sub>, Y<sub>i</sub>) is i.i.d. 

>> ○ **assumption 3.** all variables have finite 4th order moment

>> ○ **assumption** **4.** instrumental variable effectiveness

>>> ○ **4-1.** instrument relevance

>>> ○ **4-2.** instrument exogeneity

>>> ○ **4-3.** no perfectly collinearity

>> ○ if the assumptions are satisfied, the TSLS estimator satisfies consistency and asymptotic normality 

> ③ procedure

>> ○ if there is one regression variable

>>> ○ 1<sup>st</sup>. perform regression analysis of X<sub>i</sub> by using instrumental variable Z<sub>i</sub> 

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/becxa1/btrusOw2Uhz/MK4ph3N2ShAM7tFVrlUz8K/img.png" alt="drawing" />

<br>

>>> ○ 2<sup>nd</sup>. calculate the estimator of X<sub>i</sub>  

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/tGaEd/btrupIdbyV5/Bi6QXsmcrKrBTGV0Mp4OH0/img.png" alt="drawing" />

<br>

>>> ○ 3<sup>rd</sup>. perform regression analysis of Y<sub>i</sub> by using the estimator of X<sub>i</sub>  

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/bhzr4a/btruyMStLCw/4Kbg6GAKZKU1SEhXn1V6ik/img.png" alt="drawing" />

<br>

>> ○ if there are multiple regression variables 

>>> ○ 1<sup>st</sup>. perform regression analysis of **X<sub>i</sub>** by using instrumental variable **Z<sub>i</sub>** **:** for ℓ = 1, ···, k, 

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/CZ3Tj/btruvvjpBn5/pAldpvhDKagErdUjVWQ0ck/img.png" alt="drawing" />

<br>

>>> ○ 2<sup>nd</sup>. calculate the estimator of **X<sub>i</sub>** **:** for ℓ = 1, ···, k,

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/bOuUQ8/btruAs0vnUq/Np1nKoYQvoVm7xIaxhnBsk/img.png" alt="drawing" />

<br>

>>> ○ 3<sup>rd</sup>. perform regression analysis of Y<sub>i</sub> by using the estimator of **X<sub>i</sub>** **:** for ℓ = 1, ···, k, 

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/bRcwUv/btruyNw45RE/LFMMwCnNUjOTSLahmkPHV1/img.png" alt="drawing" />

<br>

>> ○ doing OLS regression twice may miscalculate the standard error

> ④ two-step least squares (TSLS) estimator 

>> ○ formula

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/qrqTz/btrurUjUkuN/iYyVeYFKhVBw1rQA6jf2jK/img.png" alt="drawing" />

<br>

>> ○ proof

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/cZ7f36/btruyObHsBO/A0PATXbgAXgX6TbqbrjTOk/img.png" alt="drawing" />

<br>

>> ○ (note) if Z<sub>i</sub> **:**\= X<sub>i</sub>, the TSLS estimator of β<sub>1</sub> is the same with the OLS estimator of β<sub>1</sub> 

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/TS253/btrurS0FWFY/8TSoKbKZAf9YE2ZeNQSkjK/img.png" alt="drawing" />

<br>

> ⑤ consistency

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/32slN/btruqFghteo/px73DL1O9BU3YPPAdnssFK/img.png" alt="drawing" />

<br>

> ⑥ asymptotic normality

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/bMoZ2j/btruqF8oJpa/P8NcpfCk62kwmU8PsXsek0/img.png" alt="drawing" />

<br>

⑶ supplement of instrumental variable effectiveness 

> ① instrument relevance

>> ○ formula

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/bNdlfa/btruBcC7Wp6/fbobV1GVzt8FwvDjiuvzC1/img.png" alt="drawing" />

<br>

>> ○ weak instrumental variable**:** the case that the instrumental variable is not sufficiently correlated with the regression variable. the estimaor shows very strange values 

>> ○ test of instrumental variable strength 

>>> ○ when calculating 1<sup>st</sup> stage F statistic, if F is bigger than 10 the instrumental variable is strong

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/cRsQhb/btruvtMGj9y/Po8IW8D5GWNZ0qMhUD8Y0k/img.png" alt="drawing" />

<br>

>>> ○ available only in **homoskedasticity**

>>> ○ W<sub>1i</sub>, ···, W<sub>ri</sub> have nothing to do with the strength of an instrumental variable

> ② instrument exogeneity 

>> ○ formula 

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/ENsGL/btruBdaZmoX/cOZrxc7K9pn54FaBwYtwjk/img.png" alt="drawing" />

<br>

>> ○ u must be specified to know the instrument exogenuity

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/v7XwC/btruvurfJXX/0vKhz8dJ60W7AUImYHECwK/img.png" alt="drawing" />

<br>

>> ○ over-identifying restrictions test 

>>> ○ when the following statistics are calculated, J follows a chi-squared distribution with a degree of freedom of m-k

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/OeEUn/btruwIoNGYW/tnKhZFBsvpX9BPax8g1qzK/img.png" alt="drawing" />

<br>

>>> ○ null hypothesis for J, H<sub>0</sub> **:** the proposition that instrumental variables are exogenous

>>> ○ logic is similar to instrument relevance **:** if the F statistic is small, it means that there is no correlation (all coefficients 0)

>>> ○ available only in **homoskedasticity** **:** many statistical programs also offer heteroskedasticity-robust J-test

>>> ○ unable to determine which instrumental variable is endogenous when rejecting null hypothesis

>> ○ meaning of the degree of freedom in J statistic

>>> ○ k instumental variables are used to make residuals: they correspond to k endogenous variables

>>> ○ the remaining m-k instrumental variables are used to test the correlation with the residuals

>>> ○ unable to apply J test in the case of exactly identified because there are no instumental variables to be used in correlation relationship analysis: J statistic is always zero in this case

> ③ no perfect collinearity

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/bZxPbu/btrurUxq1XD/VOb2XRW9Wydkadskqvsc70/img.png" alt="drawing" />

<br>

⑷ matrix notation

> ① modeling

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/bTMPyc/btruoKHYYyg/PAoZp2lEw1BfVTs7EzYel1/img.png" alt="drawing" />

<br>

>> ○ X<sub>i</sub> and Z<sub>i</sub> may overlap

> ② assumptions 

>> ○ Y<sub>i</sub> \= **X<sub>i</sub>**<sup>t</sup>β + u<sub>i</sub>

>> ○ (Y<sub>i</sub>, **X****<sub>i</sub>**, **Z****<sub>i</sub>**), i = 1, ···, N is i.i.d.

>> ○ E(u<sub>i</sub> <span>|</span> **Z****<sub>i</sub>**) = 0

>> ○ E(**Z****<sub>i</sub>****X<sub>i</sub>**<sup>t</sup>), E(**Z****<sub>i</sub>****Z<sub>i</sub>**<sup>t</sup>) have inverse matrices  

>> ○ **Z****<sub>i</sub>**, ,**X<sub>i</sub>**, and ui have finite 4<sup>th</sup> order moments 

> ③ procedure 

>> ○ 1<sup>st</sup>. perform regression analysis of **X****<sub>i</sub>** by using instrumental variable **Z****<sub>i</sub>** 

>> ○ 2<sup>nd</sup>. calculate the estimator of **X****<sub>i</sub>** 

>> ○ 3<sup>rd</sup>. perform regression analysis of Y<sub>i</sub> by using the estimator of **X****i** 

> ④ estimator

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/cjJvk6/btruhv6eWaq/MZCTfGR7zuPWx0YzXrO2ak/img.png" alt="drawing" />

<br>

> ⑤ consistency

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/cRqkaj/btrutkIwAhy/FGGvRaznzx6reCtg5jOHK0/img.png" alt="drawing" />

<br>

> ⑥ asymptotic normality

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/O3yf0/btruvu5RdWU/hMaY0FqlO6xkBWF736wB71/img.png" alt="drawing" />

<br>

> ⑦ estimator of the variance of the normal distribution  

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/72Ogn/btrutk2O1sM/v7awSfbLa5MKBkXRvZburK/img.png" alt="drawing" />

<br>

⑸ exploration of instrumental variables**:** the exploration is in the realm of art  

> ① Joshua Angrist (MIT)

> ② Steven Levitt (Chicago)**:** published "Freakonomics"

> ③ Daron Acemoglu (MIT)**:** published "Why Nations Fail"

<br>

<br>

## **4\. randomized controlled experiment** 

⑴ overview 

> ① definition**:** randomly extracting subjects from the population and then dividing the groups randomly again to perform different treatments

> ② randomized controlled experiments are rare in econometrics

> ③ randomized controlled experiments can remove omitted variable bias**:** 100% validity is not guaranteed

> ④ it provides standard for which to judge causality

⑵ formula 

> ① simple model

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/OERpP/btruxjC1vov/BtTtRDj08UbZsJkyV0k8W0/img.png" alt="drawing" />

<br>

> ② model including additional regression variables

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/b6BFCF/btruArAwvSb/k5F7As1kxbuUoP2rBmGHW0/img.png" alt="drawing" />

<br>

> ③ reason for adding additional regression variables

>> ○ **reason 1.** randomization check 

>>> ○ regardless of whether additional regression variables are present or not, β<sub>1</sub> is consistent

>>> ○ it was not random if β<sub>1</sub> changed significantly depending on the presence or absence of additional regression variables 

>> ○ **reason 2.** efficiency**:** if there are additional regression variables, the variance is smaller

>> ○ **reason 3.** conditional randomization

>>> ○ depending on the individual characteristics of a person, it may not be random even if it appears to be randomly extracted

>>> ○ random sampling with additional regression variables fixed can minimize such concerns

>>> ○ the following conditional independence must be satisfied for the β<sub>1</sub> estimator to be consistent: a weaker conditon than independence 

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/ccM7uV/btruvtMGpf1/EeryGktLxsLXkHUQbepiSk/img.png" alt="drawing" />

<br>

>> ○ interaction**:** treatment effect can depend on W

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/cs6n6C/btruuiR9ZPo/y0Q6kLjJxUuZa5QlnLHkq1/img.png" alt="drawing" />

<br>

⑶ threats to internal validity 

> ① failure to randomize

>> ○ not only the treatment effect but also the non-random assignment effect appears

>> ○ hypothesis test**:** if the coefficients are all zero when performing regression analysis of pre-treatment characteristics of W<sub>1i</sub>, ···, W<sub>ri</sub> by using X<sub>i</sub>, the experiment can be regarded as a randomized experiment 

>> ○ example**:** if random processing is performed by name, a specific ethnic groups may be assigned to the processing group preferentially 

> ② failure to follow treatment protocol (partial compliance)

>> ○ definition**:** even though random processing works well, subjects may not comply with the protocol well

>> ○ due to this, X<sub>i</sub> can be correlated with ui  

>> ○ randomized encourage design **:** partial complieance can be identified if the random treatment is the instrumental variable and the real treatment is analyzed under instrumental variable regression 

> ③ attrition

>> ○ definition**:** excluding subjects for reasons related to treatment after random sampling 

> ④ Hawthorne effect 

>> ○ definition **:** the subject's knowledge of what experiments he or she is carrying out can affect the results of the experiment

>> ○ in a new drug research, double blind test can be used to avoid this issue 

>> ○ difficult to perform double-blind test in econometrics

> ⑤ small sample

>> ○ because human-related research is expensive, the sample size is small

>> ○ many statistical estimations are based on asymptotic normality

>> ○ if the sample size is small, the sample should not be estimated by the normal distribution

⑷ threats to external validity 

> ① non-representative sample

>> ○ general econometric experiments target undergraduate volunteers

>> ○ volunteers are more motivated and can be overestimated in terms of meauring effects

> ② non-representative program or policy

>> ○ the experimental program or policy should be similar to the actual one

>> ○ example**:** the experimental program is performed for a short period of time. real-life areas of curiosity may require longer time

> ③ general equilibrium effect 

>> ○ definition**:** treatment changes the overall environment, which can amplify or suppress the effectiveness of treatment

>> ○ small experiments do not reflect changes in the environment, so external validity must be considered separately

<br>

<br>

## **5.** **quasi-experiment**

⑴ definition

> ① an experiment in which an independent variable is not under the control of a researcher and is conducted in a natural situation

> ② also known as natural experiment

> ③ objective**:** program evaluation

⑵ **method 1.** differences-in-differences (DID) estimator 

> ① the simplest model (assuming panel data)

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/ceKukp/btruqFHlNWR/5VNDoHEb8jksD6U7GWKGs0/img.jpg" alt="drawing" />

<b>Figure 3.</b> graphical representation of DID estimator

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/baZfWL/btruxkWbR0O/BLj2ugPKYbY8jGJzdbMZh0/img.png" alt="drawing" />

<br>

> ② model with additional regression variables (assuming panel data)**:** because conditions may change between before and after data

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/2XEcm/btruBbc9aUg/AsxZYGzN3Y77LL6LFyvpHk/img.png" alt="drawing" />

<br>

> ③ criterion for repeated cross-sectional data 

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/IDEnr/btrusPvUEGd/tr3w2CxMG19uReoAITMbwk/img.png" alt="drawing" />

<br>

⑶ **method 2.** instrumental variable regression 

> ① 1<sup>st</sup>. define Z<sub>i</sub> as a regression variable in a randomized controlled experiment

> ② 2<sup>nd</sup>. Zi is a good instrumental variable for Xi**:** instrument relevance is satistifed 

> ③ 3<sup>rd</sup>. Yi is the result of interest.

> ④ 4<sup>th</sup>. evaluate the effect of X<sub>i</sub> on Y<sub>i</sub> with Z<sub>i</sub> as an instrumental variable 

⑷ **method 3.** regression discontinuity design (RDD) 

> ① overview

>> ○ if you set the threshold (cut-off) ω<sub>0</sub>, the data near the threshold may be similar

>> ○ when data near the threshold is processed differently, the following difference can be completely seen as a treatment effect

>> ○ it is a very popular experimental technique

>> ○ disadvantage **:** difficult to apply a regression discontinuity design to the outlier

> ② sharp regression discontinuity design

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/ToNTQ/btruyOJxkHg/KvoklyZ5qM974jmT2QwK51/img.jpg" alt="drawing" />

<b>Figure 4.</b> sharp regression discontinuity design

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/bHupON/btrusOqgcmw/1dcSVdNj4BqGIlWwhQ3y8k/img.png" alt="drawing" />

<br>

> ③ fuzzy regression discontinuity design 

>> ○ the experiment may not be tested as smoothly as Xi defined in the sharp regression discontinuity design

>> ○ the following instrumental variable Zi can be a good instrumental variable on actual Xi  

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/GnlXc/btruyMEXjQv/kBIBhc9PcT93qDFj9PeNI1/img.png" alt="drawing" />

<br>

⑸ threats to internal validity

> ① failure to randomize

>> ○ not only the treatment effect but also the non-random assignment effect appears

>> ○ hypothesis test **:** if the coefficients are all zero when performing regression analysis of pre-treatment characteristics of W<sub>1i</sub>, ···, W<sub>ri</sub> by using X<sub>i</sub>, the experiment can be regarded as a randomized experiment 

>> ○ example **:** if random processing is performed by name, a specific ethnic group may be assigned to the processing group preferentially 

> ② failure to follow treatment protocol (partial compliance)

>> ○ definition **:** even though random processing works well, subjects may not comply with the protocol well

>> ○ due to this, X<sub>i</sub> can be correlated with ui

>> ○ randomized encourage design**:** partial compliance can be identified if the random treatment is the instrumental variable and the real treatment is analyzed under instrumental variable regression 

> ③ attrition 

>> ○ definition**:** excluding subjects for reasons related to treatment after random sampling

> ④ No Hawthorne effect 

>> ○ no reason to be cautious about the Hawthorne effect in a quasi-experiment**:** because it is a natural experiment

> ⑤ instrumental variance effectiveness 

>> ○ instrument relevance can be evaluated through data 

>> ○ instrument exogenity may not be established even if the instrumental variable appears to be randomly assigned

>> ○ example**:** Xi and ui may have a correlation while inducing low-numbered people to act to avoid conscription even if researchers want to see income according to the number of draft lottery 

⑹ threats to external validity  

> ① non-representative sample 

> ② non-representative program or policy 

> ③ general equilibrium effect 

⑺ criticism

> ① attempts are performed to find good variables in quasi-experiments

> ② there aren't that many really good quasi-experiments

<br>

<br>

## **6\. heterogeneous population** 

⑴ definition**:** the case that the coefficients of regression line β<sub>0i</sub> , β<sub>1i</sub> are not constants but vary according to to sample

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/dpUwtm/btruyN4VO9Z/9n8usQvSYqkt1ntrXbtUO0/img.png" alt="drawing" />

<br>

> ① β<sub>1i</sub> **:** heterogenous effect of X<sub>i</sub>

> ② the parameter of interest is E(β<sub>1i</sub>) 

> ③ if β<sub>1i</sub> is observable, models using interaction can be used 

> ④ if β<sub>1i</sub> is unobservable, it is analyzed as follows

⑵ OLS 

> ① assumption **:** X<sub>i</sub> should be random → X<sub>i</sub> and (u<sub>i</sub>, β<sub>0i</sub>, β<sub>1i</sub>) should be independent

>> ○ conditions that are difficult to satisfy in practice

> ② formula

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/0iPB8/btrupJwuw8Y/oH5ymVvZvu8V1pOeVMQaS1/img.png" alt="drawing" />

<br>

⑶ instrumental variables estimation (IV) 

> ① assumption **:** Z<sub>i</sub> should be random → Z<sub>i</sub> and (u<sub>i</sub>, v<sub>i</sub>, β<sub>0i</sub>, β<sub>1i</sub>, π<sub>0i</sub>, π<sub>1i</sub>) should be independent

> ② formula 

<br>

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/ehj7gV/btruxjbV1q3/GfPLZmKOx7xB4Q3ACWfmEk/img.png" alt="drawing" />

<br>

>> ○ E(β<sub>1i</sub>π<sub>1i</sub>) / E(π<sub>1i</sub>) is called local average treatment effect (LATE)

> ③ conditions for equalizing LATE and ATE

>> ○ **case 1.** β<sub>1i</sub> = β<sub>1</sub> = constant**:** no heteroscedasticity is required 

>> ○ **case 2.** π<sub>1i</sub> = π<sub>1</sub> **:** no heteroscedasticity in instumental variable

>> ○ **case 3.** β<sub>1i</sub> and π<sub>1i</sub> are independent 

> ④ connotations

>> ○ difficult to evaluate instrument exogenuity

>> ○ J-test only tells the difference between LATEs

<br>

---

*Input: 2019.11.26 10:29*
