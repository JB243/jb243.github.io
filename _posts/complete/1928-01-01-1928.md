## **Chapter 4. Eigenvalues and Eigenforms**

Recommended article: 【Linear Algebra】 [Linear Algebra Contents](https://jb243.github.io/pages/1014)

---

**1.** [Eigenvalues and Eigenvectors](#1-eigenvalues-and-eigenvectors)

**2.** [Diagonalization of Matrices](#2-diagonalization-of-matrices)

**3.** [Quadratic Forms](#3-quadratic-forms)

**4.** [Differential Equations and Eigenvalues](#4-differential-equations-and-eigenvalues)

---

<br>

## **1. Eigenvalues and Eigenvectors**

⑴ Eigenvalues and eigenvectors 

<br>

<img width="315" alt="스크린샷 2024-11-22 오전 11 06 29" src="https://github.com/user-attachments/assets/10e47df8-2077-4e9d-a508-b1af5ac4dec3">

<br>

> ① Overview: Used in diagonalizability, spectral clustering, etc.

> ② **Theorem 1.** The necessary and sufficient condition for A - λI to have a non-zero kernel in (A - λI)**x** = 0 is det(A - λI) = 0.

>> ○ Proof: [rank-nullity theorem](https://jb243.github.io/pages/513)

> ③ **Theorem 2.** When the eigenvalues of A ∈ ℝ<sup>2×2</sup> are λ<sub>1</sub>, λ<sub>2</sub>, A<sup>2</sup> - (λ<sub>1</sub> + λ<sub>2</sub>)A + λ<sub>1</sub>λ<sub>2</sub>I = O holds.

>> ○ Proof

>>> From the definition of eigenvalues, the following holds.

<br>

<img width="458" alt="스크린샷 2024-11-22 오전 11 07 36" src="https://github.com/user-attachments/assets/7cf7cc44-cf85-4b01-834a-8b442fa7872f">

<br>

>>> From this, it is easily known that λ<sub>1</sub> + λ<sub>2</sub> = a + d, and λ<sub>1</sub>λ<sub>2</sub> = ad - bc.

>>> Therefore, by the [Cayley-Hamilton theorem](https://jb243.github.io/pages/1897), the above proposition holds.

>>> Applying this to calculate A<sup>n</sup> as (A - λ<sub>1</sub>I)(A - λ<sub>2</sub>I) Q(A) + kA + sE for A<sup>n</sup> (for n ≥ 2).

> ④ **Theorem 3.** The determinant det(A) of a real matrix A of size n × n is equal to the product of its eigenvalues.

>> ○ For example, for a 3 × 3 matrix, the following holds.

>> ○ If A has three linearly independent eigenvectors, det(A) can be shown as follows:

<br>

<img width="345" alt="스크린샷 2025-03-22 오후 11 55 58" src="https://github.com/user-attachments/assets/dd68d7fc-95c0-4098-a641-dc4a3ffb7fb8" />

<br>
  
>> ○ If the eigenvalues are λ<sub>1</sub>, λ<sub>2</sub> with multiplicities 1 and 2, respectively, det(A) can be shown as follows:

<br>

<img width="364" alt="스크린샷 2025-03-22 오후 11 56 46" src="https://github.com/user-attachments/assets/664ce6fb-b882-4caf-8f49-7bfa8952e9a5" />

<br>
  
>> ○ Here, <b>w<sub>2</sub></b> is a generalized eigenvector corresponding to λ<sub>2</sub> and <b>v<sub>2</sub></b>.

>> ○ In this way, by considering each case, it can be easily shown that the product of the eigenvalues equals det(A).

> ⑤ **Theorem 4.** By applying the diagonalization of matrices, A = PDP<sup>-1</sup>, we can obtain A<sup>n</sup> = PD<sup>n</sup>P<sup>-1</sup>

> ⑥ **Theorem 5.** For real matrices A and B of size n × n, the eigenvalues of matrix AB are the same as the eigenvalues of matrix BA ([ref](https://jb243.github.io/pages/2406))

>> ○ Proof: Assume λ is an eigenvalue of AB

>>> ⇔ ABv = λv, v ≠ **0**

>>> ⇔ BABv = BA (Bv) = B · λv = λ (Bv)

>>> ⇔ Considering all possible cases as follows, the eigenvalues of BA are the same as those of AB

>>>> ○ When Bv ≠ **0**: λ is an eigenvalue of BA

>>>> ○ When Bv = **0**

>>>>> ⇔ ABv = A**0** = **0** = λv

>>>>> ⇔ λ = 0 is an eigenvalue of AB

>>>>> ⇔ 0 = det(AB) = det(A) × det(B) = det(BA)

>>>>> ⇔ Hence, there exists v' ≠ **0** such that BAv' = **0**

>>>>> ⇔ λ = 0 is an eigenvalue of BA

> ⑦ **Theorem 6.** For a real matrix A of size m × n, the nonzero eigenvalues of the matrices AA<sup>T</sup> and A<sup>T</sup>A are identical.  

>> ○ Related to **Theorem 5**.

> ⑧ **Theorem 7.** Kronecker product

>> ○ For example, given matrix J ∈ ℝ<sup>n×n</sup>, A ∈ ℝ<sup>m×m</sup>, M = J ⊗ A ∈ ℝ<sup>nm×nm</sup> can be defined as follows:

<br>

<img width="500" height="262" alt="스크린샷 2025-10-11 오후 11 52 33" src="https://github.com/user-attachments/assets/b114d3aa-c8e3-41a8-9832-37bdf930e645" />

<br>

>> ○ **Theorem 7-1.** Basic Operational Rules

<br>

<img width="336" height="133" alt="스크린샷 2025-10-11 오후 11 55 49" src="https://github.com/user-attachments/assets/6303ffa2-3516-4a83-a63e-f11c9eed8fa4" />

<br>

>> ○ **Theorem 7-2.** (A ⊗ B)<sup>T</sup> = A<sup>T<sup> ⊗ B<sup>T</sup>: this can be understood intuitively.

>> ○ **Theorem 7-3.** Multiplication rule for the Kronecker product: (A ⊗ B)(C ⊗ D) = (AC) ⊗ (BD)

>>> ○ For example, letting J = u·v<sup>T</sup> leads to (u ⊗ I)(v<sup>T</sup> ⊗ A) = (u·v<sup>T</sup>) ⊗ (I · A) = J ⊗ A = M.

>> ○ **Theorem 7-4.** rank(M) = rank(J ⊗ A) = rank(J) × rank(A)

>>> ○ Memorization tip. Because the Kronecker product multiplies components, its properties also tend to appear in multiplicative form.

>>> ○ Proof.

>>>> ○ Let J ∈ ℝ<sup>n×n</sup>, A ∈ ℝ<sup>m×m</sup>, and r = rank(J), s = rank(A). Then we obtain the following (by **Theorems 7-1** and **7-3**).

<br>

<img width="343" height="72" alt="스크린샷 2025-10-11 오후 11 58 24" src="https://github.com/user-attachments/assets/635e894d-93e3-45f7-85b9-463355c7829f" />

<br>

>>>> ○ Since M can be written as a sum of r·s rank-one vectors, the maximum possible value of rank(M) is r·s.

>>>> ○ In fact, those vectors are linearly independent, so the rank of (M) is exactly r·s.

>> ○ **Theorem 7-5.** The eigenvalues of M = J ⊗ A are all pairwise products of eigenvalues of J and eigenvalues of A.

<br>

<img width="450" height="32" alt="스크린샷 2025-10-11 오후 11 59 55" src="https://github.com/user-attachments/assets/08e2ff00-3fa5-4ee6-a776-a2871720d474" />

<br>

>> ○ **Theorem 7-6.** If x is an eigenvector of J and y is an eigenvector of A, then x ⊗ y is an eigenvector of M = J ⊗ A.

>>> ○ Proof.

<br>

<img width="450" height="32" alt="스크린샷 2025-10-12 오전 12 01 17" src="https://github.com/user-attachments/assets/c463fe4e-136b-4cd2-a0a5-62d493f46a4c" />

<br>

> ⑨ **Theorem 8.** Stable

>> ○ A square matrix A is defined to be stable if all of its eigenvalues have absolute value less than 1, since in that case A<sup>∞</sup> = 0.

>> ○ In this case, there exists K < ∞, 0 < α < 1 such that |(A<sup>j</sup>)<sub>pq</sub>| ≤ Kα<sup>j</sup> ∀p, q.

> ⑩ **Application 1.** [Hückel approximation](https://jb243.github.io/pages/1334) and Hamiltonian

> ⑪ Python code (cf. [WolframAlpha](https://www.wolframalpha.com/input?i=eigenvalues+4.56+10.2+7.68+13.68+10.2+33.2+20.2+24.8+7.68+20.2+17.84+23.44+13.68+24.8+23.44+45.44))

>> ○ Using numpy

<br>

```python
import numpy as np
matrix = np.array([
    [4.56, 10.2, 7.68, 13.68], 
    [10.2, 33.2, 20.2, 24.8],  
    [7.68, 20.2, 17.84, 23.44],
    [13.68, 24.8, 23.44, 45.55]
])
eigenvalues, eigenvectors = np.linalg.eigh(matrix)
sorted_indices = np.argsort(eigenvalues)[::-1]
sorted_eigenvalues = eigenvalues[sorted_indices]
sorted_eigenvectors = eigenvectors[:, sorted_indices]
print("Eigenvalues:\n", sorted_eigenvalues)
print("\nEigenvectors:\n", sorted_eigenvectors)
```

<br>

>> ○ Using sympy

<br>

```python
from sympy import Matrix

# Create SymPy matrix
matrix = Matrix([
    [4.56, 10.2, 7.68, 13.68], 
    [10.2, 33.2, 20.2, 24.8],  
    [7.68, 20.2, 17.84, 23.44],
    [13.68, 24.8, 23.44, 45.55]
])

# Calculate eigenvaleus and eigenvectors 
eigenvalues = matrix.eigenvals()  # 고유값 계산
eigenvectors = matrix.eigenvects()  # 고유벡터 계산

# Output 
print("Eigenvalues:")
for eigval, multiplicity in eigenvalues.items():
    print(f"Value: {eigval}, Multiplicity: {multiplicity}")

print("\nEigenvectors:")
for eigval, eigen_mult, eigvecs in eigenvectors:
    print(f"Eigenvalue: {eigval}")
    for vec in eigvecs:
        print(f"Eigenvector: {vec}")
```

<br>

 ⑵ Generalized eigenvectors

> ① Overview

>> ○ Definition: When the geometric multiplicity of an asymmetric matrix is less than the algebraic multiplicity, vectors that replace the lacking eigenvectors

>> ○ The generalized eigenvectors of matrix A of rank m are as follows

>>> ○ (A - λI)<sup>m</sup>**x**<sup>m</sup> = 0 & (A - λI)<sup>m-1</sup>**x**<sup>m</sup> ≠ **0**

>> ○ When m = 1, it is the same as the definition of eigenvector.

>> ○ Used in Jordan canonical form.

> ② **Example 1.** The eigenvalue of the following matrix A is 1, and the corresponding eigenvector is **v<sub>1</sub>** = (1, 0)<sup>T</sup>

<br>

<img width="109" alt="스크린샷 2024-11-22 오전 11 08 50" src="https://github.com/user-attachments/assets/311c5384-6abf-4e0c-b737-75e09dd9e5aa">

<br>

>> ○ The algebraic multiplicity of eigenvalue 1 is 2, and the geometric multiplicity is 1.

>> ○ (A - λI)<sup>2</sup> **v<sub>2</sub>** = (A - λI) {(A - λI) **v<sub>2</sub>**} = **0** ⇔ (A - λI) **v<sub>2</sub>** = **v<sub>1</sub>**

>> ○ **v<sub>2</sub>** = (*, 1)<sup>T</sup> (e.g., (0, 1)<sup>T</sup>) that satisfies (A - I)**v<sub>2</sub>** = (1, 0)<sup>T</sup> is called a generalized eigenvector.

> ③ **Example 2.** [Ref](https://jb243.github.io/pages/637)

⑶ Eigenfunctions

> ① Theorem of Sturm-Liouville: Different eigenfunctions are orthogonal to each other

<br>

<br>

## **2. Diagonalization of Matrices**

⑴ Diagonalization of Matrices

> ① Diagonal Matrix

>> ○ An n × n matrix where all elements except the diagonal are zero

<br>

<img width="139" alt="스크린샷 2024-11-22 오전 11 09 08" src="https://github.com/user-attachments/assets/696912bf-f148-40a7-b468-94b09898d08f">

<br>

>> ○ **Property 1.** The determinant is the product of the diagonal elements

>> ○ **Property 2.** The inverse matrix has the reciprocal of each diagonal element of the given diagonal matrix

> ② Diagonalizability

>> ○ Definition: A matrix A is diagonalizable if there exist a diagonal matrix D and an invertible matrix P satisfying the following

<br>

<img width="96" alt="스크린샷 2024-11-22 오전 11 09 54" src="https://github.com/user-attachments/assets/873e0a34-711f-43e9-b672-b63a42e3e416">

<br>

>> ○ Orthogonal Diagonalization: For a matrix P composed of [orthonormal basis vectors](https://jb243.github.io/pages/1898).

<br>

<img width="171" alt="스크린샷 2024-11-22 오전 11 10 12" src="https://github.com/user-attachments/assets/962e9286-887e-42d8-bedf-9a7daad564be">

<br>

>> ○ **Theorem 1.** The necessary and sufficient condition for an $n \times n$ matrix $A$ to be diagonalizable is that $A$ has $n$ linearly independent eigenvectors.

>> ○ **Theorem 2.** If an $n \times n$ matrix $A$ has $n$ distinct eigenvalues λ<sub>1</sub>, λ<sub>2</sub>, ···, λ<sub>n</sub>, it can be diagonalized

>>> ○ If $P$ is a matrix having linearly independent eigenvectors of $A$ as columns, $D$ is as follows:

<br>

<img width="255" alt="스크린샷 2024-11-22 오전 11 10 31" src="https://github.com/user-attachments/assets/c84cd813-66da-4bfa-9ed1-a97c1d710b27">

<br>

>> ○ **Theorem 3.** Spectral Theorem: An $n \times n$ matrix $A$ can be orthogonally diagonalized if and only if $A$ is a symmetric matrix.

>> ○ **Theorem 4.** Generalized eigenvectors of an $n \times n$ matrix $A$ always span ℝ<sup>n</sup>, thus such a matrix $A$ can be diagonalized.

>>> ○ Example: $n \times n$ matrix $A$ satisfying A<sup>2</sup> = A has eigenvalues λ = 0, 1 and can be diagonalized.

>> ○ **Theorem 5.** A matrix is diagonalizable if and only if its minimal polynomial factors into distinct linear factors.

>>> ○ Minimal polynomial: the minimal polynomial of $A$ is the nonzero polynomial $p$ of least degree such that $p(A)=O$ (the zero matrix).

>>> ○ The minimal polynomial is the greatest common divisor (gcd) of all polynomials $q$ such that $q(A)=O$.

>> ○ **Theorem 6.** If $D$ is a diagonal matrix with three distinct diagonal entries, then any matrix that commutes with $D$ must also be diagonal, and this diagonal matrix shares the same invertible matrix $P$ as $D$. Here, “commute” means satisfying $AB = BA$.

<br>

<img width="308" alt="스크린샷 2025-03-07 오후 11 45 18" src="https://github.com/user-attachments/assets/0e5070db-cb28-47c7-ab5e-cf6b17b6ac87" />

<br>

> ③ Examples

>> ○ Given matrix $A$

<br>

<img width="113" alt="스크린샷 2024-11-22 오전 11 10 47" src="https://github.com/user-attachments/assets/c20332aa-6759-4b94-964a-888504a700d8">

<br>

>> ○ Calculate eigenvalues and eigenvectors

<br>

<img width="306" alt="스크린샷 2024-11-22 오전 11 11 03" src="https://github.com/user-attachments/assets/aa54f90e-6bbb-4b4d-aff1-e22ea2439508">

<br>

>> ○ Calculate $P$ and $D$

<br>

<img width="436" alt="스크린샷 2024-11-22 오전 11 11 16" src="https://github.com/user-attachments/assets/e5ddc738-0a1a-4f63-a4b0-bffd01b1fd58">

<br>

⑵ Block Diagonalization of Matrices

> ① Block Diagonal Matrix

>> ○ Definition: When A<sub>i</sub> ∈ ℳ<sub>n<sub>i</sub>,n<sub>i</sub></sub> (F) and n<sub>1</sub> + ··· + n<sub>k</sub> = n, the matrix A in the following form

<br>

<img width="384" alt="스크린샷 2024-11-22 오전 11 09 36" src="https://github.com/user-attachments/assets/f812ada2-6f24-43c3-bb78-1232d47b9008">

<br>

>> ○ Properties: When A<sub>i</sub>, B<sub>i</sub> ∈ ℳ<sub>n<sub>i</sub>,n<sub>i</sub></sub> (F),

>>> ○ **Property 1.** diag(A<sub>1</sub>, ···, A<sub>k</sub>)·diag(B<sub>1</sub>, ···, B<sub>k</sub>) = diag(A<sub>1</sub>B<sub>1</sub>, ···, A<sub>k</sub>B<sub>k</sub>)

>>> ○ **Property 2.** If A<sub>i</sub> is invertible, (diag(A<sub>1</sub>, ···, A<sub>k</sub>))<sup>-1</sup> = diag(A<sub>1</sub><sup>-1</sup>, ···, A<sub>k</sub><sup>-1</sup>)

> ② Jordan Canonical Form

>> ○ When a matrix can't be diagonalized, but there are still generalized eigenvectors corresponding to the eigenvalues, it can be transformed into the Jordan canonical form

>> ○ Jordan Canonical Form: A = PJP<sup>-1</sup> (where J is not a complete diagonal matrix, but a matrix composed of Jordan blocks)

>> ○ **Theorem 1.** If A<sup>k</sup> = 0 (nilpotent), considering the Jordan form of the matrix A, A becomes a block diagonal matrix consisting of suitable A<sub>k</sub> matrices.

<br>

<img width="456" alt="스크린샷 2025-03-23 오전 12 11 52" src="https://github.com/user-attachments/assets/450e8641-5487-4667-9b7b-5920634017fd" />

>> ○ **Theorem 2.** If $A$ is diagonalizable and $B$ commutes with $A$ (i.e., $AB=BA$), then $B$ can be represented as a block-diagonal matrix, where each block corresponds to an eigenvalue of $A$.

>>> ○ **Proof.** If $Av=\lambda v$, then $A(Bv)=B(Av)=\lambda(Bv),$ so $Bv$ lies in the same eigenspace of $A$ as $v$. Hence, $B$ must preserve each eigenspace of $A$, which implies that $B$ is block-diagonal with respect to the decomposition into eigenspaces.

<br>

<br>

## **3. Quadratic Forms**

⑴ Definition: 4x<sub>1</sub><sup>2</sup> + 2x<sub>1</sub>x<sub>2</sub> + 3x<sub>2</sub><sup>2</sup> is a quadratic form, but 4x<sub>1</sub><sup>2</sup> + 2x<sub>1</sub> is not a quadratic form.

<br>

<img width="438" alt="스크린샷 2024-11-22 오전 11 11 30" src="https://github.com/user-attachments/assets/8a862556-356f-4c00-9a19-332ecd5d170f">

<br>

⑵ Representation using a symmetric matrix

<br>

<img width="595" alt="스크린샷 2024-11-22 오전 11 11 45" src="https://github.com/user-attachments/assets/5826ca6d-e3ed-47bb-8368-aa62e51f17d8">

<br>

⑶ Definition of signs of quadratic forms: For a symmetric matrix A ∈ ℳ<sub>n</sub> and any vector **x** ≠ **0** on ℝ<sup>n</sup>

> ① Positive definite matrix: All eigenvalues of A are greater than 0 ⇔ Q(**x**) > 0 ∀**x** ≠ 0 ⇔ **x**<sup>t</sup>A**x** = **x**<sup>t</sup>(λ**x**) = λ|**x**|<sup>2</sup> > 0

<br>

<img width="88" alt="스크린샷 2024-11-22 오전 11 11 59" src="https://github.com/user-attachments/assets/60004bc3-99bf-4d71-b987-de1a121dc6d8">

<br>

> ② Positive semi-definite matrix: All eigenvalues of A are greater than or equal to 0 ⇔ Q(**x**) ≥ 0 ∀**x** ≠ 0 ⇔ **x**<sup>t</sup>A**x** = **x**<sup>t</sup>(λ**x**) = λ|**x**|<sup>2</sup> ≥ 0 

<br>

<img width="88" alt="스크린샷 2024-11-22 오전 11 12 12" src="https://github.com/user-attachments/assets/fabbb85f-a64e-4609-9eda-6537ee65802d">

<br>

> ③ Negative definite matrix: All eigenvalues of A are less than 0 ⇔ Q(**x**) < 0 ∀**x** ≠ 0 ⇔ **x**<sup>t</sup>A**x** = **x**<sup>t</sup>(λ**x**) = λ|**x**|<sup>2</sup> < 0 

<br>

<img width="88" alt="스크린샷 2024-11-22 오전 11 12 24" src="https://github.com/user-attachments/assets/7bcf3a3d-e416-4415-8ee5-29763224962f">

<br>

> ④ Negative semi-definite matrix: All eigenvalues of A are less than or equal to 0 ⇔ Q(**x**) ≤ 0 ∀**x** ≠ 0 ⇔ **x**<sup>t</sup>A**x** = **x**<sup>t</sup>(λ**x**) = λ|**x**|<sup>2</sup> ≤ 0 

<br>

<img width="88" alt="스크린샷 2024-11-22 오전 11 12 35" src="https://github.com/user-attachments/assets/579d52b1-cf9c-4a44-b4e0-e22980b4f5d2">

<br>

> ⑤ Indefinite matrix

>> ○ When the signs of the eigenvalues are not uniform, unlike ① to ④

> ⑥ **Theorem 1.**

<br>

<img width="315" alt="스크린샷 2025-03-23 오전 12 16 32" src="https://github.com/user-attachments/assets/93fcbc23-d43b-4241-8afe-3741cb27b980" />

<br>

>> ○ Proof: Since a symmetric matrix A can be orthogonally diagonalizable (∵ spectral theorem) 

<br>

<img width="252" alt="스크린샷 2025-03-23 오전 12 16 50" src="https://github.com/user-attachments/assets/eaec2d04-c2f8-42a7-81e4-27e645ee4276" />

<br>

⑷ Determination of the signs of quadratic forms: For a symmetric matrix A ∈ ℳ<sub>n</sub> and a subset S =｛i<sub>1</sub>, i<sub>2</sub>, ···, i<sub>k</sub>｝ of ｛1, 2, ···, n｝

> ① Principal submatrix: A k × k matrix formed by selecting the rows and columns of A corresponding to S

<br>

<img width="452" alt="스크린샷 2024-11-22 오전 11 13 02" src="https://github.com/user-attachments/assets/cd232a5c-c496-4a42-ac47-d01729f08f40">

<br>

> ② Leading principal submatrix: A k × k matrix obtained by omitting the last n-k row and column vectors of A

<br>

<img width="244" alt="스크린샷 2024-11-22 오전 11 13 17" src="https://github.com/user-attachments/assets/8b48fd84-5abe-4c3a-9e6e-02855cafab61">

<br>

> ③ **Theorem 1.** A symmetric matrix A ∈ ℳ<sub>n</sub> being positive definite is equivalent to sgn(det(A<sub>k</sub>)) = 1, k = 1, 2, ···, n

> ④ **Theorem 2.** A symmetric matrix A ∈ ℳ<sub>n</sub> being negative definite is equivalent to sgn(det(A<sub>k</sub>)) = (-1)<sup>k</sup>, k = 1, 2, ···, n

> ⑤ **Theorem 3.** A symmetric matrix A ∈ ℳ<sub>n</sub> being positive semi-definite is equivalent to all principal submatrix determinants being non-negative

>> ○ If any leading principal submatrix determinant is 0 while attempting **Theorem 1** or **2**, try **Theorem 3**

>> ○ Obviously, if the sign of any leading principal submatrix determinant changes, there is no need to attempt **Theorem 3** or **4** as it indicates an indefinite matrix

> ⑥ **Theorem 4.** A symmetric matrix A ∈ ℳ<sub>n</sub> being negative semi-definite is equivalent to all principal submatrix determinants of -A being non-negative

>> ○ If the sign of any principal submatrix determinant is not uniform while attempting **Theorem 3**, try **Theorem 4**

> ⑦ sgn indicates 1 for positive values and -1 for negative values when the determinant is directly calculated

 ⑸ For a C<sup>2</sup> function f: Ω → ℝ defined on a region Ω ⊂ ℝ<sup>n</sup>, if at point **p** ∈ Ω, ∇f(**p**) = **0**, then the following holds

> ① If H<sub>f</sub>(**p**) is a positive definite matrix, then f has a local minimum at point **x** = **p**

> ② If H<sub>f</sub>(**p**) is a negative definite matrix, then f has a local maximum at point **x** = **p**

> ③ If H<sub>f</sub>(**p**) is an indefinite matrix, then point **x** = **p** is a saddle point of f

> ④ If H<sub>f</sub>(**p**) is a positive semi-definite matrix, then f has a relative minimum at point **x** = **p**

> ⑤ If H<sub>f</sub>(**p**) is a negative semi-definite matrix, then f has a relative maximum at point **x** = **p**

⑹ Determining the sign of a constrained quadratic form: For symmetric matrices A ∈ ℳ<sub>n</sub> and B ∈ ℳ<sub>m,n</sub> (with n ＞ m), given a n-dimensional quadratic form Q<sub>A</sub>(**x**) = **x**<sup>t</sup>A**x** with the linear constraint ｛**x** ∈ ℝ<sup>n</sup> | B**x** = **0**｝, the following statements hold when defining a new (m+n) × (m+n) matrix H as below.

<br>

<img width="119" alt="스크린샷 2024-11-22 오전 11 13 30" src="https://github.com/user-attachments/assets/41b8cc6c-dedd-4c5a-ac83-ac95e573d490">

<br>

> ① **Theorem 1.** If sgn(|H<sub>m+i</sub>|) = (-1)<sup>m</sup>, i = m+1, m+2, ···, n, then Q<sub>A</sub>(**x**) ＞ 0 holds

>> ○ Keep in mind it is not a necessary and sufficient condition

> ② **Theorem 2.** If sgn(|H<sub>m+i</sub>|) = (-1)<sup>i</sup>, i = m+1, m+2, ···, n, then Q<sub>A</sub>(**x**) ＜ 0 holds

>> ○ Keep in mind it is not a necessary and sufficient condition

> ③ Example: The following example observes a negative definite symmetric matrix A

<br>

<img width="494" alt="스크린샷 2024-11-22 오전 11 13 45" src="https://github.com/user-attachments/assets/18b6958b-942b-4e08-81d3-971f6452a4ea">

<br>

> ④ Even a quadratic form without a sign may have a sign under certain constraints

<br>

<br>

---

_Input: 2020.05.19 23:48_

_Modified: 2024.11.12 09:45_
