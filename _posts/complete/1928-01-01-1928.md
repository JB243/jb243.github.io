## **Chapter 4. Eigenvalues and Eigenforms**

Recommended article: 【Linear Algebra】 [Linear Algebra Contents](https://jb243.github.io/pages/1014)

---

**1.** [Eigenvalues and Eigenvectors](#1-eigenvalues-and-eigenvectors)

**2.** [Diagonalization of Matrices](#2-diagonalization-of-matrices)

**3.** [Quadratic Forms](#3-quadratic-forms)

**4.** [Differential Equations and Eigenvalues](#4-differential-equations-and-eigenvalues)

---

<br>

## **1. Eigenvalues and Eigenvectors**

⑴ Eigenvalues and eigenvectors 

<br>

<img width="315" alt="스크린샷 2024-11-22 오전 11 06 29" src="https://github.com/user-attachments/assets/10e47df8-2077-4e9d-a508-b1af5ac4dec3">

<br>

> ① Used in diagonalizability, spectral clustering, etc.

> ② **Theorem 1.** The necessary and sufficient condition for A - λI to have a non-zero kernel in (A - λI)**x** = 0 is det(A - λI) = 0

>> ○ Proof: [rank-nullity theorem](https://jb243.github.io/pages/513)

> ③ **Theorem 2.** When the eigenvalues of A ∈ ℝ2×2 are λ1, λ2, A2 \- (λ1 \+ λ2)A + λ1λ2I = O holds

>> ○ Proof

>>> From the definition of eigenvalues, the following holds

<br>

<img width="458" alt="스크린샷 2024-11-22 오전 11 07 36" src="https://github.com/user-attachments/assets/7cf7cc44-cf85-4b01-834a-8b442fa7872f">

<br>

>>> From this, it is easily known that λ1 \+ λ2 = a + d, and λ1λ2 = ad - bc

>>> Therefore, by the [Cayley-Hamilton theorem](https://jb243.github.io/pages/1897#:1.-,% B4 ,-\(CayleyHamiltontheorem), the proposition holds

>>> Applying this to calculate An as (A - λ1I)(A - λ2I) Q(A) + kA + sE for An (for n ≥ 2)

> ④ **Theorem 3.** By applying the diagonalization of matrices, A = PDP-1, we can obtain An = PDnP-1

> ⑤ **Theorem 4.** For real matrices A and B of size n × n, the eigenvalues of matrix AB are the same as the eigenvalues of matrix BA ([ref](https://jb243.github.io/pages/2406#:k1\)-,5,-.))

>> ○ Proof: Assume λ is an eigenvalue of AB

>>> ⇔ ABv = λv, v ≠ **0**

>>> ⇔ BABv = BA (Bv) = B · λv = λ (Bv)

>>> ⇔ Considering all possible cases as follows, the eigenvalues of BA are the same as those of AB

>>>> ○ When Bv ≠ **0**: λ is an eigenvalue of BA

>>>> ○ When Bv = **0**

>>>>> ⇔ ABv = A**0** = **0** = λv

>>>>> ⇔ λ = 0 is an eigenvalue of AB

>>>>> ⇔ 0 = det(AB) = det(A) × det(B) = det(BA)

>>>>> ⇔ Hence, there exists v' ≠ **0** such that BAv' = **0**

>>>>> ⇔ λ = 0 is an eigenvalue of BA

> ⑥ **Application 1.** [Hückel approximation](https://jb243.github.io/pages/1334#:-,,-\(Hckelmethod\)) and Hamiltonian

> ⑦ Python code (cf. [WolframAlpha](https://www.wolframalpha.com/input?i=eigenvalues+4.56+10.2+7.68+13.68+10.2+33.2+20.2+24.8+7.68+20.2+17.84+23.44+13.68+24.8+23.44+45.44))

>> ○ Using numpy

<br>

```python
import numpy as np
matrix = np.array([
    [4.56, 10.2, 7.68, 13.68], 
    [10.2, 33.2, 20.2, 24.8],  
    [7.68, 20.2, 17.84, 23.44],
    [13.68, 24.8, 23.44, 45.55]
])
eigenvalues, eigenvectors = np.linalg.eigh(matrix)
sorted_indices = np.argsort(eigenvalues)[::-1]
sorted_eigenvalues = eigenvalues[sorted_indices]
sorted_eigenvectors = eigenvectors[:, sorted_indices]
print("Eigenvalues:\n", sorted_eigenvalues)
print("\nEigenvectors:\n", sorted_eigenvectors)
```

<br>

>> ○ Using sympy

<br>

```python
from sympy import Matrix

# Create SymPy matrix
matrix = Matrix([
    [4.56, 10.2, 7.68, 13.68], 
    [10.2, 33.2, 20.2, 24.8],  
    [7.68, 20.2, 17.84, 23.44],
    [13.68, 24.8, 23.44, 45.55]
])

# Calculate eigenvaleus and eigenvectors 
eigenvalues = matrix.eigenvals()  # 고유값 계산
eigenvectors = matrix.eigenvects()  # 고유벡터 계산

# Output 
print("Eigenvalues:")
for eigval, multiplicity in eigenvalues.items():
    print(f"Value: {eigval}, Multiplicity: {multiplicity}")

print("\nEigenvectors:")
for eigval, eigen_mult, eigvecs in eigenvectors:
    print(f"Eigenvalue: {eigval}")
    for vec in eigvecs:
        print(f"Eigenvector: {vec}")
```

<br>

 ⑵ Generalized eigenvectors

> ① Overview

>> ○ Definition: When the geometric multiplicity of an asymmetric matrix is less than the algebraic multiplicity, vectors that replace the lacking eigenvectors

>> ○ The generalized eigenvectors of matrix A of rank m are as follows

>>> ○ (A - λI)m**x** m = 0 & (A - λI)m-1**x** m ≠ **0**

>> ○ When m = 1, it is the same as the definition of eigenvector

>> ○ Used in Jordan canonical form

> ② Example: The eigenvalue of the following matrix A is 1, and the corresponding eigenvector is **v 1** = (1, 0)T

<br>

<img width="109" alt="스크린샷 2024-11-22 오전 11 08 50" src="https://github.com/user-attachments/assets/311c5384-6abf-4e0c-b737-75e09dd9e5aa">

<br>

>> ○ The algebraic multiplicity of eigenvalue 1 is 2, and the geometric multiplicity is 1

>> ○ (A - λI)2 **v 2** = (A - λI) {(A - λI) **v 2**} = **0** ⇔ (A - λI) **v 2** = **v 1**

>> ○ **v 2** satisfies (A - I)**v 2 **= (1, 0)T, and **v 2** = (*, 1)T (e.g., (0, 1)T) is called a generalized eigenvector

 ⑶ Eigenfunctions

> ① Theorem of Sturm-Liouville: Different eigenfunctions are orthogonal to each other

<br>

<br>

## **2. Diagonalization of Matrices**

⑴ Diagonal Matrix

> ① An n × n matrix where all elements except the diagonal are zero

<br>

<img width="139" alt="스크린샷 2024-11-22 오전 11 09 08" src="https://github.com/user-attachments/assets/696912bf-f148-40a7-b468-94b09898d08f">

<br>

> ② Properties

>> ○ **Property 1.** The determinant is the product of the diagonal elements

>> ○ **Property 2.** The inverse matrix has the reciprocal of each diagonal element of the given diagonal matrix

⑵ Block Diagonal Matrix

> ① Definition: When Ai ∈ ℳni,ni (F) and n1 \+ ··· + nk = n, the matrix A in the following form

<br>

<img width="384" alt="스크린샷 2024-11-22 오전 11 09 36" src="https://github.com/user-attachments/assets/f812ada2-6f24-43c3-bb78-1232d47b9008">

<br>

> ② Properties: When Ai, Bi ∈ ℳni,ni (F),

>> ○ **Property 1.** diag(A1, ···, Ak)·diag(B1, ···, Bk) = diag(A1B1, ···, AkBk)

>> ○ **Property 2.** If Ai is invertible, (diag(A1, ···, Ak))-1 = diag(A1-1, ···, Ak-1)

 ⑶ Diagonalizability

> ① Definition: A matrix A is diagonalizable if there exist a diagonal matrix D and an invertible matrix P satisfying the following

<br>

<img width="96" alt="스크린샷 2024-11-22 오전 11 09 54" src="https://github.com/user-attachments/assets/873e0a34-711f-43e9-b672-b63a42e3e416">

<br>

> ② Orthogonal Diagonalization: For a matrix P composed of orthonormal basis vectors

<br>

<img width="171" alt="스크린샷 2024-11-22 오전 11 10 12" src="https://github.com/user-attachments/assets/962e9286-887e-42d8-bedf-9a7daad564be">

<br>

> ③ **Theorem 1.** The necessary and sufficient condition for an n × n matrix A to be diagonalizable is that A has n linearly independent eigenvectors

> ④ **Theorem 2.** If an n × n matrix A has n distinct eigenvalues λ1, λ2, ···, λn, it can be diagonalized

>> ○ If P is a matrix having linearly independent eigenvectors of A as columns, D is as follows

<br>

<img width="255" alt="스크린샷 2024-11-22 오전 11 10 31" src="https://github.com/user-attachments/assets/c84cd813-66da-4bfa-9ed1-a97c1d710b27">

<br>

> ⑤ **Theorem 3.** Spectral Theorem: An n × n matrix A can be orthogonally diagonalized if and only if A is a symmetric matrix

> ⑥ **Theorem 4.** Generalized eigenvectors of an n × n matrix A always span ℝn, thus such a matrix A can be diagonalized

>> ○ Example: An n × n matrix satisfying A<sup>2</sup> = A has eigenvalues λ = 0, 1 and can be diagonalized.

> ⑦ **Theorem 5.** A matrix that commutes with a diagonal matrix D having three distinct diagonal elements is also a diagonal matrix.

<br>

<img width="308" alt="스크린샷 2025-03-07 오후 11 45 18" src="https://github.com/user-attachments/assets/0e5070db-cb28-47c7-ab5e-cf6b17b6ac87" />

<br>

⑷ Jordan Canonical Form

> ① When a matrix can't be diagonalized, but there are still generalized eigenvectors corresponding to the eigenvalues, it can be transformed into the Jordan canonical form

> ② Jordan Canonical Form: A = PJP-1 (where J is not a complete diagonal matrix, but a matrix composed of Jordan blocks)

 ⑸ Examples

> ① Given matrix A

<br>

<img width="113" alt="스크린샷 2024-11-22 오전 11 10 47" src="https://github.com/user-attachments/assets/c20332aa-6759-4b94-964a-888504a700d8">

<br>

> ② Calculate eigenvalues and eigenvectors

<br>

<img width="306" alt="스크린샷 2024-11-22 오전 11 11 03" src="https://github.com/user-attachments/assets/aa54f90e-6bbb-4b4d-aff1-e22ea2439508">

<br>

> ③ Calculate P and D

<br>

<img width="436" alt="스크린샷 2024-11-22 오전 11 11 16" src="https://github.com/user-attachments/assets/e5ddc738-0a1a-4f63-a4b0-bffd01b1fd58">

<br>

<br>

## **3. Quadratic Forms**

⑴ Definition: 4x12 \+ 2x1x2 \+ 3x22 is a quadratic form, but 4x12 \+ 2x1 is not a quadratic form

<br>

<img width="438" alt="스크린샷 2024-11-22 오전 11 11 30" src="https://github.com/user-attachments/assets/8a862556-356f-4c00-9a19-332ecd5d170f">

<br>

⑵ Representation using a symmetric matrix

<br>

<img width="595" alt="스크린샷 2024-11-22 오전 11 11 45" src="https://github.com/user-attachments/assets/5826ca6d-e3ed-47bb-8368-aa62e51f17d8">

<br>

⑶ Definition of signs of quadratic forms: For a symmetric matrix A ∈ ℳn and any vector **x** ≠ **0** on ℝn

> ① Positive definite matrix: All eigenvalues of A are greater than 0 ⇔ Q(**x**) > 0 for all **x** ≠ **0** ⇔

<br>

<img width="88" alt="스크린샷 2024-11-22 오전 11 11 59" src="https://github.com/user-attachments/assets/60004bc3-99bf-4d71-b987-de1a121dc6d8">

<br>

> ② Positive semi-definite matrix: All eigenvalues of A are greater than or equal to 0 ⇔ Q(**x**) ≥ 0 for all **x** ≠ **0** ⇔

<br>

<img width="88" alt="스크린샷 2024-11-22 오전 11 12 12" src="https://github.com/user-attachments/assets/fabbb85f-a64e-4609-9eda-6537ee65802d">

<br>

> ③ Negative definite matrix: All eigenvalues of A are less than 0 ⇔ Q(**x**) < 0 for all **x** ≠ **0** ⇔

<br>

<img width="88" alt="스크린샷 2024-11-22 오전 11 12 24" src="https://github.com/user-attachments/assets/7bcf3a3d-e416-4415-8ee5-29763224962f">

<br>

> ④ Negative semi-definite matrix: All eigenvalues of A are less than or equal to 0 ⇔ Q(**x**) ≤ 0 for all **x** ≠ **0** ⇔

<br>

<img width="88" alt="스크린샷 2024-11-22 오전 11 12 35" src="https://github.com/user-attachments/assets/579d52b1-cf9c-4a44-b4e0-e22980b4f5d2">

<br>

> ⑤ Indefinite matrix: When the signs of the eigenvalues are not uniform, unlike ① to ④

> ⑥ Proof

>> ○ Let A = PΛPT (where Λ is a diagonal matrix) and **y** = PT**x**, then,

>> **○ x** TA**x** = **y** TΛ**y**, thus it is easy to confirm the relationship between the signs of the eigenvalues and **x** TA**x**

> ⑦ Application

<br>

<img width="297" alt="스크린샷 2024-11-22 오전 11 12 48" src="https://github.com/user-attachments/assets/1e563580-e633-4c2b-99ac-7783388c6ce0">

<br>

⑷ Determination of the signs of quadratic forms: For a symmetric matrix A ∈ ℳn and a subset S =｛i1, i2, ···, ik｝ of ｛1, 2, ···, n｝

> ① Principal submatrix: A k × k matrix formed by selecting the rows and columns of A corresponding to S

<br>

<img width="452" alt="스크린샷 2024-11-22 오전 11 13 02" src="https://github.com/user-attachments/assets/cd232a5c-c496-4a42-ac47-d01729f08f40">

<br>

> ② Leading principal submatrix: A k × k matrix obtained by omitting the last n-k row and column vectors of A

<br>

<img width="244" alt="스크린샷 2024-11-22 오전 11 13 17" src="https://github.com/user-attachments/assets/8b48fd84-5abe-4c3a-9e6e-02855cafab61">

<br>

> ③ **Theorem 1.** A symmetric matrix A ∈ ℳn being positive definite is equivalent to sgn(det(Ak)) = 1, k = 1, 2, ···, n

> ④ **Theorem 2.** A symmetric matrix A ∈ ℳn being negative definite is equivalent to sgn(det(Ak)) = (-1)k, k = 1, 2, ···, n

> ⑤ **Theorem 3.** A symmetric matrix A ∈ ℳn being positive semi-definite is equivalent to all principal submatrix determinants being non-negative

>> ○ (Note) If any leading principal submatrix determinant is 0 while attempting **Theorem 1** or **2**, try **Theorem 3**

>> ○ Obviously, if the sign of any leading principal submatrix determinant changes, there is no need to attempt **Theorem 3** or **4** as it indicates an indefinite matrix

> ⑥ **Theorem 4.** A symmetric matrix A ∈ ℳn being negative semi-definite is equivalent to all principal submatrix determinants of -A being non-negative

>> ○ (Note) If the sign of any principal submatrix determinant is not uniform while attempting **Theorem 3**, try **Theorem 4**

> ⑦ (Reference) sgn indicates 1 for positive values and -1 for negative values when the determinant is directly calculated

 ⑸ For a C2 function f: Ω → ℝ defined on a region Ω ⊂ ℝn, if at point **p** ∈ Ω, ∇f(**p**) = **0**, then the following holds

> ① If Hf(**p**) is a positive definite matrix, then f has a local minimum at point **x** = **p**

> ② If Hf(**p**) is a negative definite matrix, then f has a local maximum at point **x** = **p**

> ③ If Hf(**p**) is an indefinite matrix, then point **x** = **p** is a saddle point of f

> ④ If Hf(**p**) is a positive semi-definite matrix, then f has a relative minimum at point **x** = **p**

> ⑤ If Hf(**p**) is a negative semi-definite matrix, then f has a relative maximum at point **x** = **p**

 ⑹ Determining the sign of a constrained quadratic form: For symmetric matrices A ∈ ℳn and B ∈ ℳm, n (with n ＞ m), given a n-dimensional quadratic form QA(**x**) = **x** tA**x** with the linear constraint ｛**x** ∈ ℝn | B**x** = **0**｝, the following statements hold when defining a new (m+n) × (m+n) matrix H as below.

<br>

<img width="119" alt="스크린샷 2024-11-22 오전 11 13 30" src="https://github.com/user-attachments/assets/41b8cc6c-dedd-4c5a-ac83-ac95e573d490">

<br>

> ① **Theorem 1.** If sgn(|Hm+i|) = (-1)m, i = m+1, m+2, ···, n, then QA(**x**) ＞ 0 holds

>> ○ (Note) Keep in mind it is not a necessary and sufficient condition

> ② **Theorem 2.** If sgn(|Hm+i|) = (-1)i, i = m+1, m+2, ···, n, then QA(**x**) ＜ 0 holds

>> ○ (Note) Keep in mind it is not a necessary and sufficient condition

> ③ Example: The following example observes a negative definite symmetric matrix A

<br>

<img width="494" alt="스크린샷 2024-11-22 오전 11 13 45" src="https://github.com/user-attachments/assets/18b6958b-942b-4e08-81d3-971f6452a4ea">

<br>

> ④ (Reference) Even a quadratic form without a sign may have a sign under certain constraints

<br>

<br>

## **4. Differential Equations and Eigenvalues**

⑴ Systems of Linear Ordinary Differential Equations: For systems of linear ordinary differential equations represented by a 2 × 2 matrix A,

<br>

<img width="178" alt="스크린샷 2024-11-22 오전 11 14 10" src="https://github.com/user-attachments/assets/0f5f8e34-c633-45da-b968-ed09d000a382">

<br>

> ① **Case 1.** If A has eigenvalues λ1, λ2 and is diagonalizable: The solution can be expressed as follows with eigenvectors v1, v2 corresponding to λ1, λ2

<br>

<img width="338" alt="스크린샷 2024-11-22 오전 11 14 22" src="https://github.com/user-attachments/assets/01caa972-6b46-4b2c-865d-0b77d121eb82">

<br>

> ② **Case 2.** If A has complex eigenvalues p ± qi: The solution can be expressed as follows

<br>

<img width="405" alt="스크린샷 2024-11-22 오전 11 14 38" src="https://github.com/user-attachments/assets/bd67de24-ce53-4d3f-8fc7-aacd665fc59a">

<br>

> ③ **Case 3.** If A has a single eigenvalue λ with multiplicity 2 and is not diagonalizable, the general solution x together with generalized eigenvectors is as follows

<br>

<img width="365" alt="스크린샷 2024-11-22 오전 11 14 51" src="https://github.com/user-attachments/assets/a8b180e0-2f28-4044-8add-c568b94bf9d9">

<br>

⑵ Linear Approximation

> ① Example: For x' = x3 \- y, y' = x, the classification of the equilibrium point (0, 0)

> ② **Step 1.** The following approximations can be found

<br>

<img width="335" alt="스크린샷 2024-11-22 오전 11 15 04" src="https://github.com/user-attachments/assets/02acbb2b-321f-4344-978d-609db42115e9">

<br>

> ③ **Step 2.** Construct the Jacobian matrix

<br>

<img width="259" alt="스크린샷 2024-11-22 오전 11 15 17" src="https://github.com/user-attachments/assets/1a823653-1518-4cd6-a272-7a777768c25c">

<br>

> ④ **Step 3.** Calculate eigenvalues

>> ○ When all eigenvalues are negative real numbers: The equilibrium point is stable

>> ○ When one eigenvalue is positive and the other is negative: The equilibrium point is a saddle

>> ○ When all eigenvalues are positive real numbers: The equilibrium point is unstable

>> ○ When all eigenvalues are complex numbers: The equilibrium point is a spiral

>> ○ Since A's eigenvalues are ± i, a spiral pattern appears around (0, 0)

<br>

![image](https://github.com/user-attachments/assets/6ca0c234-a734-46ae-9d23-17c63934f496)

**Figure 1.** Stream plot of the given system

<br>

---

_Input: 2020.05.19 23:48_

_Modified: 2024.11.12 09:45_
