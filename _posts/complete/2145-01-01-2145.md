## **Chapter 21. Information Theory**

Higher category: 【Statistics】 [Statistics Table of Contents](https://jb243.github.io/pages/1641)

---

**1.** [Information Theory](#1-information-theory)

**2.** [Variational Inference](#2-variational-inference)

**3.** [Modern Information Theory](#3-modern-information-theory)

---

<br>

## **1. Information Theory** 

⑴ Overview

> ① low probability event: high information (surprising)

> ② high probability event: low information (unsurprising)

⑵ **entropy**

> ① Problem situation: X is Raining / Not Raining. Y is Cloudy / Not Cloudy

<br>

![image](https://github.com/user-attachments/assets/f4447d82-f083-4fba-bfaf-878ce3eddad5)

**Figure. 1.** Entropy example

<br>

> ② Basic concepts of statistics

<br>

![image](https://github.com/user-attachments/assets/90a112db-da94-4402-b82e-1fec9f6f8250)

<br>

> ③ **Information** (information content) or **Uncertainty** (surprisal, unexpectedness, randomness)

>> ○ The amount of mathematical uncertainty measured.

<br>

![image](https://github.com/user-attachments/assets/eecbe1e8-4df2-4734-b607-b052a68d2970)

<br>

>> ○ When the base is 2, the unit used is **bits**

>> ○ When the base is e (natural logarithm), the unit used is **nats**

>> ○ **Feature 1.** If p(x<sub>i</sub>) = 1, I(x<sub>i</sub>) = 0

>> ○ **Feature 2.** I(x<sub>i</sub>) ≥ 0

>> ○ **Feature 3.** If P(x = x<sub>i</sub>) < P(x = x<sub>j</sub>), I(x<sub>i</sub>) > I(x<sub>j</sub>)

>> ○ **Feature 4.** If x<sub>i</sub> and x<sub>j</sub> are independent, I(x<sub>i</sub>x<sub>j</sub>) = I(x<sub>i</sub>) + I(x<sub>j</sub>)

> ④ **entropy** (self information, Shannon entropy)

<br>

![image](https://github.com/user-attachments/assets/1cc47b13-3030-4c88-a752-67883f0a78e0)

<br>

>> ○ Understanding

>>> ○ In other words, entropy is defined as the average value of uncertainty

>>> ○ Similar to entropy in physics, higher values are obtained as disorder increases: higher entropy means higher heterogeneity

>>> ○ It applies the surprisal (-log p(x)) of each event to the probability weight (p(x)) of each event

>>> ○ Redundancy = Maximum entropy - Actual entropy 

>> ○ Examples

>>> ○ Fair coin toss: H = -0.5 log<sub>2</sub> 0.5 - 0.5 log<sub>2</sub> 0.5 = 1

>>> ○ Fair die ****: H = 6 × (-(1/6) log2 (1/6)) = log<sub>2</sub> 6 = 2.58

>>> ○ If X follows a uniform distribution, H(X) = log n holds

>> ○ **Source Coding Theorem**

>>> ○ Definition: For a very long data sequence, if the compression rate is maximized, the average number of bits of the compressed data approaches the entropy.

>>> ○ In other words, it refers to the number of bits required to represent a given set of possibilities.

>>> ○ When stored in base log2, it represents the number of bits; when stored in base logk, it indicates the required storage capacity in a k-ary system.

>>> ○ Kraft's Inequality: A constraint that ensures every entity can be represented by a unique bit sequence.

<br>

<img width="110" alt="스크린샷 2024-10-01 오후 1 26 20" src="https://github.com/user-attachments/assets/2c4e1074-4ad3-4761-bc84-d71a8a2e7e60">

<br>

>>> ○ Proof of the Source Coding Theorem: For the average number of bits p<sub>1</sub>L<sub>1</sub> + ··· + p<sub>n</sub>L<sub>n</sub>, 

<br>

<img width="408" alt="스크린샷 2024-10-01 오후 1 26 39" src="https://github.com/user-attachments/assets/9eead6f7-6a45-4122-97cd-78e3419c5bb2">

<br>

>> ○ **Feature 1.** H(X) is shift-invariant

>>> ○ When Y = X + a, H(Y) = H(X) is established because p<sub>Y</sub>(y) = p<sub>X</sub>(x)

>> ○ **Feature 2.** H(X) ≥ 0

>> ○ **Feature 3.** H<sub>b</sub>(X) = (log<sub>b</sub>a) H<sub>a</sub>(X) (where a and b are under the log)

>>> ○ log<sub>b</sub>p = (log<sub>b</sub>a) log<sub>a</sub>p

>> ○ [Python code](https://jb243.github.io/pages/1892#6-data-science)

<br>

```python
import numpy as np

def shannon_index(data):
    # Count unique values and their occurrences in the data
    values, counts = np.unique(data, return_counts=True)
    # Calculate proportions using the counts
    proportions = counts / sum(counts)
    # Calculate Shannon index, considering only non-zero proportions
    return -np.sum(proportions * np.log(proportions))

# Example data, where '1' appears 3 times, '2' appears 2 times, and '3' appears 1 time
data = [1, 1, 1, 2, 2, 3]

# Calculate the Shannon index
index = shannon_index(data)
print(f"Shannon index: {index}")
```

<br>

> ⑤ **joint entropy**

<br>

![image](https://github.com/user-attachments/assets/a1629e13-50e6-41dc-ab4c-d3e019f43956)

<br>

>> ○ **Feature 1.** H(X, Y) ≤ H(X) + H(Y): If X and Y are independent, H(X, Y) = H(X) + H(Y)

<br>

![image](https://github.com/user-attachments/assets/99f05cd3-ae07-47be-9fea-6b47cf76bab6)

<br>

> ⑥ Conditional entropy 

<br>

<img width="399" alt="스크린샷 2024-10-01 오후 1 27 05" src="https://github.com/user-attachments/assets/79573d57-e38c-4814-a98a-c6b406887948">

**Figure 2.** Conditional entropy visualization

<br>

>> ○ Conditional entropy for a specific condition

<br>

![image](https://github.com/user-attachments/assets/02940ee2-e413-4764-86c1-c5908990fad6)

<br>

>> ○ Total conditional entropy

<br>

![image](https://github.com/user-attachments/assets/7515233b-4df3-4c8e-bb12-633fb81515ca)

<br>

>> ○ Information gain (IG): The increase in order when knowing the information X

<br>

![image](https://github.com/user-attachments/assets/3cafead2-4743-49b1-b16c-a31f94078af7)

<br>

>> ○ Information bottleneck method (IB): Describing the trade-off between complexity and accuracy using information theory

>>> ○ Application: For example, it can determine the number of clusters in given data.

>> ○ **Feature 1.** <span>If X and Y are independent, H(Y | X) = H(Y) (cf. H(Y | X) ≤ H(Y))</span>

>> ○ **Feature 2.** <span>H(Y | Y) = 0</span>

>> ○ **Feature 3.** <span>H(X, Y) = H(X) + H(Y | X) = H(Y) + H(X | Y) (Chain rule)</span>

<br>

![image](https://github.com/user-attachments/assets/3f18240f-075c-4472-a5d7-654367f1ec05)

<br>

>> ○ **Feature 4.** <span>H(X, Y | Z) = H(X | Z) + H(Y | X, Z)</span>

<br>

![image](https://github.com/user-attachments/assets/db9b3304-b22b-45c1-ba1a-fc5144bc54c4)

<br>

> ⑦ Mutual information

<br>

![image](https://github.com/user-attachments/assets/48c3b578-404b-4a60-8716-075f76d167a4)

**Figure 3.** Mutual information visualization

<br>

![image](https://github.com/user-attachments/assets/cf5601dc-4005-46e4-90eb-aac7d2ea3213)

<br>

>> ○ Information gain and mutual information are equivalent.

<br>

![image](https://github.com/user-attachments/assets/623c7074-1df2-4754-aea9-462b3bd902cd)

<br>

>> ○ **Feature 1.** <span>I(X; Y) = H(Y) - H(Y | X) = H(X) - H(X | Y)</span>

>> ○ **Feature 2.** Mutual information ≥ 0 (equality condition: X and Y are independent)

<br>

![image](https://github.com/user-attachments/assets/59461716-2a41-4e0e-a12d-c71b84cd7176)

<br>

>> ○ **Feature 3.** <span>H(X | Y) ≤ H(X)</span> (equality condition: X and Y are independent)

<br>

![image](https://github.com/user-attachments/assets/35f238be-2a97-43df-91d4-03ba6f4da15e)

<br>

>> ○ **Feature 4.** I(X; Y) ≤ H(X, Y): H(X, Y) - I(X; Y) satisfies the definition of [mathematical distance](https://jb243.github.io/pages/1897).

>> ○ **Application 1.** Mutual information can be used for assessing image [similarity](https://jb243.github.io/pages/879).

>> ○ **Application 2.** mRMR (maximum relevance minimum redundancy): A technique used to select features that have high mutual information with the output while having low mutual information among themselves. It is commonly used in machine learning. Matlab code is as follows:

<br>

```python
% Rank features using mRMR
selected_features_idx = fscmrmr(features, labels);
```

<br>

> ⑧ Relative entropy (Kullback Leibler divergence, KL divergence, KLD) 

<br>

![image](https://github.com/user-attachments/assets/8b185bbb-b87b-4aed-9c94-13473e673ecb)

<br>

>> ○ Overview

>>> ○ A measure of how two probability distributions diverge.

>>> ○ It indicates the divergence between an approximate distribution p(x), which aims to mimic the true distribution, and the true distribution q(x).

>>> ○ Typically, the true distribution is a complex probability distribution function, while the approximate distribution is represented using a parameterized model.

>>> ○ The approximate distribution can be made manageable, often resembling a simple distribution like the normal distribution.

>>> ○ Strictly speaking, it does not satisfy the definition of [mathematical distance](https://jb243.github.io/pages/1897).

>> ○ **Feature 1.** asymmetry: <span>D<sub>KL</sub>(p(x) || q(x)) ≠ D<sub>KL</sub>(q(x) || p(x)) (∴ violates the definition of [mathematical distance](https://jb243.github.io/pages/1897#2-vector-space))</span>

<br>

![image](https://github.com/user-attachments/assets/c8c4f819-5503-4f00-87d3-7b3dd8467844)

**Figure 4.** The difference between D<sub>KL</sub>(p <span>||</span> q) and D<sub>KL</sub>(q <span>||</span> p)

<br>

>> ○ **Feature 2.** <span>D(p || q) ≥ 0 (equality condition: p(x) = q(x))</span>

<br>

![image](https://github.com/user-attachments/assets/83f900e8-5a68-4c98-addf-9f05977d65b5)

<br>

>> ○ **Feature 3.** H(X) ≤ log n (where n is the number of elements X can take)

<br>

![image](https://github.com/user-attachments/assets/1a2088c2-6894-4ea2-8401-4f826c073399)

<br>

>> ○ **Application.** Determining the predictive data distribution p with the smallest KL distance to the true unknown data distribution q.

>>> ○ Maximizing log-likelihood minimizes KL distance.

<br>

![image](https://github.com/user-attachments/assets/ad9dee99-bddb-4510-b47e-49df98ab4fa1)

<br>

>> ○ **Application.** Jensen-Shannon divergence (JSD)

<br>

<img width="320" alt="스크린샷 2024-10-05 오후 8 19 21" src="https://github.com/user-attachments/assets/7da22610-67d2-4a0f-a0a3-2c58ed8fcfdf">

<br>

> ⑨ **cross entropy**

>> ○ Definition: Average number of bits required to distinguish two probability distributions, p and q.

>> ○ In machine learning, model prediction.

<br>

<img width="368" alt="스크린샷 2024-10-01 오후 1 29 36" src="https://github.com/user-attachments/assets/53aeb867-600e-4992-a32f-81d2ff344d9a">

<br>

>> ○ **Feature 1.** <span>-∑ p<sub>i</sub> log q<sub>i</sub> = H(p) + D<sub>KL</sub>(p || q)</span>

>> ○ **Feature 2.** <span>Minimizing D<sub>KL</sub>(p || q) is the same as minimizing cross-entropy because H(p) is constant.</span>

>> ○ **Feature 3.** When minimizing -∑ p<sub>i</sub> log qi under the assumption that p<sub>i</sub> follows a uniform distribution, this is called [maximum likelihood estimation](https://jb243.github.io/pages/1630).

> ⑩ Asymptotic equipartition property (AEP)

<br>

![image](https://github.com/user-attachments/assets/2b493dbc-eb31-4576-bb58-a747e1ae8770)

<br>

⑶ **Comparison 1.** `Rényi Entropy`

> ① Hartley (max) entropy: For a source represented by M symbols (e.g., if expressed with the alphabet, M = 26)

<br>

<img width="99" alt="스크린샷 2024-10-01 오후 1 42 57" src="https://github.com/user-attachments/assets/dfc6dad8-e734-4a56-8ea6-12ac59454d20">

<br>

> ② Min entropy: Related to signal processing

<br>
 
<img width="164" alt="스크린샷 2024-10-01 오후 1 43 23" src="https://github.com/user-attachments/assets/3a4c9f50-0410-42b8-847e-e0e4ca37ad74">

<br>

> ③ Rényi entropy: Encompasses Shannon entropy (α → 1; by [L'Hôpital's rule](https://jb243.github.io/pages/1810)), max entropy (α → 0), and min entropy (α → ∞)

<br>

<img width="209" alt="스크린샷 2024-10-01 오후 1 43 42" src="https://github.com/user-attachments/assets/50945683-db17-4a63-be38-7ba92b3f8181">

<br>

> ④ Rényi divergence: As α → ∞, Rényi divergence converges to KL divergence (∵ [L'Hôpital's rule](https://jb243.github.io/pages/1810))

<br>

<img width="504" alt="스크린샷 2024-10-01 오후 1 44 09" src="https://github.com/user-attachments/assets/9d1695e0-3858-4a2f-b4c7-7d890a7a9ae3">

<br>

⑷ **Comparison 2.** Sample Entropy

> ① A dynamic measure that reflects time

> ② Important in signal processing

⑸ **Comparison 3.** `Gini impurity`

<br>

![image](https://github.com/user-attachments/assets/88b5c89d-77b8-49c2-8e50-a9db6bb72306)

<br>

> ① Entropy is not the unique measure, alternative measures like Gini impurity are proposed.

> ② Gini impurity considers the probability of mislabeling (1 - P(x<sub>i</sub>)) weighted by the weight of each sample (P(x<sub>i</sub>)).

> ③ Both Gini impurity and entropy increase as the given data becomes more disordered: Origin of impurity.

> ④ (Conceptual Distinction) The Gini Coefficient in Economics

>> ○ The concept was first introduced to evaluate income inequality: It can assess not only income but also the degree of inequality in the distribution of a discrete variable.

>> ○ As the Gini coefficient moved from economics to the fields of machine learning and information theory, it implies similar inequality, but the mathematical definition changes.

>> ○ Difference: If the distribution of the variable is entirely uniform,

>>> ○ In economics, the Gini coefficient becomes extremely egalitarian, so it has a value of 0.

>>> ○ In information theory, the Gini coefficient attains its maximum value due to the Cauchy-Schwarz inequality. Intuitively, it becomes most disordered, thus reaching the maximum value.

⑹ **Comparison 4.** `Connectivity index (CI)`

> ① In a spatial lattice, if a specific spot and its nearest neighbor are in different clusters, the CI value increases.

> ② Therefore, a higher CI value implies higher heterogeneity.

⑺ **Comparison 5.** `Simpson index`

> ① Another measure based on information theory, along with Shannon entropy

> ② Based on spatial transcriptomics, the probability of two random spots belonging to the same cluster ([ref](https://genomemedicine.biomedcentral.com/articles/10.1186/s13073-023-01185-4#Sec2))

> ③ The lower the Simpson index, the higher the heterogeneity implied

⑻ **Comparison 6.** `Cluster modularity`

> ① Based on spatial transcriptomics, a metric indicating the purity (homogeneity) of spots within a cluster ([ref](https://genomemedicine.biomedcentral.com/articles/10.1186/s13073-023-01185-4#Sec2))

⑼ **Comparison 7.** `Lempel-Ziv complexity`

⑽ **Comparison 8.** `Silhouette coefficient`

⑾ **Comparison 9.** `Calinski-Harabasz index`

<br>

<br>

## **2. Variational Inference**

⑴ Overview

> ① The probability that B was the cause when A was observed, denoted as P(B | A), is called the posterior probability (cf. Bayes' theorem).

> ② Variational Inference: A method to infer a complex posterior distribution by introducing a simpler variational distribution and conducting inference in a way that reduces the difference between the two.

⑵ ELBO (Evidence Lower Bound): The standard method for variational inference

> ① Jensen's inequality: For functions like the logarithmic function, which are concave, the following holds:

<br>

<img width="173" alt="스크린샷 2024-10-09 오전 10 37 33" src="https://github.com/user-attachments/assets/bcac6af1-fe2c-415b-b9be-274b6b760867">

<br>

> ② Problem situation: It is difficult to directly calculate the log evidence of a model, log p(x), for the given data x and latent variables z.

<br>

<img width="238" alt="스크린샷 2024-10-09 오전 10 37 59" src="https://github.com/user-attachments/assets/4cffefc3-3dbd-4dd2-b043-12baa731711b">

<br>

> ③ [Mathematical formulation](https://mpatacchiola.github.io/blog/2021/01/25/intro-variational-inference.html):

<br>

<img width="550" alt="스크린샷 2024-10-09 오전 10 38 44" src="https://github.com/user-attachments/assets/f19a0dc8-7ed9-44c2-8483-2bb896181d3d">

<br>

>> ○ ELBO = 𝔼<sub>q(z)</sub> [log p(x, z)] - 𝔼<sub>q(z)</sub> [log q(z)]

>> ○ In variational inference, maximizing the ELBO makes the inequality approach equality.

>> ○ When the equality is established p(x,z)/q(z) becomes constant with respect to z.

>> ○ p(x, z) / q(z) = constant with respect to z ⇔ q(z) ∝ p(x, z) ⇔ q(z) = q(z | x) = p(x, z) / p(x) = p(z | x) 

>> ○ **Conclusion:** Maximizing the ELBO makes the variational distribution q(z∣x) close to the true posterior p(z∣x).

> ④ **Example 1**

<br>

<img width="503" alt="스크린샷 2024-10-09 오전 10 39 49" src="https://github.com/user-attachments/assets/9b3762d5-cb07-45b7-8e92-7b085f2bf10d">

<br>

> ⑤ **Example 2.** [Loss functions of Variational Autoencoder (VAE) and Graph Variational Autoencoder (GVAE)](https://jb243.github.io/pages/2432)

<br>

<br>

## **3. Modern Information Theory** 

⑴ History

> ① Ludwig Boltzmann (1872): Proposed the H-theorem to explain the entropy of gas particles.

> ② Harry Nyquist (1924): Quantified the speed at which "intelligence" is propagated through a communication system.

> ③ John von Neumann (1927): Extended Gibbs free energy to quantum mechanics.

> ④ Ralph Hartley (1928): Expressed the number of distinguishable information by using logarithms.

> ⑤ Alan Turing (1940): Introduced deciban for decrypting the German Enigma cipher.

> ⑥ Richard W. Hamming (1947): Developed Hamming code.

> ⑦ Claude E. Shannon (1948): Pioneered information theory. [A Mathematical Theory of Communication](https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf)

> ⑧ Solomon Kullback & Richard Leibler (1951): Introduced Kullback-Leibler divergence

> ⑨ David A. Huffman (1951): Developed Huffman encoding.

> ⑩ Alexis Hocquenghem & Raj Chandra Bose & Dwijendra Kumar Ray-Chaudhuri (1960): Developed BCH code.

> ⑪ Irving S. Reed & Gustave Solomon (1960): Developed Reed-Solomon code.

> ⑫ Robert G. Gallager (1962): Introduced low-density parity check.

> ⑬ Kolmogorov (1963): Introduced Kolmogorov complexity (minimum description length).

> ⑭ Abraham Lempel & Jacob Ziv (1977): Developed Lempel-Ziv compression (LZ77).

> ⑮ Claude Berrou & Alain Glavieux & Punya Thitimajshima (1993): Developed Turbo code.

> ⑯ Erdal Arıkan (2008): Introduced polar code.

⑵ **Type 1.** Encryption

> ① Huffman code

> ② Shannon code

> ③ Shannon-Fano-Elias code (alphabetic code)

⑶ **Type 2.** Compression

> ① Uniquely decodable (UD)

> ② Kraft inequality

⑷ **Type 3.** Network information theory

⑸ **Type 4.** Quantum information theory

<br>

---

_Input: 2021-11-10 22:28_

_Modified: 2023-03-21 16:22_
