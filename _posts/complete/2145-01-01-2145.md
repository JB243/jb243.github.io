## **Chapter 20. Information Theory**

Higher category **:** 【Statistics】 [Statistics Table of Contents](https://jb243.github.io/pages/1641)

---

**1.** [Information Theory](#1-information-theory)

**2.** [Modern Information Theory](#2-modern-information-theory)

---

<br>

## **1. Information Theory** 

⑴ Overview

> ① low probability event: high information (surprising)

> ② high probability event: low information (unsurprising)

⑵ **entropy**

> ① Problem situation: X is Raining / Not Raining. Y is Cloudy / Not Cloudy

<br>

![image](https://github.com/user-attachments/assets/f4447d82-f083-4fba-bfaf-878ce3eddad5)

**Figure. 1.** Entropy example

<br>

> ② Basic concepts of statistics

<br>

![image](https://github.com/user-attachments/assets/90a112db-da94-4402-b82e-1fec9f6f8250)

<br>

> ③ **information**: everything obtained when the result is known

> ④ **uncertainty** (surprisal, unexpectedness, randomness)

<br>

![image](https://github.com/user-attachments/assets/eecbe1e8-4df2-4734-b607-b052a68d2970)

<br>

>> ○ When the base is 2, the unit used is **bits**

>> ○ When the base is e (natural logarithm), the unit used is **nats**

> ⑤ **entropy** (self information, Shannon entropy)

<br>

![image](https://github.com/user-attachments/assets/1cc47b13-3030-4c88-a752-67883f0a78e0)

<br>

>> ○ Understanding

>>> ○ In other words, entropy is defined as the average value of uncertainty

>>> ○ Similar to entropy in physics, higher values are obtained as disorder increases: higher entropy means higher heterogeneity

>>> ○ It applies the surprisal (-log p(x)) of each event to the probability weight (p(x)) of each event

>> ○ Examples

>>> ○ Fair coin toss **:** H = -0.5 log<sub>2</sub> 0.5 - 0.5 log<sub>2</sub> 0.5 = 1

>>> ○ Fair die ****: H = 6 × (-(1/6) log2 (1/6)) = log<sub>2</sub> 6 = 2.58

>>> ○ If X follows a uniform distribution, H(X) = log n holds

>> ○ **Source Coding Theorem**

>>> ○ Definition: For a very long data sequence, if the compression rate is maximized, the average number of bits of the compressed data approaches the entropy.

>>> ○ In other words, it refers to the number of bits required to represent a given set of possibilities.

>>> ○ When stored in base log2, it represents the number of bits; when stored in base logk, it indicates the required storage capacity in a k-ary system.

>>> ○ Kraft's Inequality: A constraint that ensures every entity can be represented by a unique bit sequence.

<br>

<img width="110" alt="스크린샷 2024-10-01 오후 1 26 20" src="https://github.com/user-attachments/assets/2c4e1074-4ad3-4761-bc84-d71a8a2e7e60">

<br>

>>> ○ Proof of the Source Coding Theorem: For the average number of bits p<sub>1</sub>L<sub>1</sub> + ··· + p<sub>n</sub>L<sub>n</sub>, 

<br>

<img width="408" alt="스크린샷 2024-10-01 오후 1 26 39" src="https://github.com/user-attachments/assets/9eead6f7-6a45-4122-97cd-78e3419c5bb2">

<br>

>> ○ **Feature 1.** H(X) is shift-invariant

>>> ○ When Y = X + a, H(Y) = H(X) is established because p<sub>Y</sub>(y) = p<sub>X</sub>(x)

>> ○ **Feature 2.** H(X) ≥ 0

>> ○ **Feature 3.** H<sub>b</sub>(X) = (log<sub>b</sub>a) H<sub>a</sub>(X) (where a and b are under the log)

>>> ○ log<sub>b</sub>p = (log<sub>b</sub>a) log<sub>a</sub>p

>> ○ [Python code](https://jb243.github.io/pages/1892#6-data-science)

<br>

```python
import numpy as np

def shannon_index(data):
    # Count unique values and their occurrences in the data
    values, counts = np.unique(data, return_counts=True)
    # Calculate proportions using the counts
    proportions = counts / sum(counts)
    # Calculate Shannon index, considering only non-zero proportions
    return -np.sum(proportions * np.log(proportions))

# Example data, where '1' appears 3 times, '2' appears 2 times, and '3' appears 1 time
data = [1, 1, 1, 2, 2, 3]

# Calculate the Shannon index
index = shannon_index(data)
print(f"Shannon index: {index}")
```

<br>

> ⑥ **joint entropy**

<br>

![image](https://github.com/user-attachments/assets/a1629e13-50e6-41dc-ab4c-d3e019f43956)

<br>

>> ○ **Feature 1.** H(X, Y) ≤ H(X) + H(Y): If X and Y are independent, H(X, Y) = H(X) + H(Y)

<br>

![image](https://github.com/user-attachments/assets/99f05cd3-ae07-47be-9fea-6b47cf76bab6)

<br>

> ⑦ Conditional entropy 

<br>

<img width="399" alt="스크린샷 2024-10-01 오후 1 27 05" src="https://github.com/user-attachments/assets/79573d57-e38c-4814-a98a-c6b406887948">

**Figure 2.** Conditional entropy visualization

<br>

>> ○ Conditional entropy for a specific condition

<br>

![image](https://github.com/user-attachments/assets/02940ee2-e413-4764-86c1-c5908990fad6)

<br>

>> ○ Total conditional entropy

<br>

![image](https://github.com/user-attachments/assets/7515233b-4df3-4c8e-bb12-633fb81515ca)

<br>

>> ○ Information gain (IG): The increase in order when knowing the information X

<br>

![image](https://github.com/user-attachments/assets/3cafead2-4743-49b1-b16c-a31f94078af7)

<br>

>> ○ Information bottleneck method (IB) **:** Describing the trade-off between complexity and accuracy using information theory

>>> ○ Application **:** For example, it can determine the number of clusters in given data.

>> ○ **Feature 1.** <span>If X and Y are independent, H(Y | X) = H(Y) (cf. H(Y | X) ≤ H(Y))</span>

>> ○ **Feature 2.** <span>H(Y | Y) = 0</span>

>> ○ **Feature 3.** <span>H(X, Y) = H(X) + H(Y | X) = H(Y) + H(X | Y) (Chain rule)</span>

<br>

![image](https://github.com/user-attachments/assets/3f18240f-075c-4472-a5d7-654367f1ec05)

<br>

>> ○ **Feature 4.** <span>H(X, Y | Z) = H(X | Z) + H(Y | X, Z)</span>

<br>

![image](https://github.com/user-attachments/assets/db9b3304-b22b-45c1-ba1a-fc5144bc54c4)

<br>

> ⑧ Mutual information

<br>

![image](https://github.com/user-attachments/assets/48c3b578-404b-4a60-8716-075f76d167a4)

**Figure 3.** Mutual information visualization

<br>

![image](https://github.com/user-attachments/assets/cf5601dc-4005-46e4-90eb-aac7d2ea3213)

<br>

>> ○ Information gain and mutual information are equivalent.

<br>

![image](https://github.com/user-attachments/assets/623c7074-1df2-4754-aea9-462b3bd902cd)

<br>

>> ○ **Feature 1.** <span>I(X; Y) = H(Y) - H(Y | X) = H(X) - H(X | Y)</span>

>> ○ **Feature 2.** Mutual information ≥ 0 (equality condition: X and Y are independent)

<br>

![image](https://github.com/user-attachments/assets/59461716-2a41-4e0e-a12d-c71b84cd7176)

<br>

>> ○ **Feature 3.** <span>H(X | Y) ≤ H(X)</span> (equality condition: X and Y are independent)

<br>

![image](https://github.com/user-attachments/assets/35f238be-2a97-43df-91d4-03ba6f4da15e)

<br>

>> ○ **Feature 4.** I(X; Y) ≤ H(X, Y): H(X, Y) - I(X; Y) satisfies the definition of [mathematical distance](https://jb243.github.io/pages/1897).

>> ○ **Application 1.** Mutual information can be used for assessing image [similarity](https://jb243.github.io/pages/879).

>> ○ **Application 2.** mRMR (maximum relevance minimum redundancy): A technique used to select features that have high mutual information with the output while having low mutual information among themselves. It is commonly used in machine learning.

> ⑨ Relative entropy (Kullback Leibler divergence, KL divergence, KLD) 

<br>

![image](https://github.com/user-attachments/assets/8b185bbb-b87b-4aed-9c94-13473e673ecb)

<br>

>> ○ Overview

>>> ○ A measure of how two probability distributions diverge.

>>> ○ It indicates the divergence between an approximate distribution p(x), which aims to mimic the true distribution, and the true distribution q(x).

>>> ○ Typically, the true distribution is a complex probability distribution function, while the approximate distribution is represented using a parameterized model.

>>> ○ The approximate distribution can be made manageable, often resembling a simple distribution like the normal distribution.

>>> ○ Strictly speaking, it does not satisfy the definition of [mathematical distance](https://jb243.github.io/pages/1897).

>> ○ **Feature 1.** asymmetry **:** <span>D<sub>KL</sub>(p(x) || q(x)) ≠ D<sub>KL</sub>(q(x) || p(x)) (∴ violates the definition of mathematical distance)</span>

>> ○ **Feature 2.** <span>D(p || q) ≥ 0 (equality condition: p(x) = q(x))</span>

<br>

![image](https://github.com/user-attachments/assets/83f900e8-5a68-4c98-addf-9f05977d65b5)

<br>

>> ○ **Feature 3.** H(X) ≤ log n (where n is the number of elements X can take)

<br>

![image](https://github.com/user-attachments/assets/1a2088c2-6894-4ea2-8401-4f826c073399)

<br>

>> ○ **Application.** Determining the predictive data distribution p with the smallest KL distance to the true unknown data distribution q.

>>> ○ Maximizing log-likelihood minimizes KL distance.

<br>

![image](https://github.com/user-attachments/assets/ad9dee99-bddb-4510-b47e-49df98ab4fa1)

<br>

> ⑩ **cross entropy**

>> ○ Definition: Average number of bits required to distinguish two probability distributions, p and q.

>> ○ In machine learning, model prediction.

<br>

<img width="368" alt="스크린샷 2024-10-01 오후 1 29 36" src="https://github.com/user-attachments/assets/53aeb867-600e-4992-a32f-81d2ff344d9a">

<br>

>> ○ **Feature 1.** <span>-∑ p<sub>i</sub> log q<sub>i</sub> = H(p) + D<sub>KL</sub>(p || q)</span>

>> ○ **Feature 2.** <span>Minimizing D<sub>KL</sub>(p || q) is the same as minimizing cross-entropy because H(p) is constant.</span>

>> ○ **Feature 3.** When minimizing -∑ p<sub>i</sub> log qi under the assumption that p<sub>i</sub> follows a uniform distribution, this is called [maximum likelihood estimation](https://jb243.github.io/pages/1630).

> ⑪ Asymptotic equipartition property (AEP)

<br>

![image](https://github.com/user-attachments/assets/2b493dbc-eb31-4576-bb58-a747e1ae8770)

<br>

⑶ **Comparison 1.** `Rényi Entropy`

> ① Hartley (max) entropy: For a source represented by M symbols (e.g., if expressed with the alphabet, M = 26)

<br>

<img width="99" alt="스크린샷 2024-10-01 오후 1 42 57" src="https://github.com/user-attachments/assets/dfc6dad8-e734-4a56-8ea6-12ac59454d20">

<br>

> ② Min entropy: Related to signal processing

<br>
 
<img width="164" alt="스크린샷 2024-10-01 오후 1 43 23" src="https://github.com/user-attachments/assets/3a4c9f50-0410-42b8-847e-e0e4ca37ad74">

<br>

> ③ Rényi entropy: Encompasses Shannon entropy (α → 1; by [L'Hôpital's rule](https://jb243.github.io/pages/1810)), max entropy (α → 0), and min entropy (α → ∞)

<br>

<img width="209" alt="스크린샷 2024-10-01 오후 1 43 42" src="https://github.com/user-attachments/assets/50945683-db17-4a63-be38-7ba92b3f8181">

<br>

> ④ Rényi divergence: As α → ∞, Rényi divergence converges to KL divergence (∵ [L'Hôpital's rule](https://jb243.github.io/pages/1810))

<br>

<img width="504" alt="스크린샷 2024-10-01 오후 1 44 09" src="https://github.com/user-attachments/assets/9d1695e0-3858-4a2f-b4c7-7d890a7a9ae3">

<br>

⑷ **Comparison 2.** Sample Entropy

> ① A dynamic measure that reflects time

> ② Important in signal processing

⑸ **Comparison 3.** `Gini impurity`

<br>

![image](https://github.com/user-attachments/assets/88b5c89d-77b8-49c2-8e50-a9db6bb72306)

<br>

> ① Entropy is not the unique measure, alternative measures like Gini impurity are proposed.

> ② Gini impurity considers the probability of mislabeling (1 - P(x<sub>i</sub>)) weighted by the weight of each sample (P(x<sub>i</sub>)).

> ③ Both Gini impurity and entropy increase as the given data becomes more disordered: Origin of impurity.

> ④ (Conceptual Distinction) The Gini Coefficient in Economics

>> ○ The concept was first introduced to evaluate income inequality: It can assess not only income but also the degree of inequality in the distribution of a discrete variable.

>> ○ As the Gini coefficient moved from economics to the fields of machine learning and information theory, it implies similar inequality, but the mathematical definition changes.

>> ○ Difference: If the distribution of the variable is entirely uniform,

>>> ○ In economics, the Gini coefficient becomes extremely egalitarian, so it has a value of 0.

>>> ○ In information theory, the Gini coefficient attains its maximum value due to the Cauchy-Schwarz inequality. Intuitively, it becomes most disordered, thus reaching the maximum value.

⑹ **Comparison 4.** `Connectivity index (CI)`

> ① In a spatial lattice, if a specific spot and its nearest neighbor are in different clusters, the CI value increases.

> ② Therefore, a higher CI value implies higher heterogeneity.

⑺ **Comparison 5.** `Simpson index`

> ① Another measure based on information theory, along with Shannon entropy

> ② Based on spatial transcriptomics, the probability of two random spots belonging to the same cluster ([ref](https://genomemedicine.biomedcentral.com/articles/10.1186/s13073-023-01185-4#Sec2))

> ③ The lower the Simpson index, the higher the heterogeneity implied

⑻ **Comparison 6.** `Cluster modularity`

> ① Based on spatial transcriptomics, a metric indicating the purity (homogeneity) of spots within a cluster ([ref](https://genomemedicine.biomedcentral.com/articles/10.1186/s13073-023-01185-4#Sec2))

⑼ **Comparison 7.** `Lempel-Ziv complexity`

⑽ **Comparison 8.** `Silhouette coefficient`

⑾ **Comparison 9.** `Calinski-Harabasz index`

⑿ **Application 1.** `Markov process`

> ① Definition: A system where the future state depends only on the current state and not on past states.

<br>

![image](https://github.com/user-attachments/assets/6c8244d1-738d-404e-8fc2-2b8a634ffa8b)

<br>

> ② two-state Markov chain

<br>

<img width="387" alt="스크린샷 2024-10-01 오후 1 30 27" src="https://github.com/user-attachments/assets/8f42910d-4bb6-4150-a4f0-78dbed3eef7d">

![image](https://github.com/user-attachments/assets/c6fb483d-6d4b-4b88-8cab-8dbaef469bc7)

<br>

**Figure 4.** two-scale Markov chain

<br>

> ③ Hidden Markov model

>> ○ If χ = {X<sub>i</sub>} is a Markov process and Y<sub>i</sub> = ϕ(X<sub>i</sub>) (where ϕ is a deterministic function), then y = {Y<sub>i</sub>} is a hidden Markov model.

> ④ **Feature 1.** If <span>Pr(x<sub>n</sub> | x<sub>n-1</sub>)</span> is independent of n, the Markov process is stationary (time-invariant).

> ⑤ **Feature 2.** Aperiodic ⊂ irreducible.

> ⑥ **Feature 3.** Markov process can prove the second law of thermodynamics (law of increasing entropy).

>> ○ Assumes uniform stationary distribution.

<br>

<br>

## **2. Modern Information Theory** 

⑴ History

> ① Ludwig Boltzmann (1872) **:** Proposed the H-theorem to explain the entropy of gas particles.

> ② Harry Nyquist (1924) **:** Quantified the speed at which "intelligence" is propagated through a communication system.

> ③ John von Neumann (1927): Extended Gibbs free energy to quantum mechanics.

> ④ Ralph Hartley (1928): Expressed the number of distinguishable information by using logarithms.

> ⑤ Alan Turing (1940): Introduced deciban for decrypting the German Enigma cipher.

> ⑥ Richard W. Hamming (1947): Developed Hamming code.

> ⑦ Claude E. Shannon (1948): Pioneered information theory. [A Mathematical Theory of Communication](https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf)

> ⑧ Solomon Kullback & Richard Leibler (1951): Introduced Kullback-Leibler divergence

> ⑨ David A. Huffman (1951): Developed Huffman encoding.

> ⑩ Alexis Hocquenghem & Raj Chandra Bose & Dwijendra Kumar Ray-Chaudhuri (1960): Developed BCH code.

> ⑪ Irving S. Reed & Gustave Solomon (1960): Developed Reed-Solomon code.

> ⑫ Robert G. Gallager (1962): Introduced low-density parity check.

> ⑬ Kolmogorov (1963): Introduced Kolmogorov complexity (minimum description length).

> ⑭ Abraham Lempel & Jacob Ziv (1977): Developed Lempel-Ziv compression (LZ77).

> ⑮ Claude Berrou & Alain Glavieux & Punya Thitimajshima (1993): Developed Turbo code.

> ⑯ Erdal Arıkan (2008): Introduced polar code.

⑵ **Type 1.** Encryption

> ① Huffman code

> ② Shannon code

> ③ Shannon-Fano-Elias code (alphabetic code)

⑶ **Type 2.** Compression

> ① Uniquely decodable (UD)

> ② Kraft inequality

⑷ **Type 3.** Network information theory

⑸ **Type 4.** Quantum information theory

<br>

---

_Input: 2021-11-10 22:28_

_Modified: 2023-03-21 16:22_
