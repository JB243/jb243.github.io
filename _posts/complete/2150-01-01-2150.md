## **Chapter 8. Clustering Algorithms** 

Recommended Article: 【Algorithm】 [Algorithm Index](https://jb243.github.io/pages/1278)

---

**1.** [Introduction](#1-introduction)

**2.** **Type 1.** [unsupervised hierarchical clustering](#2-type-1-unsupervised-hierarchical-clustering)

**3.** **Type 2.** [K-means clustering](#3-type-2-k-means-clustering)

**4.** **Type 3.** [Density-based clustering](#4-type-3-density-based-clustering-dbscan-density-based-spatial-clustering-of-applications-with-noise)

**5.** **Type 4.** [Mixture distribution clustering](#5-type-4-mixture-distribution-clustering)

**6.** **Type 5.** [Graph-based clustering](#6-type-5-graph-based-clustering-som-self-organizing-maps)

**7.** [Other Clustering Algorithms](#7-other-clustering-algorithms)

---

<br>

## **1\. Introduction**

 ⑴ Clustering is an optimization problem

> ① **variability**

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/b4cadd85-9e67-4587-b59f-b63a6939d14e)

<br>

>> ○ variability can be defined as the sum of distance or distance squared

>> ○ [Types of distance functions](https://jb243.github.io/pages/879#footnote_link_67_51)

>> ○ Reason: big and bad is worse than small and bad

> ② **dissimilarity**

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/0fd55311-e1e8-4970-88dc-309d91840a03)

<br>

>> ○ Clustering is not about finding a global minimum for dissimilarity

>> ○ **Reason 1.** A trivial solution exists where ∀variability = 0 with as many clusters as samples

>> ○ **Reason 2.** It's unfair to penalize large clusters per se: big is not necessarily worse than small

>> ○ Constraints are needed: minimum distance between clusters, number of clusters, etc.

⑵ Clustering is an unsupervised algorithm

> ① Due to this, evaluating the performance of clustering algorithms is difficult, but it can be assessed as follows

> ② Clustering Quality Metrics

>> ○ **Method 1:** Average Silhouette Width (ASW)

>>> ○ Formula 

<br> 

<img width="228" alt="스크린샷 2025-03-12 오후 3 37 43" src="https://github.com/user-attachments/assets/af671e80-f22f-49ea-a9cd-1784b6d8d838" />

<br>

>>>> ○ a(i): The average distance from i to all data points in the same cluster c<sub>i</sub>

>>>> ○ b(i): The lowest average distance from i to all data points in the other cluster c among all clusters c.

>>>> ○ The silhouette coefficient S(c) of cluster c is the average of s(i) for the data points within that cluster.

<br>

<img width="139" alt="스크린샷 2025-04-02 오전 9 08 49" src="https://github.com/user-attachments/assets/d231d1d2-f5ed-47e1-a780-fe86f54c6289" />

<br>

>>> ○ Ranges between -1 and 1.  

>>> ○ A higher value indicates better clustering quality, meaning higher cohesion within clusters and better separation between clusters.  

>>> ○ Disadvantages:

>>>> ○ The score is inversely proportional to the number of clusters.  

>>>> ○ It is inaccurate for non-spherical clusters.  

>>> ○ Code: [`sklearn.metrics.silhouette_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html)

>> ○ **Method 2:** Davies-Bouldin Index (DBI)

<br>

<img width="300" height="78" alt="스크린샷 2025-09-08 오전 9 52 47" src="https://github.com/user-attachments/assets/b6a9f94d-0f71-48ba-8714-b8a0b2402f83" />

<br>

>>> ○ Calculation Process:  

>>>> ○ Measure the similarity between each cluster and the most similar other cluster.  

>>>> ○ Define similarity as the ratio of inter-cluster distance to intra-cluster distance.  

>>>> ○ Average these similarities across all clusters to derive the DBI.  

>>> ○ The value is 0 or higher; a lower index indicates better cluster separation.  

>>> ○ Advantages:  

>>>> ○ Less affected by the number of clusters.  

>>> ○ Disadvantages:  

>>>> ○ May become inaccurate if clusters are poorly separated or if distances vary significantly.  

>>> ○ Code: [`sklearn.metrics.davies_bouldin_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.davies_bouldin_score.html)

>> ○ **Method 3:** Calinski-Harabasz Index (CHI)

>>> ○ Formula: Relates the number of clusters k and the number of data points n.  

<br>

<img width="502" alt="스크린샷 2025-01-21 오전 11 05 50" src="https://github.com/user-attachments/assets/6b3f1eca-992f-4706-8f71-17114f23057f" />

<br>

>>> ○ The value is 0 or higher; a higher value indicates better clustering quality.  

>>> ○ Disadvantages:  
  
>>>> ○ The index decreases as cluster sizes increase.  

>>>> ○ Assumes variances are relatively consistent.  

>>> ○ Code: [`sklearn.metrics.calinski_harabasz_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski_harabasz_score.html)

>> ○ **Method 4:** Landscape Shape Index (LSI)

>> ○ **Method 5:** Percentage of Abnormal Spots (PAS)

>>> ○ Higher value indicates lower quality: the proportion of spots considered abnormal (i.e., not consistent with their neighboring spots) among all spots.

>>> ○ Used in spatial transcriptomics analysis.
 
> ② Clustering Comparison Metrics

>> ○ **Method 1:** Normalized Mutual Information (NMI)

>>> ○ Measures [mutual information](https://jb243.github.io/pages/2145) between two clusters and normalizes it to remain invariant to the cluster size or number of classes.  

>>> ○ Ranges between 0 and 1.  

>>> ○ Code: [`sklearn.metrics.normalized_mutual_info_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.normalized_mutual_info_score.html)

>> ○ **Method 2:** Adjusted Rand Index (ARI)

>>> ○ Evaluates the similarity between two clusters.  

>>> ○ Ranges between -1 and 1 (perfect match). A score of 0 indicates random clustering.  

>>> ○ Rand Index (RI)

<br>

<img width="235" alt="스크린샷 2025-01-21 오전 11 03 52" src="https://github.com/user-attachments/assets/da7033ee-f809-49be-b81c-d8498eb431b6" />

<br>

>>> ○ Adjusted for the expected value of random cluster results.  

<br>

<img width="229" alt="스크린샷 2025-01-21 오전 11 04 28" src="https://github.com/user-attachments/assets/7cd882a6-1141-4e94-a528-14437f3cfa97" />

<br>

>>> ○ Similarly, regarding n<sub>ij</sub>, a<sub>i</sub>, and b<sub>j</sub> from the contingency table, 

<br>

<img width="474" height="98" alt="스크린샷 2025-09-08 오전 9 50 35" src="https://github.com/user-attachments/assets/30955ad3-e221-477c-b879-b6ebcdd3bf49" />

<br>

>>> ○ Code: [`sklearn.metrics.adjusted_rand_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html)

>> ○ **Method 3:** Adjusted Mutual Information (AMI)

>>> ○ Adjusts NMI for the expected value of random clustering.

<br>

<img width="400" alt="스크린샷 2025-01-21 오전 11 02 51" src="https://github.com/user-attachments/assets/c553fffe-514e-4471-95c6-8ef415b09949" />

<br>

>>> ○ Ranges between -1 and 1.  

>>> ○ Effective in removing random clustering bias when there are many classes or imbalanced data.  

>>> ○ Code: [`sklearn.metrics.adjusted_mutual_info_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html)

>> ○ **Method 4:** Fowlkes-Mallows Index (FMI)

>>> ○ The geometric mean of precision and recall.  

>>> ○ Ranges between 0 and 1.  

>>> ○ Performance decreases with imbalanced data.  

>>> ○ Code: [`sklearn.metrics.fowlkes_mallows_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fowlkes_mallows_score.html)

>> ○ **Method 5:** Homogeneity

>>> ○ Measures the extent to which samples with the same label are grouped into a single cluster.  

>>> ○ Code: [`sklearn.metrics.homogeneity_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_score.html)

>> ○ **Method 6:** Completeness

>>> ○ Evaluates how well samples within the same cluster belong to the same label.  

>>> ○ Code: [`sklearn.metrics.completeness_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.completeness_score.html)

>> ○ **Method 7:** V-measure

>>> ○ Defined as: v = (1 + beta) × homogeneity × completeness / (beta × homogeneity + completeness)

>>> ○ Ranges between 0 and 1.  

>>> ○ Code: [`sklearn.metrics.v_measure_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.v_measure_score.html)

⑶ Types of Clustering

> ① **Type 1.** Hierarchical algorithm: Successively splits or merges groups based on some distance measurement. 

>> ○ A deterministic approach. 

>> ○ **1-1.** Agglomerative: bottom-up approach. An approach that builds larger clusters.

>> ○ **1-2.** divisive: top-down approach. An approach that divides into smaller clusters. Most common implementation. 

> ② **Type 2.** Partitioning algorithm: Looks for a specific delineation that optimizes some global measure

>> ○ **2-1.** Hard

>> ○ **2-2.** Soft 

<br>

<br>

## **2\. Type 1.** Unsupervised hierarchical clustering

 ⑴ Overview

> ① Often used to draw a [heatmap](https://www.r-graph-gallery.com/heatmap) from an internal matrix (Example: RStudio's heatmap function)

> ② A type of graph-based method

> ③ Unlike non-hierarchical clustering, hierarchical clustering does not pre-determine the number of clusters

 ⑵ **Type 1.** **Divisive Analysis**

> ① A technique that starts from the entire group and separates objects with decreasing similarity

> ② **Stage 1:** Initially assign N elements to N clusters

> ③ **Stage 2:** Merge the closest clusters into one

> ④ **Stage 3:** Repeat stage 2 until constraints such as the number of clusters are met

>> ○ In this case, a dendrogram is used

 ⑶ **Type 2.** **Agglomerative Hierarchical Clustering** 

> ① Definition: A method that considers each entity as a subgroup and progressively merges similar subgroups to form new subgroups

> ② **Example 1.** UPGMA(unweighted pair group method with arithmetic mean)

<br>

![image](https://github.com/user-attachments/assets/232035c9-72cd-4ab2-9364-19c402eddf91)

<br>

 ⑷ Linkage metric: Defining distance between clusters

> ① **Type 1.** Single-linkage: The shortest distance between elements of one cluster and another

> ② **Type 2.** Complete-linkage: The farthest distance between elements of one cluster and another

> ③ **Type 3.** Average-linkage: The distance between the centers of two clusters

>> ○ Can also mean the average distance between elements of one cluster and another

> ④ **Type 4.** Centroid-linkage: Measuring the distance between the centers of two clusters

> ⑤ **Type 5.** Ward-linkage: A method of measuring distance between clusters based on within-cluster sum of squares

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/3724d8fb-165a-478d-aecd-b927ad889d2e)

**Figure 1.** Performance of clustering according to linkage metric

<br>

 ⑸ Features

> ① One of the most frequently used clustering algorithms along with K-means clustering

> ② **Advantage 1.** Deterministic: can reach the same conclusion in any environment

> ③ **Advantage 2.** Can try various linkage criteria

> ④ **Disadvantage 1.** Slow naïve algorithm. Generally requires Ο(n3) time complexity

>> ○ Reason: Ο(n<sup>2</sup>) is required to perform stage 2 and it has to be repeated Ο(n) times

>> ○ In some cases, Ο(n<sup>2</sup>) time complexity is required depending on the linkage criteria

 ⑹ [References](https://medium.com/machine-learning-researcher/clustering-k-mean-and-hierarchical-cluster-fa2de08b4a4b)

<br>

<br>

## **3\. Type 2.** K-means clustering

 ⑴ Overview

> ① Often used in clustering algorithms targeting images

 ⑵ Algorithm

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/d3918cd3-65fe-4587-9169-4559368fe1d8)

**Figure 2.** How K-means clustering works

<br>

> ① **Stage 1:** Randomly select k initial centroids

> ② **Stage 2:** Assign clusters based on which centroid each element is closest to

> ③ **Stage 3:** Calculate the center of each cluster to define new centroids

> ④ **Stage 4:** Clustering ends when centroids no longer change

 ⑶ Features

> ① The most widely used clustering algorithm

> ② K-means clustering is one of the greedy algorithms for clustering

> ③ **Determining K**

>> ○ **Background knowledge**

>>> ○ Example: Differentiating between cancerous and non-cancerous data in bio multi-omics data

>> ○ **Elbow Method**

>>> ○ A method to determine the number of clusters in cluster analysis by plotting the total within-cluster sum of squares against the number of clusters and selecting the elbow point as the appropriate number of clusters

>>> ○ Choose the cluster corresponding to the elbow part where the slope is gentle when the number of clusters is on the x-axis and SSE value on the y-axis

>> ○ **Silhouette Method**

>>> ○ A method that quantitatively calculates the quality of clustering using silhouette coefficients

>>> ○ Shows how well clusters are separated from each other

>>> ○ A silhouette coefficient close to 1 indicates well-optimized, well-separated clusters

>>> ○ A silhouette coefficient close to 0 indicates poorly optimized, closely spaced clusters

>> ○ **Dendrogram**

>>> ○ Use dendrogram visualization in hierarchical cluster analysis to determine the number of clusters

> ④ Advantage

>> ○ **Advantage 1.** Fast. Ο(Kn) time is required to perform stage 2 (K is the number of clusters, n is the number of data points)

>> ○ **Advantage 2.** Mathematically converges after a finite number of iterations as the objective function decreases with each iteration

> ⑤ Disadvantage

>> ○ **Disadvantage 1.** K should be given: Choosing the wrong K can lead to incorrect results

>> ○ **Disadvantage 2.** Depends on initial centroids: Results can vary depending on initial centroid position, so it's not deterministic

>> ○ **Disadvantage 3.** The complex data cannot be clustered because it relies on the Euclidean distance for specific points.

>>> ○ Each feature that makes up the data needs to be bi-modal or multi-modal for it to work well. ([ref](https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html))

>>> ○ **Example 1:** When the data consists of two concentric circles, the positions of the two centroids are the same, so K-means clustering cannot be used.

>>> ○ **Example 2:** When the clusters are not spherical.

<br>

<img width="420" alt="스크린샷 2025-04-02 오전 9 39 19" src="https://github.com/user-attachments/assets/86440d7c-b782-4966-95e0-5087b3635246" />

**Figure 3.** When the clusters are not spherical 

<br>

>>> ○ **Example 3:** When the clusters have different densities.

<br>

<img width="420" alt="스크린샷 2025-04-02 오전 9 39 58" src="https://github.com/user-attachments/assets/4b381feb-6fef-43bd-889c-51ef17d7904d" />

**Figure 4.** When the clusters have different densities

<br>

> ⑥ Overcoming

>> ○ ISODATA: Estimates the value of K

>> ○ Kohonen Self-Organizing Map: Similar to evolutionary learning, gradually increases the value of K to reduce variability

>> ○ K-medoid clustering

>> ○ Lloyd's K-means clustering algorithm

>> ○ Bisecting K-Means: Provided by [scikit package](https://scikit-learn.org/stable/modules/clustering.html) 

>> ○ Fuzzy c-means clustering: Points belong to more than one cluster.

>> ○ Mean-shif: Provided by [scikit package](https://scikit-learn.org/stable/modules/clustering.html). Points are moved to the densest nearby area.

 ⑷ Python code

```python
from sklearn.cluster import KMeans

X = [[1, 2, 3],[1, 2, 3.1],[2, 2, 4],[2, 2, 4.1]]
kmeans = KMeans(n_clusters=2, random_state=0)
kmeans.fit(X)
cluster_labels = kmeans.labels_

print(cluster_labels)
# [1 1 0 0]
```

 ⑸ **unsupervised hierarchical clustering vs K-means clustering**

<br>

|   | **Unsupervised Hierarchical Clustering** | **K-means Clustering** |
| --- | --- | --- |
| Time Complexity | O(n<sup>3</sup>) | O(kn) × iteration |
| Randomness | Deterministic | Random |

 **Table. 1.** Comparison of unsupervised hierarchical clustering and K-means clustering

<br>

 ⑹ **K-NN vs K-means clustering**

<br>

| **Item** | **K-NN** | **K-means Clustering** |
| --- | --- | --- |
| Type | Supervised Learning | Unsupervised Learning |
| Meaning of k | Number of Nearest Neighbors | Number of Clusters |
| Optimization Techniques | Cross Validation, Confusion Matrix | Elbow Method, Silhouette Method |
| Application | Classification | Clustering |

 **Table 2.** K-NN vs K-means clustering

<br>

<br>

## **4. Type 3.** Density-Based Clustering (DBSCAN, density-based spatial clustering of applications with noise)

 ⑴ Overview: One of the non-hierarchical clustering analyses like K-means clustering

> ① An algorithm that groups closely distributed entities based on the calculation of their densities

> ② No need to pre-specify the number of clusters

> ③ Clusters are connected based on cluster density, allowing for clustering of geometric shapes

 ⑵ Components

> ① Core point

>> ○ A data point that has a minimum number(min_samples) of other data points within a certain radius(epsilon)

>> ○ The minimum number of data points required within the radius is set as a hyperparameter

> ② Neighbor point

>> ○ Other data existing within a certain radius around a specific data point

> ③ Border point

>> ○ Not a core point but exists within the radius of a core point

>> ○ Included in the cluster centered around the core point and typically forms the outskirts of the cluster

> ④ Noise point

>> ○ Neither a core point nor satisfies the border point condition

>> ○ Also considered as an outlier

 ⑶ Procedure

> ① **Stage 1.** Identify core points with a minimum number(min_samples) of points within a certain radius(epsilon).

> ② **Stage 2.** Explore connected components in the adjacency graph using only core points.

> ③ **Stage 3.** Assign non-core points as noise.

<br>

![image](https://blog.kakaocdn.net/dn/Gd7Gt/btsM4PLwQcy/RFfGswvkKVo8jIkSpPH0N0/img.gif)

**Figure 5.** Gibbs Sampling initialization

<br>

 ⑷ Features

 > ① **Advantage 1.** No need to predefine the number of clusters, unlike k-means clustering

> ② **Advantage 2.** Can identify clusters with geometric shapes due to clustering based on cluster density

> ③ **Disadvantage 1.** Difficult to determine hyperparameters, sensitive to parameter choices

> ④ **Disadvantage 2.** Challenges in computation when clusters have varying densities or in higher dimensions

⑸ Python example

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/07f9f44c-610e-4782-99b0-751eaf4bc10d)

<br>

**Figure 6.** DBSCAN Python example

<br>

```python
import pandas as pd
from sklearn.datasets import make_blobs
from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt

# Step 1: Generate sample data
X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)

# Step 2: Convert to DataFrame
df = pd.DataFrame(X, columns=['x', 'y'])

# DBSCAN parameters
epsilon = 0.5  
min_samples = 5 

# Step 3: Perform DBSCAN clustering
db = DBSCAN(eps=epsilon, min_samples=min_samples)
df['cluster_label'] = db.fit_predict(df[['x', 'y']])

# Step 4: Visualize the clustering results
plt.figure(figsize=(12, 8))
plt.scatter(df['x'], df['y'], c=df['cluster_label'], cmap='viridis', marker='o')
plt.title('DBSCAN Clustering')
plt.xlabel('X')
plt.ylabel('Y')
plt.show()
```

<br>

⑹ Application

> ① OPTICS

<br>

```python
(skip)

from sklearn.cluster import OPTICS

# OPTICS parameters
min_samples = 5
xi = 0.05
min_cluster_size = 0.05

# Perform OPTICS clustering
optics = OPTICS(min_samples=min_samples, xi=xi, min_cluster_size=min_cluster_size)
df['cluster_label'] = optics.fit_predict(df[['x', 'y']])

(skip)
```

<br>

> ② HDBSCAN

<br>

```python
(skip)

import hdbscan

# HDBSCAN parameters
min_cluster_size = 5
min_samples = 5

# Perform HDBSCAN clustering
hdb = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples)
df['cluster_label'] = db.fit_predict(df[['x', 'y']])

(skip)
```

<br>

> ③ DBSCAN++

<br>

<br>

## **5. Type 4. Mixture Distribution Clustering**

 ⑴ Mixture Distribution Clustering: One of the non-hierarchical clustering analyses, like K-means clustering

> ① Assumes data comes from a population model represented as a weighted sum of k parametric models, estimates parameters and weights from the data

> ② Each of the k models represents a cluster, and each data point is classified into a cluster based on which model it most likely came from

> ③ Definition of Mixture Model: A mixture model represented as a weighted sum of M distributions (components)

>> <span>p(x | θ) = Σ p(x | Ci, θ<sub>i</sub>) p(C<sub>i</sub>)</span>

>> ○ p(x | C<sub>i</sub>, θ<sub>i</sub>): The individual probability density function in the mixture model

>> ○ θ<sub>i</sub>: The parameter vector of the i-th distribution

>> ○ C<sub>i</sub>: The i-th cluster (class)

>> ○ p(C<sub>i</sub>): The importance or weight (αi) of the i-th cluster in the mixture model

> ④ Estimating parameters of the mixture model is complex compared to single models, often using the EM algorithm instead of differentiation

> ⑤ Accuracy of estimation may decrease or become challenging if the size of clusters is too small

> ⑥ Sensitive to outliers, hence preprocessing such as outlier removal is necessary

 ⑵ Gaussian Mixture Model (GMM)

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/f4da2578-d054-41ca-a694-feb36b7f46df)

 **Figure 7.** Gaussian Mixture Model

<br>

> ① Overview
 
>> ○ Assumes the overall data probability distribution is a linear combination of k Gaussian distributions.

>> ○ Clusters are formed based on the probability of belonging to each distribution.

>> ○ Each feature that makes up the data needs to be bi-modal or multi-modal for it to work well. ([ref](https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html))

> ② Formula

>> ○ In GMM, estimates the weights, means, and covariances of the appropriate k Gaussian distributions for the given data X = {x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>N</sub>}

>> ○ Simplification: When x is one-dimensional data

<br>

<img width="352" alt="스크린샷 2024-10-16 오후 11 51 24" src="https://github.com/user-attachments/assets/a975248a-20ec-4721-93c6-edb7ea6c3b94">

<br>

>> ○ Matrix Representation: When x is multi-dimensional data, for the mean **μ**<sub>k</sub> and covariance matrix ∑<sub>k</sub> of the k-th cluster

<br>

<img width="451" alt="스크린샷 2024-10-16 오후 11 51 53" src="https://github.com/user-attachments/assets/81bc09a4-6e8a-42fc-b253-813e83144c3f">

<br>

> ③ EM algorithm (MLE estimation)

>> ○ EM algorithm can be used to estimate the optimal Gaussian distribution each data point belongs to

>> ○ Objective function: Expressed as log likelihood

<br>

<img width="526" alt="스크린샷 2024-10-17 오전 12 23 21" src="https://github.com/user-attachments/assets/946abae7-6f04-455b-8f4b-453dbdf0f980">

<br>

>> ○ **Step 1:** Assign random values to γ<sub>i,k</sub>

>> ○ **Step 2:** E-step: Estimate γ<sub>i,k</sub>

<br>

<img width="451" alt="스크린샷 2024-10-17 오전 12 23 47" src="https://github.com/user-attachments/assets/bcbfc1f7-5e0d-405c-b8e8-6b2c918edcbb">

<br>

>> ○ **Step 3:** M-step: Calculate a<sub>k</sub>, **μ**<sub>k</sub>, ∑<sub>k</sub> from γ<sub>i,k</sub>

<br>

<img width="353" alt="스크린샷 2024-10-17 오전 12 24 10" src="https://github.com/user-attachments/assets/b9a308a2-fad3-4f69-b4a4-49bac160f3fb">

<br>

>> ○ **Step 4:** If γ<sub>i,k</sub> converges, stop; otherwise, move to **Step 2**: Convergence is guaranteed due to the characteristics of the Gaussian function.

> ④ Comparison between K means clustering and Gaussian mixture model

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/2f3faec2-2312-4aa8-85f9-33b4b6cb1765)

 **Figure 8.** Comparison of K means clustering and Gaussian mixture model

<br>

<br>

## **6. Type 5.** Graph-Based Clustering (SOM, Self-Organizing Maps)

 ⑴ Overview: One of the non-hierarchical clustering analyses, like K-means clustering

> ① SOM, developed by Kohonen, also known as Kohonen maps

> ② SOM is an artificial neural network modeled on the learning processes of the cerebral cortex and visual cortex

> ③ Clustering by autonomous learning methods

> ④ A type of unsupervised neural network that maps high-dimensional data into an easy-to-understand low-dimensional arrangement of neurons

> ⑤ The mapping preserves the positional relationships of input variables

> ⑥ In the actual space, if input variables are close, they are mapped close to each other on the map

> ⑦ Very fast due to using only one **forward pass**

⑵ **Type 1.** SOM

> ① **Component 1.** Input Layer

>> ○ Receives the input vector, contains neurons equal to the number of input variables

>> ○ The input layer data is aligned in the competitive layer through learning, referred to as the map

>> ○ Each neuron in the input layer is **fully connected** to every neuron in the competitive layer

> ② **Component 2.** Competitive Layer

>> ○ A layer arranged in a 2D grid, where input vectors cluster into a single point based on their characteristics

>> ○ SOM uses competitive learning, where each neuron calculates its closeness to the input vector, repeatedly adjusting connection strengths

>> ○ Through the learning process, the connection strength makes the neuron most similar to the input pattern the winner

>> ○ Due to the winner-take-all structure, only the winning neuron appears in the competitive layer, with similar input patterns arranged in the same competitive neuron

> ③ Learning Algorithm

>> ○ **Step 1.** Initialization: Initialize connection strengths for nodes in the SOM map

>> ○ **Step 2.** Input Vector: Present the input vector

>> ○ **Step 3.** Similarity Calculation: Calculate similarity between the input vector and prototype vectors using Euclidean distance

>> ○ **Step 4.** Prototype Vector Search: Find the prototype vector (BMU) closest to the input vector

>> ○ **Step 5.** Strength Readjustment: Readjust the connection strengths of the BMU and its neighbors

>> ○ **Step 6.** Repeat: Return to step 2 and repeat

⑶ **Type 2.** [Spectral Clustering](https://jb243.github.io/pages/616#1-basics)

> ① Problem definition

<br>

<img width="127" alt="스크린샷 2025-04-06 오후 11 40 18" src="https://github.com/user-attachments/assets/9a6f8e7f-94b4-419e-8fd4-1491fe7f1881" />

<br>

>> ○ Reason why the condition **y**<sup>T</sup>D**y**=1 is necessary: Without this condition, there exists a trivial solution **y**=**0**, and for any nonzero **y**, there exists a scalar c with 0 ≤ 𝑐 < 1 0≤c<1 such that c**y** makes the objective function arbitrarily small.

>> ○ Reason why the condition **y**<sup>T</sup>D**1**=0 is necessary: This condition is necessary and sufficient for **y**≠**1** (orthogonality). Without it, the minimum of the objective function is always 0 and always achieved at **y**=**1** (by the method of [Lagrange multipliers](https://jb243.github.io/pages/1813)).

<br>

<img width="335" alt="스크린샷 2025-04-06 오후 11 40 00" src="https://github.com/user-attachments/assets/18861836-66b5-416c-afcc-df2e1194b482" />

<br>

> ② **Step 1:** Calculate the distance between the given n data points to create an n × n adjacency matrix **A**.

>> ○ The Gaussian kernel is often used.

> ③ **Step 2:** Based on the adjacency matrix, calculate the [Laplacian matrix](https://jb243.github.io/pages/616) **L**: **L**<sub>n</sub> or **L**<sub>r</sub> can also be used.

>> ○ **L** = **D** - **A**, where **D**<sub>ii</sub> = ∑<sub>j</sub> **A**<sub>ij</sub>

>> ○ **L**<sub>n</sub> = **D**<sup>-1/2</sup>**LD**<sup>-1/2</sup> = **I** - **D**<sup>-1/2</sup>**AD**<sup>-1/2</sup>

>> ○ **L**<sub>t</sub> = **D**<sup>-1</sup>**A**

>> ○ **L**<sub>r</sub> = **D**<sup>-1</sup>**L** = **I** - **L**<sup>t</sup> = **D**<sup>-1/2</sup>**L**<sub>n</sub>**D**<sup>1/2</sup>

> ④ **Step 3:** Calculate the matrix of eigenvectors **W** = [**ω**<sub>2</sub>, ···, **ω**<sub>k</sub>] ∈ ℝ<sup>n×k</sup> corresponding to the k smallest eigenvalues of the Laplacian matrix.

>> ○ The closer the eigenvalue is to 0, the more connected the graph is.

>> ○ Exclude **ω**<sub>1</sub> = 1, which corresponds to λ<sub>1</sub> = 0, as it is trivial.

> ⑤ **Step 4:** The n data points are naturally embedded into k dimensions: **Y** = **W**<sup>T</sup> = [**y**<sub>1</sub>, ⋯, **y**<sub>n</sub>], with each column vector representing the embedded data points.

> ⑥ **Step 5:** Perform clustering, such as K-means clustering, on the column vectors of **Y**.

⑷ **Type 3.** Louvain Clustering: Uses Louvain Modularity Optimization. A type of graph-based method

<br>

![image](https://github.com/user-attachments/assets/3fa46ad6-34dc-41bf-abd0-4beb495ef5f9)

**Figure 9.** The principle of Louvain clustering

<br>

⑸ **Type 4.** Kernighan-Lin Algorithm: Uses Pearson correlation maximization.

⑹ **Type 5.** Stochastic Block Modeling: Uses statistical inference.

⑺ **Type 6.** Leiden Clustering

> ① Graph-based clustering method

> ② The Leiden algorithm used in the scanpy pipeline

<br>

```python
import scanpy as sc
import numpy as np

# Load your AnnData object
adata = sc.read_h5ad('my_h5ad.h5ad')

# Run PCA to reduce dimensionality
sc.pp.pca(adata, n_comps=20) 

# Compute the neighborhood graph
sc.pp.neighbors(adata)

# Run clustering using the Leiden algorithm (HERE!)
sc.tl.leiden(adata, resolution = 0.5)

# Optionally, compute UMAP for visualization
sc.tl.umap(adata)

# Plotting the results
sc.pl.umap(adata, color='leiden', title='Leiden Clustering')
sc.pl.spatial(adata, color = 'leiden')
```

<br>

<br>

## **7\. Other Clustering Algorithms**

 ⑴ SNN (Shared Nearest Neighbor) Modularity Optimization Based Clustering Algorithm

> ① **Step 1.** Search for K-nearest neighbors

> ② **Step 2.** Construct the SNN graph

> ③ **Step 3.** Optimize the modularity function to define clusters

> ④ Reference Paper: Waltman and van Eck (2013) _The European Physical Journal B_

⑵ [Topic Modeling](https://jb243.github.io/pages/956#5-type-4-latent-topic-model): LDA, NMF, CountClust (Dey et al., 2017), FastTopics (Carbonetto et al., 2023)

⑶ Mean-Shift Clustering

⑷ [NMF](https://jb243.github.io/pages/2050#footnote_link_67_52) (Non-Negative Matrix Factorization)

> ① Algorithm for decomposing known matrix A into the product of matrices W and H: A ~ W × H

>> ○ A Matrix: Sample-feature matrix, known from samples

>> ○ H Matrix: Variable-feature matrix

>> ○ This helps in extracting metagenes

>> ○ Similar to K means clustering, PCA algorithms

> ② Algorithm: Searches for U, V which satisfy R = UV, R ∈ ℝ<sup>5×4</sup>, U ∈ ℝ<sup>5×2</sup>, and V ∈ ℝ<sup>2×4</sup>.

<br>

```python
import numpy as np
R = np.outer([3,1,4,2.,3],[1,1,0,1]) + np.outer([1,3,2,1,2],[1,1,4,1])
U=np.random.rand(5,2)
V=np.random.rand(2,4)
for i in range(3):
    V,_,_,_=np.linalg.lstsq(U, R, rcond=None)
    Ut,_,_,_=np.linalg.lstsq(V.T, R.T, rcond=None)
    U=Ut.T
U@V # it is similar to R!
```

<br>

> ③ **NMF**(non-negative matrix factorization)

<br>

```python
import numpy as np
R = np.outer([3,1,4,2.,3],[1,1,0,1]) + np.outer([1,3,2,1,2],[1,1,4,1])
U=np.random.rand(5,2)
V=np.random.rand(2,4)
for i in range(20):
    V,_,_,_=np.linalg.lstsq(U, R, rcond=None)
    V = np.where(V >= 0, V, 0) #set negative entries equal zero
    Ut,_,_,_=np.linalg.lstsq(V.T, R.T, rcond=None)
    U=Ut.T
    U = np.where(U >= 0, U, 0) #set negative entries equal zero
U@V # it is similar to R!
```

<br>

> ④ Matrix completion (such as the Netflix algorithm): A process of applying matrix factorization to a masked version of matrix R.

<br>

```python
import numpy as np
R = np.outer([3,1,4,2.,3],[1,1,0,1]) + np.outer([1,3,2,1,2],[1,1,4,1])
mask=np.array([[1.,1,1,1],
               [1,0,0,0],
               [1,0,0,0],
               [1,0,0,0],
               [1,0,0,0]])
               
U=np.random.rand(5,2)
V=np.random.rand(2,4)
RR = U@V
RR = RR*(1-mask)+R*mask

for i in range(20):
    V,_,_,_=np.linalg.lstsq(U, R, rcond=None)
    V = np.where(V >= 0, V, 0)
    Ut,_,_,_=np.linalg.lstsq(V.T, R.T, rcond=None)
    U=Ut.T
    U = np.where(U >= 0, U, 0)
    RR = U@V
    RR = RR*(1-mask)+R*mask

RR*(1-mask)+R*mask # it is similar to R!
```

<br>

> ⑤ [SVD](https://jb243.github.io/pages/2158#5-type-4-singular-value-decomposition-svd)(singular value decomposition)

> ⑥ **Application 1.** Cell Type Classification

>> ○ For determining cell type proportion from scRNA-seq obtained from tissue

>> ○ Important to reduce the confounding effect of cell type heterogeneity

>> ○ **1-1.** Constrained linear regression

>> ○ **1-2.** Reference-based approach

>>> ○ **1-2-1.** CIBERSORT(cell-type identification by estimating relative subsets of RNA transcript): Allows checking cell type proportion, p value per sample

> ⑦ **Application 2.** Joint NMF: Extends to multi-omics

> ⑧ **Application 3.** Extracting metagenes

> ⑨ **Application 4.** Starfysh: The following describes an algorithm that infers archetypes from spatial transcriptomics data and identifies anchor spots representing each archetype.

>> ○ **Step 1.** Autoencoder Construction

<br>

<img width="100" alt="스크린샷 2025-04-15 오전 10 49 00" src="https://github.com/user-attachments/assets/1493b8d2-e85f-49a5-af15-1cf913ba5deb" />

<br>

>>> ○ X ∈ ℝ<sup>S×G</sup>: Input data (spots × genes)

>>> ○ D: Number of archetypes

>>> ○ B ∈ ℝ<sup>D×S</sup>: Encoder. This represents the inference of archetypes. The sum of each archetype’s distribution over all spots should be 1.

>>> ○ H = BX: Latent variable

>>> ○ W ∈ ℝ<sup>S×D</sup>: Decoder. This reconstructs the input data. The sum of all archetype weights for each spot should be 1.

>>> ○  Y = WBX: Reconstructed input

>> ○ **Step 2.** Solve the optimization problem to compute W and B

<br>

<img width="374" alt="스크린샷 2025-04-15 오전 10 51 27" src="https://github.com/user-attachments/assets/41b7792a-2992-45bd-82c1-0a76c8318f38" />

<br>

>> ○ **Step 3.** Anchor spots are selected as the spots with the highest weight for each archetype from the W matrix.

>> ○ **Step 4.** Granularity adjustment: If archetypes are close to each other, they are merged or adjusted using a hierarchical structure.

>> ○ **Step 5.** For each anchor, the nearest spots are identified to form an archetypal community, and marker genes are explored.

>> ○ **Step 6.** When a signature gene set is provided, archetypal marker genes are added to the existing gene set, and anchors are recalculated.

>>> ○ In this step, the stable marriage matching algorithm is used to pair each archetype with the most similar signature.

⑸ Edge Detection Algorithm 

> ① **Type 1.** Canny edge detection 

>> ○ **Step 1.** Gaussian blurring 

>> ○ **Step 2.** Find edge gradient strength and direction 

>> ○ **Step 3.** Trace along the edge: If the gradient size is greater than a certain value, the next pixel is selected based on edge direction

>> ○ Step 4. Suppress non-maximum edge: Remove the weak edge parallel to the strong edge

> ② **Type 2.** Region-oriented segmentation 

>> ○ **Step 1.** Start with a set of seed points 

>> ○ **Step 2.** From the seed points, grow regions by appending to each seed point the neighboring pixels that have similar properties. 

>> ○ Drawbacks: Selection of seed points, Selection of appropriate parameters, Lack of stopping rules 

<br>

<img width="502" alt="스크린샷 2024-11-21 오전 11 49 02" src="https://github.com/user-attachments/assets/9b89136e-1223-4b21-8ed5-c3a3e416e9da">

**Figure 10.** Region-oriented segmentation (Note that T = 3 means the cutoff of the difference in values)

<br>

> ③ **Type 3.** Region splitting and merging 

>> ○ **Step 1.** Split an image into four disjoint quadrants.

>> ○ **Step 2.** Merge any similar adjacent regions.

>> ○ **Step 3.** Stop when no further merging or splitting is possible.

>> ○ Drawback: Takes a long time, but can be alleviated by parallel computing.

<br>

![image](https://github.com/user-attachments/assets/ea261bb5-ed94-4230-9dac-387854d46d21)

**Figure 11.** region splitting and merging

<br>

⑹ [Watershed Algorithm](https://opencv-python.readthedocs.io/en/latest/doc/27.imageWaterShed/imageWaterShed.html)

> ① ****An algorithm to identify ROI in images based on threshold values like certain brightness levels

⑺ Thresholding Method

> ① Definition: A technique to create a binary image based on a threshold

> ② Example **:**[Otsu Thresholding Method](https://github.com/mohabmes/Otsu-Thresholding)

⑻ MST (Minimum Spanning Tree)

⑼ Curve Evolution

⑽ Sparse Neighboring Graph: A type of graph-based method

⑾ SC3

⑿ SIMLR

⒀ FICT

⒁ [Faiss](https://github.com/facebookresearch/faiss)

> ① A similarity-based search and dense vector clustering algorithm developed by Meta in 2023.

> ② Fast search algorithm: capable of obtaining the nearest neighbor and the k-th nearest neighbor.

> ③ Allows for the search of multiple vectors at once (batch processing).

> ④ There is a trade-off between accuracy and speed.

> ⑤ Instead of minimizing Euclidean distance, it calculates by maximizing the maximum inner product.

> ⑥ Returns all elements contained within a given radius.

⒂ mclust  

⒃ PAM clustering

⒄ Affinity propagation: Provided by [scikit package](https://scikit-learn.org/stable/modules/clustering.html)

⒅ BIRCH: Provided by [scikit package](https://scikit-learn.org/stable/modules/clustering.html)

<br>

---

_Entered: 2021.11.16 13:17_

_Edited: 2025.04.02 00:20_
