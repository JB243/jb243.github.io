## **Chapter 8. Clustering Algorithms** 

Recommended Article: „ÄêAlgorithm„Äë [Algorithm Index](https://jb243.github.io/pages/1278)

---

**1.** [Introduction](#1-introduction)

**2.** **Type 1.** [unsupervised hierarchical clustering](#2-type-1-unsupervised-hierarchical-clustering)

**3.** **Type 2.** [K-means clustering](#3-type-2-k-means-clustering)

**4.** **Type 3.** [Density-based clustering](#4-type-3-density-based-clustering-dbscan-density-based-spatial-clustering-of-applications-with-noise)

**5.** **Type 4.** [Mixture distribution clustering](#5-type-4-mixture-distribution-clustering)

**6.** **Type 5.** [Graph-based clustering](#6-type-5-graph-based-clustering-som-self-organizing-maps)

**7.** [Other Clustering Algorithms](#7-other-clustering-algorithms)

---

<br>

## **1\. Introduction**

 ‚ë¥ Clustering is an optimization problem

> ‚ë† **variability**

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/b4cadd85-9e67-4587-b59f-b63a6939d14e)

<br>

>> ‚óã variability can be defined as the sum of distance or distance squared

>> ‚óã [Types of distance functions](https://jb243.github.io/pages/879#footnote_link_67_51)

>> ‚óã Reason: big and bad is worse than small and bad

> ‚ë° **dissimilarity**

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/0fd55311-e1e8-4970-88dc-309d91840a03)

<br>

>> ‚óã Clustering is not about finding a global minimum for dissimilarity

>> ‚óã **Reason 1.** A trivial solution exists where ‚àÄvariability = 0 with as many clusters as samples

>> ‚óã **Reason 2.** It's unfair to penalize large clusters per se: big is not necessarily worse than small

>> ‚óã Constraints are needed: minimum distance between clusters, number of clusters, etc.

‚ëµ Clustering is an unsupervised algorithm

> ‚ë† Due to this, evaluating the performance of clustering algorithms is difficult, but it can be assessed as follows

> ‚ë° Clustering Quality Metrics

>> ‚óã **Method 1:** Average Silhouette Width (ASW)

>>> ‚óã Formula 

<br> 

<img width="228" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2025-03-12 ·Ñã·Ö©·Ñí·ÖÆ 3 37 43" src="https://github.com/user-attachments/assets/af671e80-f22f-49ea-a9cd-1784b6d8d838" />

<br>

>>>> ‚óã a(i): The average distance from i to all data points in the same cluster c<sub>i</sub>

>>>> ‚óã b(i): The lowest average distance from i to all data points in the other cluster c among all clusters c.

>>>> ‚óã The silhouette coefficient S(c) of cluster c is the average of s(i) for the data points within that cluster.

<br>

<img width="139" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2025-04-02 ·Ñã·Ö©·Ñå·Ö•·Ü´ 9 08 49" src="https://github.com/user-attachments/assets/d231d1d2-f5ed-47e1-a780-fe86f54c6289" />

<br>

>>> ‚óã Ranges between -1 and 1.  

>>> ‚óã A higher value indicates better clustering quality, meaning higher cohesion within clusters and better separation between clusters.  

>>> ‚óã Disadvantages:

>>>> ‚óã The score is inversely proportional to the number of clusters.  

>>>> ‚óã It is inaccurate for non-spherical clusters.  

>>> ‚óã Code: [`sklearn.metrics.silhouette_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html)

>> ‚óã **Method 2:** Davies-Bouldin Index (DBI)

<br>

<img width="300" height="78" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2025-09-08 ·Ñã·Ö©·Ñå·Ö•·Ü´ 9 52 47" src="https://github.com/user-attachments/assets/b6a9f94d-0f71-48ba-8714-b8a0b2402f83" />

<br>

>>> ‚óã Calculation Process:  

>>>> ‚óã Measure the similarity between each cluster and the most similar other cluster.  

>>>> ‚óã Define similarity as the ratio of inter-cluster distance to intra-cluster distance.  

>>>> ‚óã Average these similarities across all clusters to derive the DBI.  

>>> ‚óã The value is 0 or higher; a lower index indicates better cluster separation.  

>>> ‚óã Advantages:  

>>>> ‚óã Less affected by the number of clusters.  

>>> ‚óã Disadvantages:  

>>>> ‚óã May become inaccurate if clusters are poorly separated or if distances vary significantly.  

>>> ‚óã Code: [`sklearn.metrics.davies_bouldin_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.davies_bouldin_score.html)

>> ‚óã **Method 3:** Calinski-Harabasz Index (CHI)

>>> ‚óã Formula: Relates the number of clusters k and the number of data points n.  

<br>

<img width="502" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2025-01-21 ·Ñã·Ö©·Ñå·Ö•·Ü´ 11 05 50" src="https://github.com/user-attachments/assets/6b3f1eca-992f-4706-8f71-17114f23057f" />

<br>

>>> ‚óã The value is 0 or higher; a higher value indicates better clustering quality.  

>>> ‚óã Disadvantages:  
  
>>>> ‚óã The index decreases as cluster sizes increase.  

>>>> ‚óã Assumes variances are relatively consistent.  

>>> ‚óã Code: [`sklearn.metrics.calinski_harabasz_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski_harabasz_score.html)

>> ‚óã **Method 4:** Landscape Shape Index (LSI)

>> ‚óã **Method 5:** Percentage of Abnormal Spots (PAS)

>>> ‚óã Higher value indicates lower quality: the proportion of spots considered abnormal (i.e., not consistent with their neighboring spots) among all spots.

>>> ‚óã Used in spatial transcriptomics analysis.
 
> ‚ë° Clustering Comparison Metrics

>> ‚óã **Method 1:** Normalized Mutual Information (NMI)

>>> ‚óã Measures [mutual information](https://jb243.github.io/pages/2145) between two clusters and normalizes it to remain invariant to the cluster size or number of classes.  

>>> ‚óã Ranges between 0 and 1.  

>>> ‚óã Code: [`sklearn.metrics.normalized_mutual_info_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.normalized_mutual_info_score.html)

>> ‚óã **Method 2:** Adjusted Rand Index (ARI)

>>> ‚óã Evaluates the similarity between two clusters.  

>>> ‚óã Ranges between -1 and 1 (perfect match). A score of 0 indicates random clustering.  

>>> ‚óã Rand Index (RI)

<br>

<img width="235" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2025-01-21 ·Ñã·Ö©·Ñå·Ö•·Ü´ 11 03 52" src="https://github.com/user-attachments/assets/da7033ee-f809-49be-b81c-d8498eb431b6" />

<br>

>>> ‚óã Adjusted for the expected value of random cluster results.  

<br>

<img width="229" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2025-01-21 ·Ñã·Ö©·Ñå·Ö•·Ü´ 11 04 28" src="https://github.com/user-attachments/assets/7cd882a6-1141-4e94-a528-14437f3cfa97" />

<br>

>>> ‚óã Similarly, regarding n<sub>ij</sub>, a<sub>i</sub>, and b<sub>j</sub> from the contingency table, 

<br>

<img width="474" height="98" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2025-09-08 ·Ñã·Ö©·Ñå·Ö•·Ü´ 9 50 35" src="https://github.com/user-attachments/assets/30955ad3-e221-477c-b879-b6ebcdd3bf49" />

<br>

>>> ‚óã Code: [`sklearn.metrics.adjusted_rand_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html)

>> ‚óã **Method 3:** Adjusted Mutual Information (AMI)

>>> ‚óã Adjusts NMI for the expected value of random clustering.

<br>

<img width="400" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2025-01-21 ·Ñã·Ö©·Ñå·Ö•·Ü´ 11 02 51" src="https://github.com/user-attachments/assets/c553fffe-514e-4471-95c6-8ef415b09949" />

<br>

>>> ‚óã Ranges between -1 and 1.  

>>> ‚óã Effective in removing random clustering bias when there are many classes or imbalanced data.  

>>> ‚óã Code: [`sklearn.metrics.adjusted_mutual_info_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html)

>> ‚óã **Method 4:** Fowlkes-Mallows Index (FMI)

>>> ‚óã The geometric mean of precision and recall.  

>>> ‚óã Ranges between 0 and 1.  

>>> ‚óã Performance decreases with imbalanced data.  

>>> ‚óã Code: [`sklearn.metrics.fowlkes_mallows_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fowlkes_mallows_score.html)

>> ‚óã **Method 5:** Homogeneity

>>> ‚óã Measures the extent to which samples with the same label are grouped into a single cluster.  

>>> ‚óã Code: [`sklearn.metrics.homogeneity_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_score.html)

>> ‚óã **Method 6:** Completeness

>>> ‚óã Evaluates how well samples within the same cluster belong to the same label.  

>>> ‚óã Code: [`sklearn.metrics.completeness_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.completeness_score.html)

>> ‚óã **Method 7:** V-measure

>>> ‚óã Defined as: v = (1 + beta) √ó homogeneity √ó completeness / (beta √ó homogeneity + completeness)

>>> ‚óã Ranges between 0 and 1.  

>>> ‚óã Code: [`sklearn.metrics.v_measure_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.v_measure_score.html)

‚ë∂ Types of Clustering

> ‚ë† **Type 1.** Hierarchical algorithm: Successively splits or merges groups based on some distance measurement. 

>> ‚óã A deterministic approach. 

>> ‚óã **1-1.** Agglomerative: bottom-up approach. An approach that builds larger clusters.

>> ‚óã **1-2.** divisive: top-down approach. An approach that divides into smaller clusters. Most common implementation. 

> ‚ë° **Type 2.** Partitioning algorithm: Looks for a specific delineation that optimizes some global measure

>> ‚óã **2-1.** Hard

>> ‚óã **2-2.** Soft 

<br>

<br>

## **2\. Type 1.** Unsupervised hierarchical clustering

 ‚ë¥ Overview

> ‚ë† Often used to draw a [heatmap](https://www.r-graph-gallery.com/heatmap) from an internal matrix (Example: RStudio's heatmap function)

> ‚ë° A type of graph-based method

> ‚ë¢ Unlike non-hierarchical clustering, hierarchical clustering does not pre-determine the number of clusters

 ‚ëµ **Type 1.** **Divisive Analysis**

> ‚ë† A technique that starts from the entire group and separates objects with decreasing similarity

> ‚ë° **Stage 1:** Initially assign N elements to N clusters

> ‚ë¢ **Stage 2:** Merge the closest clusters into one

> ‚ë£ **Stage 3:** Repeat stage 2 until constraints such as the number of clusters are met

>> ‚óã In this case, a dendrogram is used

 ‚ë∂ **Type 2.** **Agglomerative Hierarchical Clustering** 

> ‚ë† Definition: A method that considers each entity as a subgroup and progressively merges similar subgroups to form new subgroups

> ‚ë° **Example 1.** UPGMA(unweighted pair group method with arithmetic mean)

<br>

![image](https://github.com/user-attachments/assets/232035c9-72cd-4ab2-9364-19c402eddf91)

<br>

 ‚ë∑ Linkage metric: Defining distance between clusters

> ‚ë† **Type 1.** Single-linkage: The shortest distance between elements of one cluster and another

> ‚ë° **Type 2.** Complete-linkage: The farthest distance between elements of one cluster and another

> ‚ë¢ **Type 3.** Average-linkage: The distance between the centers of two clusters

>> ‚óã Can also mean the average distance between elements of one cluster and another

> ‚ë£ **Type 4.** Centroid-linkage: Measuring the distance between the centers of two clusters

> ‚ë§ **Type 5.** Ward-linkage: A method of measuring distance between clusters based on within-cluster sum of squares

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/3724d8fb-165a-478d-aecd-b927ad889d2e)

**Figure 1.** Performance of clustering according to linkage metric

<br>

 ‚ë∏ Features

> ‚ë† One of the most frequently used clustering algorithms along with K-means clustering

> ‚ë° **Advantage 1.** Deterministic: can reach the same conclusion in any environment

> ‚ë¢ **Advantage 2.** Can try various linkage criteria

> ‚ë£ **Disadvantage 1.** Slow na√Øve algorithm. Generally requires Œü(n3) time complexity

>> ‚óã Reason: Œü(n<sup>2</sup>) is required to perform stage 2 and it has to be repeated Œü(n) times

>> ‚óã In some cases, Œü(n<sup>2</sup>) time complexity is required depending on the linkage criteria

 ‚ëπ [References](https://medium.com/machine-learning-researcher/clustering-k-mean-and-hierarchical-cluster-fa2de08b4a4b)

<br>

<br>

## **3\. Type 2.** K-means clustering

 ‚ë¥ Overview

> ‚ë† Often used in clustering algorithms targeting images

 ‚ëµ Algorithm

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/d3918cd3-65fe-4587-9169-4559368fe1d8)

**Figure 2.** How K-means clustering works

<br>

> ‚ë† **Stage 1:** Randomly select k initial centroids

> ‚ë° **Stage 2:** Assign clusters based on which centroid each element is closest to

> ‚ë¢ **Stage 3:** Calculate the center of each cluster to define new centroids

> ‚ë£ **Stage 4:** Clustering ends when centroids no longer change

 ‚ë∂ Features

> ‚ë† The most widely used clustering algorithm

> ‚ë° K-means clustering is one of the greedy algorithms for clustering

> ‚ë¢ **Determining K**

>> ‚óã **Background knowledge**

>>> ‚óã Example: Differentiating between cancerous and non-cancerous data in bio multi-omics data

>> ‚óã **Elbow Method**

>>> ‚óã A method to determine the number of clusters in cluster analysis by plotting the total within-cluster sum of squares against the number of clusters and selecting the elbow point as the appropriate number of clusters

>>> ‚óã Choose the cluster corresponding to the elbow part where the slope is gentle when the number of clusters is on the x-axis and SSE value on the y-axis

>> ‚óã **Silhouette Method**

>>> ‚óã A method that quantitatively calculates the quality of clustering using silhouette coefficients

>>> ‚óã Shows how well clusters are separated from each other

>>> ‚óã A silhouette coefficient close to 1 indicates well-optimized, well-separated clusters

>>> ‚óã A silhouette coefficient close to 0 indicates poorly optimized, closely spaced clusters

>> ‚óã **Dendrogram**

>>> ‚óã Use dendrogram visualization in hierarchical cluster analysis to determine the number of clusters

> ‚ë£ Advantage

>> ‚óã **Advantage 1.** Fast. Œü(Kn) time is required to perform stage 2 (K is the number of clusters, n is the number of data points)

>> ‚óã **Advantage 2.** Mathematically converges after a finite number of iterations as the objective function decreases with each iteration

> ‚ë§ Disadvantage

>> ‚óã **Disadvantage 1.** K should be given: Choosing the wrong K can lead to incorrect results

>> ‚óã **Disadvantage 2.** Depends on initial centroids: Results can vary depending on initial centroid position, so it's not deterministic

>> ‚óã **Disadvantage 3.** The complex data cannot be clustered because it relies on the Euclidean distance for specific points.

>>> ‚óã Each feature that makes up the data needs to be bi-modal or multi-modal for it to work well. ([ref](https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html))

>>> ‚óã **Example 1:** When the data consists of two concentric circles, the positions of the two centroids are the same, so K-means clustering cannot be used.

>>> ‚óã **Example 2:** When the clusters are not spherical.

<br>

<img width="420" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2025-04-02 ·Ñã·Ö©·Ñå·Ö•·Ü´ 9 39 19" src="https://github.com/user-attachments/assets/86440d7c-b782-4966-95e0-5087b3635246" />

**Figure 3.** When the clusters are not spherical 

<br>

>>> ‚óã **Example 3:** When the clusters have different densities.

<br>

<img width="420" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2025-04-02 ·Ñã·Ö©·Ñå·Ö•·Ü´ 9 39 58" src="https://github.com/user-attachments/assets/4b381feb-6fef-43bd-889c-51ef17d7904d" />

**Figure 4.** When the clusters have different densities

<br>

> ‚ë• Overcoming

>> ‚óã ISODATA: Estimates the value of K

>> ‚óã Kohonen Self-Organizing Map: Similar to evolutionary learning, gradually increases the value of K to reduce variability

>> ‚óã K-medoid clustering

>> ‚óã Lloyd's K-means clustering algorithm

>> ‚óã Bisecting K-Means: Provided by [scikit package](https://scikit-learn.org/stable/modules/clustering.html) 

>> ‚óã Fuzzy c-means clustering: Points belong to more than one cluster.

>> ‚óã Mean-shif: Provided by [scikit package](https://scikit-learn.org/stable/modules/clustering.html). Points are moved to the densest nearby area.

 ‚ë∑ Python code

```python
from sklearn.cluster import KMeans

X = [[1, 2, 3],[1, 2, 3.1],[2, 2, 4],[2, 2, 4.1]]
kmeans = KMeans(n_clusters=2, random_state=0)
kmeans.fit(X)
cluster_labels = kmeans.labels_

print(cluster_labels)
# [1 1 0 0]
```

 ‚ë∏ **unsupervised hierarchical clustering vs K-means clustering**

<br>

|   | **Unsupervised Hierarchical Clustering** | **K-means Clustering** |
| --- | --- | --- |
| Time Complexity | O(n<sup>3</sup>) | O(kn) √ó iteration |
| Randomness | Deterministic | Random |

 **Table. 1.** Comparison of unsupervised hierarchical clustering and K-means clustering

<br>

 ‚ëπ **K-NN vs K-means clustering**

<br>

| **Item** | **K-NN** | **K-means Clustering** |
| --- | --- | --- |
| Type | Supervised Learning | Unsupervised Learning |
| Meaning of k | Number of Nearest Neighbors | Number of Clusters |
| Optimization Techniques | Cross Validation, Confusion Matrix | Elbow Method, Silhouette Method |
| Application | Classification | Clustering |

 **Table 2.** K-NN vs K-means clustering

<br>

<br>

## **4. Type 3.** Density-Based Clustering (DBSCAN, density-based spatial clustering of applications with noise)

 ‚ë¥ Overview: One of the non-hierarchical clustering analyses like K-means clustering

> ‚ë† An algorithm that groups closely distributed entities based on the calculation of their densities

> ‚ë° No need to pre-specify the number of clusters

> ‚ë¢ Clusters are connected based on cluster density, allowing for clustering of geometric shapes

 ‚ëµ Components

> ‚ë† Core point

>> ‚óã A data point that has a minimum number(min_samples) of other data points within a certain radius(epsilon)

>> ‚óã The minimum number of data points required within the radius is set as a hyperparameter

> ‚ë° Neighbor point

>> ‚óã Other data existing within a certain radius around a specific data point

> ‚ë¢ Border point

>> ‚óã Not a core point but exists within the radius of a core point

>> ‚óã Included in the cluster centered around the core point and typically forms the outskirts of the cluster

> ‚ë£ Noise point

>> ‚óã Neither a core point nor satisfies the border point condition

>> ‚óã Also considered as an outlier

 ‚ë∂ Procedure

> ‚ë† **Stage 1.** Identify core points with a minimum number(min_samples) of points within a certain radius(epsilon).

> ‚ë° **Stage 2.** Explore connected components in the adjacency graph using only core points.

> ‚ë¢ **Stage 3.** Assign non-core points as noise.

<br>

![image](https://blog.kakaocdn.net/dn/Gd7Gt/btsM4PLwQcy/RFfGswvkKVo8jIkSpPH0N0/img.gif)

**Figure 5.** Gibbs Sampling initialization

<br>

 ‚ë∑ Features

 > ‚ë† **Advantage 1.** No need to predefine the number of clusters, unlike k-means clustering

> ‚ë° **Advantage 2.** Can identify clusters with geometric shapes due to clustering based on cluster density

> ‚ë¢ **Disadvantage 1.** Difficult to determine hyperparameters, sensitive to parameter choices

> ‚ë£ **Disadvantage 2.** Challenges in computation when clusters have varying densities or in higher dimensions

‚ë∏ Python example

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/07f9f44c-610e-4782-99b0-751eaf4bc10d)

<br>

**Figure 6.** DBSCAN Python example

<br>

```python
import pandas as pd
from sklearn.datasets import make_blobs
from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt

# Step 1: Generate sample data
X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)

# Step 2: Convert to DataFrame
df = pd.DataFrame(X, columns=['x', 'y'])

# DBSCAN parameters
epsilon = 0.5  
min_samples = 5 

# Step 3: Perform DBSCAN clustering
db = DBSCAN(eps=epsilon, min_samples=min_samples)
df['cluster_label'] = db.fit_predict(df[['x', 'y']])

# Step 4: Visualize the clustering results
plt.figure(figsize=(12, 8))
plt.scatter(df['x'], df['y'], c=df['cluster_label'], cmap='viridis', marker='o')
plt.title('DBSCAN Clustering')
plt.xlabel('X')
plt.ylabel('Y')
plt.show()
```

<br>

‚ëπ Application

> ‚ë† OPTICS

<br>

```python
(skip)

from sklearn.cluster import OPTICS

# OPTICS parameters
min_samples = 5
xi = 0.05
min_cluster_size = 0.05

# Perform OPTICS clustering
optics = OPTICS(min_samples=min_samples, xi=xi, min_cluster_size=min_cluster_size)
df['cluster_label'] = optics.fit_predict(df[['x', 'y']])

(skip)
```

<br>

> ‚ë° HDBSCAN

<br>

```python
(skip)

import hdbscan

# HDBSCAN parameters
min_cluster_size = 5
min_samples = 5

# Perform HDBSCAN clustering
hdb = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples)
df['cluster_label'] = db.fit_predict(df[['x', 'y']])

(skip)
```

<br>

> ‚ë¢ DBSCAN++

<br>

<br>

## **5. Type 4. Mixture Distribution Clustering**

 ‚ë¥ Mixture Distribution Clustering: One of the non-hierarchical clustering analyses, like K-means clustering

> ‚ë† Assumes data comes from a population model represented as a weighted sum of k parametric models, estimates parameters and weights from the data

> ‚ë° Each of the k models represents a cluster, and each data point is classified into a cluster based on which model it most likely came from

> ‚ë¢ Definition of Mixture Model: A mixture model represented as a weighted sum of M distributions (components)

>> <span>p(x | Œ∏) = Œ£ p(x | Ci, Œ∏<sub>i</sub>) p(C<sub>i</sub>)</span>

>> ‚óã p(x | C<sub>i</sub>, Œ∏<sub>i</sub>): The individual probability density function in the mixture model

>> ‚óã Œ∏<sub>i</sub>: The parameter vector of the i-th distribution

>> ‚óã C<sub>i</sub>: The i-th cluster (class)

>> ‚óã p(C<sub>i</sub>): The importance or weight (Œ±i) of the i-th cluster in the mixture model

> ‚ë£ Estimating parameters of the mixture model is complex compared to single models, often using the EM algorithm instead of differentiation

> ‚ë§ Accuracy of estimation may decrease or become challenging if the size of clusters is too small

> ‚ë• Sensitive to outliers, hence preprocessing such as outlier removal is necessary

 ‚ëµ Gaussian Mixture Model (GMM)

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/f4da2578-d054-41ca-a694-feb36b7f46df)

 **Figure 7.** Gaussian Mixture Model

<br>

> ‚ë† Overview
 
>> ‚óã Assumes the overall data probability distribution is a linear combination of k Gaussian distributions.

>> ‚óã Clusters are formed based on the probability of belonging to each distribution.

>> ‚óã Each feature that makes up the data needs to be bi-modal or multi-modal for it to work well. ([ref](https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html))

> ‚ë° Formula

>> ‚óã In GMM, estimates the weights, means, and covariances of the appropriate k Gaussian distributions for the given data X = {x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>N</sub>}

>> ‚óã Simplification: When x is one-dimensional data

<br>

<img width="352" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2024-10-16 ·Ñã·Ö©·Ñí·ÖÆ 11 51 24" src="https://github.com/user-attachments/assets/a975248a-20ec-4721-93c6-edb7ea6c3b94">

<br>

>> ‚óã Matrix Representation: When x is multi-dimensional data, for the mean **Œº**<sub>k</sub> and covariance matrix ‚àë<sub>k</sub> of the k-th cluster

<br>

<img width="451" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2024-10-16 ·Ñã·Ö©·Ñí·ÖÆ 11 51 53" src="https://github.com/user-attachments/assets/81bc09a4-6e8a-42fc-b253-813e83144c3f">

<br>

> ‚ë¢ EM algorithm (MLE estimation)

>> ‚óã EM algorithm can be used to estimate the optimal Gaussian distribution each data point belongs to

>> ‚óã Objective function: Expressed as log likelihood

<br>

<img width="526" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2024-10-17 ·Ñã·Ö©·Ñå·Ö•·Ü´ 12 23 21" src="https://github.com/user-attachments/assets/946abae7-6f04-455b-8f4b-453dbdf0f980">

<br>

>> ‚óã **Step 1:** Assign random values to Œ≥<sub>i,k</sub>

>> ‚óã **Step 2:** E-step: Estimate Œ≥<sub>i,k</sub>

<br>

<img width="451" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2024-10-17 ·Ñã·Ö©·Ñå·Ö•·Ü´ 12 23 47" src="https://github.com/user-attachments/assets/bcbfc1f7-5e0d-405c-b8e8-6b2c918edcbb">

<br>

>> ‚óã **Step 3:** M-step: Calculate a<sub>k</sub>, **Œº**<sub>k</sub>, ‚àë<sub>k</sub> from Œ≥<sub>i,k</sub>

<br>

<img width="353" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2024-10-17 ·Ñã·Ö©·Ñå·Ö•·Ü´ 12 24 10" src="https://github.com/user-attachments/assets/b9a308a2-fad3-4f69-b4a4-49bac160f3fb">

<br>

>> ‚óã **Step 4:** If Œ≥<sub>i,k</sub> converges, stop; otherwise, move to **Step 2**: Convergence is guaranteed due to the characteristics of the Gaussian function.

> ‚ë£ Comparison between K means clustering and Gaussian mixture model

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/2f3faec2-2312-4aa8-85f9-33b4b6cb1765)

 **Figure 8.** Comparison of K means clustering and Gaussian mixture model

<br>

<br>

## **6. Type 5.** Graph-Based Clustering (SOM, Self-Organizing Maps)

 ‚ë¥ Overview: One of the non-hierarchical clustering analyses, like K-means clustering

> ‚ë† SOM, developed by Kohonen, also known as Kohonen maps

> ‚ë° SOM is an artificial neural network modeled on the learning processes of the cerebral cortex and visual cortex

> ‚ë¢ Clustering by autonomous learning methods

> ‚ë£ A type of unsupervised neural network that maps high-dimensional data into an easy-to-understand low-dimensional arrangement of neurons

> ‚ë§ The mapping preserves the positional relationships of input variables

> ‚ë• In the actual space, if input variables are close, they are mapped close to each other on the map

> ‚ë¶ Very fast due to using only one **forward pass**

‚ëµ **Type 1.** SOM

> ‚ë† **Component 1.** Input Layer

>> ‚óã Receives the input vector, contains neurons equal to the number of input variables

>> ‚óã The input layer data is aligned in the competitive layer through learning, referred to as the map

>> ‚óã Each neuron in the input layer is **fully connected** to every neuron in the competitive layer

> ‚ë° **Component 2.** Competitive Layer

>> ‚óã A layer arranged in a 2D grid, where input vectors cluster into a single point based on their characteristics

>> ‚óã SOM uses competitive learning, where each neuron calculates its closeness to the input vector, repeatedly adjusting connection strengths

>> ‚óã Through the learning process, the connection strength makes the neuron most similar to the input pattern the winner

>> ‚óã Due to the winner-take-all structure, only the winning neuron appears in the competitive layer, with similar input patterns arranged in the same competitive neuron

> ‚ë¢ Learning Algorithm

>> ‚óã **Step 1.** Initialization: Initialize connection strengths for nodes in the SOM map

>> ‚óã **Step 2.** Input Vector: Present the input vector

>> ‚óã **Step 3.** Similarity Calculation: Calculate similarity between the input vector and prototype vectors using Euclidean distance

>> ‚óã **Step 4.** Prototype Vector Search: Find the prototype vector (BMU) closest to the input vector

>> ‚óã **Step 5.** Strength Readjustment: Readjust the connection strengths of the BMU and its neighbors

>> ‚óã **Step 6.** Repeat: Return to step 2 and repeat

‚ë∂ **Type 2.** [Spectral Clustering](https://jb243.github.io/pages/616#1-basics)

> ‚ë† Problem definition

<br>

<img width="127" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2025-04-06 ·Ñã·Ö©·Ñí·ÖÆ 11 40 18" src="https://github.com/user-attachments/assets/9a6f8e7f-94b4-419e-8fd4-1491fe7f1881" />

<br>

>> ‚óã Reason why the condition **y**<sup>T</sup>D**y**=1 is necessary: Without this condition, there exists a trivial solution **y**=**0**, and for any nonzero **y**, there exists a scalar c with 0 ‚â§ ùëê < 1 0‚â§c<1 such that c**y** makes the objective function arbitrarily small.

>> ‚óã Reason why the condition **y**<sup>T</sup>D**1**=0 is necessary: This condition is necessary and sufficient for **y**‚â†**1** (orthogonality). Without it, the minimum of the objective function is always 0 and always achieved at **y**=**1** (by the method of [Lagrange multipliers](https://jb243.github.io/pages/1813)).

<br>

<img width="335" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2025-04-06 ·Ñã·Ö©·Ñí·ÖÆ 11 40 00" src="https://github.com/user-attachments/assets/18861836-66b5-416c-afcc-df2e1194b482" />

<br>

> ‚ë° **Step 1:** Calculate the distance between the given n data points to create an n √ó n adjacency matrix **A**.

>> ‚óã The Gaussian kernel is often used.

> ‚ë¢ **Step 2:** Based on the adjacency matrix, calculate the [Laplacian matrix](https://jb243.github.io/pages/616) **L**: **L**<sub>n</sub> or **L**<sub>r</sub> can also be used.

>> ‚óã **L** = **D** - **A**, where **D**<sub>ii</sub> = ‚àë<sub>j</sub> **A**<sub>ij</sub>

>> ‚óã **L**<sub>n</sub> = **D**<sup>-1/2</sup>**LD**<sup>-1/2</sup> = **I** - **D**<sup>-1/2</sup>**AD**<sup>-1/2</sup>

>> ‚óã **L**<sub>t</sub> = **D**<sup>-1</sup>**A**

>> ‚óã **L**<sub>r</sub> = **D**<sup>-1</sup>**L** = **I** - **L**<sup>t</sup> = **D**<sup>-1/2</sup>**L**<sub>n</sub>**D**<sup>1/2</sup>

> ‚ë£ **Step 3:** Calculate the matrix of eigenvectors **W** = [**œâ**<sub>2</sub>, ¬∑¬∑¬∑, **œâ**<sub>k</sub>] ‚àà ‚Ñù<sup>n√ók</sup> corresponding to the k smallest eigenvalues of the Laplacian matrix.

>> ‚óã The closer the eigenvalue is to 0, the more connected the graph is.

>> ‚óã Exclude **œâ**<sub>1</sub> = 1, which corresponds to Œª<sub>1</sub> = 0, as it is trivial.

> ‚ë§ **Step 4:** The n data points are naturally embedded into k dimensions: **Y** = **W**<sup>T</sup> = [**y**<sub>1</sub>, ‚ãØ, **y**<sub>n</sub>], with each column vector representing the embedded data points.

> ‚ë• **Step 5:** Perform clustering, such as K-means clustering, on the column vectors of **Y**.

‚ë∑ **Type 3.** Louvain Clustering: Uses Louvain Modularity Optimization. A type of graph-based method

<br>

![image](https://github.com/user-attachments/assets/3fa46ad6-34dc-41bf-abd0-4beb495ef5f9)

**Figure 9.** The principle of Louvain clustering

<br>

‚ë∏ **Type 4.** Kernighan-Lin Algorithm: Uses Pearson correlation maximization.

‚ëπ **Type 5.** Stochastic Block Modeling: Uses statistical inference.

‚ë∫ **Type 6.** Leiden Clustering

> ‚ë† Graph-based clustering method

> ‚ë° The Leiden algorithm used in the scanpy pipeline

<br>

```python
import scanpy as sc
import numpy as np

# Load your AnnData object
adata = sc.read_h5ad('my_h5ad.h5ad')

# Run PCA to reduce dimensionality
sc.pp.pca(adata, n_comps=20) 

# Compute the neighborhood graph
sc.pp.neighbors(adata)

# Run clustering using the Leiden algorithm (HERE!)
sc.tl.leiden(adata, resolution = 0.5)

# Optionally, compute UMAP for visualization
sc.tl.umap(adata)

# Plotting the results
sc.pl.umap(adata, color='leiden', title='Leiden Clustering')
sc.pl.spatial(adata, color = 'leiden')
```

<br>

<br>

## **7\. Other Clustering Algorithms**

 ‚ë¥ SNN (Shared Nearest Neighbor) Modularity Optimization Based Clustering Algorithm

> ‚ë† **Step 1.** Search for K-nearest neighbors

> ‚ë° **Step 2.** Construct the SNN graph

> ‚ë¢ **Step 3.** Optimize the modularity function to define clusters

> ‚ë£ Reference Paper: Waltman and van Eck (2013) _The European Physical Journal B_

‚ëµ [Topic Modeling](https://jb243.github.io/pages/956#5-type-4-latent-topic-model): LDA, NMF, CountClust (Dey et al., 2017), FastTopics (Carbonetto et al., 2023)

‚ë∂ Mean-Shift Clustering

‚ë∑ [NMF](https://jb243.github.io/pages/2050#footnote_link_67_52) (Non-Negative Matrix Factorization)

> ‚ë† Algorithm for decomposing known matrix A into the product of matrices W and H: A ~ W √ó H

>> ‚óã A Matrix: Sample-feature matrix, known from samples

>> ‚óã H Matrix: Variable-feature matrix

>> ‚óã This helps in extracting metagenes

>> ‚óã Similar to K means clustering, PCA algorithms

> ‚ë° Algorithm: Searches for U, V which satisfy R = UV, R ‚àà ‚Ñù<sup>5√ó4</sup>, U ‚àà ‚Ñù<sup>5√ó2</sup>, and V ‚àà ‚Ñù<sup>2√ó4</sup>.

<br>

```python
import numpy as np
R = np.outer([3,1,4,2.,3],[1,1,0,1]) + np.outer([1,3,2,1,2],[1,1,4,1])
U=np.random.rand(5,2)
V=np.random.rand(2,4)
for i in range(3):
    V,_,_,_=np.linalg.lstsq(U, R, rcond=None)
    Ut,_,_,_=np.linalg.lstsq(V.T, R.T, rcond=None)
    U=Ut.T
U@V # it is similar to R!
```

<br>

> ‚ë¢ **NMF**(non-negative matrix factorization)

<br>

```python
import numpy as np
R = np.outer([3,1,4,2.,3],[1,1,0,1]) + np.outer([1,3,2,1,2],[1,1,4,1])
U=np.random.rand(5,2)
V=np.random.rand(2,4)
for i in range(20):
    V,_,_,_=np.linalg.lstsq(U, R, rcond=None)
    V = np.where(V >= 0, V, 0) #set negative entries equal zero
    Ut,_,_,_=np.linalg.lstsq(V.T, R.T, rcond=None)
    U=Ut.T
    U = np.where(U >= 0, U, 0) #set negative entries equal zero
U@V # it is similar to R!
```

<br>

> ‚ë£ Matrix completion (such as the Netflix algorithm): A process of applying matrix factorization to a masked version of matrix R.

<br>

```python
import numpy as np
R = np.outer([3,1,4,2.,3],[1,1,0,1]) + np.outer([1,3,2,1,2],[1,1,4,1])
mask=np.array([[1.,1,1,1],
               [1,0,0,0],
               [1,0,0,0],
               [1,0,0,0],
               [1,0,0,0]])
               
U=np.random.rand(5,2)
V=np.random.rand(2,4)
RR = U@V
RR = RR*(1-mask)+R*mask

for i in range(20):
    V,_,_,_=np.linalg.lstsq(U, R, rcond=None)
    V = np.where(V >= 0, V, 0)
    Ut,_,_,_=np.linalg.lstsq(V.T, R.T, rcond=None)
    U=Ut.T
    U = np.where(U >= 0, U, 0)
    RR = U@V
    RR = RR*(1-mask)+R*mask

RR*(1-mask)+R*mask # it is similar to R!
```

<br>

> ‚ë§ [SVD](https://jb243.github.io/pages/2158#5-type-4-singular-value-decomposition-svd)(singular value decomposition)

> ‚ë• **Application 1.** Cell Type Classification

>> ‚óã For determining cell type proportion from scRNA-seq obtained from tissue

>> ‚óã Important to reduce the confounding effect of cell type heterogeneity

>> ‚óã **1-1.** Constrained linear regression

>> ‚óã **1-2.** Reference-based approach

>>> ‚óã **1-2-1.** CIBERSORT(cell-type identification by estimating relative subsets of RNA transcript): Allows checking cell type proportion, p value per sample

> ‚ë¶ **Application 2.** Joint NMF: Extends to multi-omics

> ‚ëß **Application 3.** Extracting metagenes

> ‚ë® **Application 4.** Starfysh: The following describes an algorithm that infers archetypes from spatial transcriptomics data and identifies anchor spots representing each archetype.

>> ‚óã **Step 1.** Autoencoder Construction

<br>

<img width="100" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2025-04-15 ·Ñã·Ö©·Ñå·Ö•·Ü´ 10 49 00" src="https://github.com/user-attachments/assets/1493b8d2-e85f-49a5-af15-1cf913ba5deb" />

<br>

>>> ‚óã X ‚àà ‚Ñù<sup>S√óG</sup>: Input data (spots √ó genes)

>>> ‚óã D: Number of archetypes

>>> ‚óã B ‚àà ‚Ñù<sup>D√óS</sup>: Encoder. This represents the inference of archetypes. The sum of each archetype‚Äôs distribution over all spots should be 1.

>>> ‚óã H = BX: Latent variable

>>> ‚óã W ‚àà ‚Ñù<sup>S√óD</sup>: Decoder. This reconstructs the input data. The sum of all archetype weights for each spot should be 1.

>>> ‚óã  Y = WBX: Reconstructed input

>> ‚óã **Step 2.** Solve the optimization problem to compute W and B

<br>

<img width="374" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2025-04-15 ·Ñã·Ö©·Ñå·Ö•·Ü´ 10 51 27" src="https://github.com/user-attachments/assets/41b7792a-2992-45bd-82c1-0a76c8318f38" />

<br>

>> ‚óã **Step 3.** Anchor spots are selected as the spots with the highest weight for each archetype from the W matrix.

>> ‚óã **Step 4.** Granularity adjustment: If archetypes are close to each other, they are merged or adjusted using a hierarchical structure.

>> ‚óã **Step 5.** For each anchor, the nearest spots are identified to form an archetypal community, and marker genes are explored.

>> ‚óã **Step 6.** When a signature gene set is provided, archetypal marker genes are added to the existing gene set, and anchors are recalculated.

>>> ‚óã In this step, the stable marriage matching algorithm is used to pair each archetype with the most similar signature.

‚ë∏ Edge Detection Algorithm 

> ‚ë† **Type 1.** Canny edge detection 

>> ‚óã **Step 1.** Gaussian blurring 

>> ‚óã **Step 2.** Find edge gradient strength and direction 

>> ‚óã **Step 3.** Trace along the edge: If the gradient size is greater than a certain value, the next pixel is selected based on edge direction

>> ‚óã Step 4. Suppress non-maximum edge: Remove the weak edge parallel to the strong edge

> ‚ë° **Type 2.** Region-oriented segmentation 

>> ‚óã **Step 1.** Start with a set of seed points 

>> ‚óã **Step 2.** From the seed points, grow regions by appending to each seed point the neighboring pixels that have similar properties. 

>> ‚óã Drawbacks: Selection of seed points, Selection of appropriate parameters, Lack of stopping rules 

<br>

<img width="502" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2024-11-21 ·Ñã·Ö©·Ñå·Ö•·Ü´ 11 49 02" src="https://github.com/user-attachments/assets/9b89136e-1223-4b21-8ed5-c3a3e416e9da">

**Figure 10.** Region-oriented segmentation (Note that T = 3 means the cutoff of the difference in values)

<br>

> ‚ë¢ **Type 3.** Region splitting and merging 

>> ‚óã **Step 1.** Split an image into four disjoint quadrants.

>> ‚óã **Step 2.** Merge any similar adjacent regions.

>> ‚óã **Step 3.** Stop when no further merging or splitting is possible.

>> ‚óã Drawback: Takes a long time, but can be alleviated by parallel computing.

<br>

![image](https://github.com/user-attachments/assets/ea261bb5-ed94-4230-9dac-387854d46d21)

**Figure 11.** region splitting and merging

<br>

‚ëπ [Watershed Algorithm](https://opencv-python.readthedocs.io/en/latest/doc/27.imageWaterShed/imageWaterShed.html)

> ‚ë† ****An algorithm to identify ROI in images based on threshold values like certain brightness levels

‚ë∫ Thresholding Method

> ‚ë† Definition: A technique to create a binary image based on a threshold

> ‚ë° Example **:**[Otsu Thresholding Method](https://github.com/mohabmes/Otsu-Thresholding)

‚ëª MST (Minimum Spanning Tree)

‚ëº Curve Evolution

‚ëΩ Sparse Neighboring Graph: A type of graph-based method

‚ëæ SC3

‚ëø SIMLR

‚íÄ FICT

‚íÅ [Faiss](https://github.com/facebookresearch/faiss)

> ‚ë† A similarity-based search and dense vector clustering algorithm developed by Meta in 2023.

> ‚ë° Fast search algorithm: capable of obtaining the nearest neighbor and the k-th nearest neighbor.

> ‚ë¢ Allows for the search of multiple vectors at once (batch processing).

> ‚ë£ There is a trade-off between accuracy and speed.

> ‚ë§ Instead of minimizing Euclidean distance, it calculates by maximizing the maximum inner product.

> ‚ë• Returns all elements contained within a given radius.

‚íÇ mclust  

‚íÉ PAM clustering

‚íÑ Affinity propagation: Provided by [scikit package](https://scikit-learn.org/stable/modules/clustering.html)

‚íÖ BIRCH: Provided by [scikit package](https://scikit-learn.org/stable/modules/clustering.html)

<br>

---

_Entered: 2021.11.16 13:17_

_Edited: 2025.04.02 00:20_
