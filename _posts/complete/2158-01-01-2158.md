## **Chapter 7. Dimension Reduction Algorithm**

Recommended post **:** 【Algorithm】 [Algorithm Index](https://jb243.github.io/pages/1278)

---

**1.** [Overview](#1-overview)

**2.** **Type 1.** [Principal Component Analysis (PCA)](#2-type-1-principal-component-analysis-pca)

**3\. Type 2.** [SNE, symmetric-SNE, tSNE](#3-type-2-sne-symmetric--sne-tsne)

**4\. Type 3.** [UMAP](#4-type-3-umap)

**5\. Type 4.** [SVD](#5-type-4-svd)

**6\. Type 5.** [ICA](#6-type-5-ica)

**7. Type 6.** [PPCA](#7-type-6-ppca)

**8\.** [Others](#8-others)

---

**a.** [SNE, symmetric-SNE, tSNE](https://jb243.github.io/pages/1730)

**b.** [UMAP](https://jb243.github.io/pages/2203) (uniform manifold approximation and projection)

---

<br>

## **1\. Overview**

 ⑴ Definition of Dimension Reduction

> ① When given high-dimensional data X<sub>(1)</sub>, ···, X<sub>(n)</sub> ∈ ℝ<sup>p</sup>,

> ② Find an appropriate [subspace](https://jb243.github.io/pages/1897) V ⊂ ℝ<sup>p</sup> (where dim(V) = k),

> ③ Calculate the projection of X onto V, denoted as X̃.

> ④ It is an unsupervised algorithm.

⑵ Characteristics

> ① Information preservation

> ② Easier model training **:** In machine learning, it requires fewer parameters, so it is faster with lower computation.

> ③ Easier result interpretation **:** Visualization of high-dimensional data.

> ④ Noise reduction

> ⑤ It can reveal the true dimensionality of the given data.

 ⑶ Methods

> ① Feature selection

>> ○ Selecting a few important variables from the existing ones and discarding the rest.

>> ○ Choose one variable with a high correlation or high variance inflation factor (VIF).

> ② Feature extraction

>> ○ Combine all variables to derive new variables that best represent the data.

>> ○ Techniques that combine existing variables to create new ones.

<br>

<br>

## **2\. Type 1.** Principal Component Analysis (PCA)

⑴ Overview

> ① Definition **:** A dimension reduction technique that transforms correlated variables into orthogonal variables.

> ② In other words, it is a dimension reduction method that leaves only a few meaningful coordinate systems.

> ③ The orthogonal variables are called principal components, which can be used when they contain as much information as the original data.

> ④ It originates from the awareness that the given dimensionality and the true dimensionality of the data may differ.

> ⑤ Introduced by Pearson in 1901.

> ⑥ PCA can be defined not only in subspace (passing through the origin) but also in affine space.

> ⑦ **** Normalize by the mean and then perform PCA on the subspace.

> ⑧ PCA does not require multivariate normality of the given data.

> ⑨ It can be used when the data is linearly distributed.

> ⑩ Whitening: It is better to standardize features along the principal component directions when performing standardization.

⑵ Premises

> ① Represent the basis of the space where the data belongs as e<sub>1</sub>, ···, e<sub>p</sub> (the set of bases does not need to be orthogonal).

>> ○ e<sub>1</sub>, ···, e<sub>p</sub> can be considered as bases representing the given dimensions (no need for orthogonality).

> ② Goal **:** To find the orthogonal basis Z<sub>1</sub>, ···, Z<sub>k</sub> of such a subspace (k < n) (typical unsupervised problem setting).

>> ○ Z<sub>1</sub>, ···, Z<sub>k</sub> can be considered as bases representing the true dimensionality of the data.

<br>

![image](https://github.com/user-attachments/assets/a040256a-0f86-4033-a62e-5a0b740e92d4)

<br>

> ③ **Random vector**

>> ○ X represents a specific data point **:** x<sub>1</sub>, ···, x<sub>n</sub> are each component.

<br>

![image](https://github.com/user-attachments/assets/15c6f2f7-84d2-4c1e-980d-6b64528b5949)

<br>

> ④ **Mean vector** **:** Also called population mean.

<br>

![image](https://github.com/user-attachments/assets/a2a5e5c0-320a-4f1b-a3c0-c2f2d07055ef)

<br>

> ⑤ **Variance-covariance matrix**

<br>

![image](https://github.com/user-attachments/assets/0e9b988b-f0ef-4a41-b8df-e10caf819d54)

<br>

> ⑥ **Unbiased sample variance-covariance matrix** **:** With the design matrix **X**, and center matrix **X<sub>c</sub>**,

<br>

![image](https://github.com/user-attachments/assets/c937618b-0adc-486c-b16d-4eb67d43f50e)

<br>

>> ○ Used when the population variance-covariance matrix is unknown.

>> ○ Since **X** ∈ ℝ<sup>n×p</sup>, **X**<sup>T</sup>**X** ∈ ℝ<sup>p×n</sup> × ℝ<sup>n×p</sup> = ℝ<sup>p×p</sup>.

> ⑦ Expression of the goal orthogonal basis

<br>

![image](https://github.com/user-attachments/assets/08803bfd-bdfb-43bc-8f23-bed4bbf0e675)

<br>

>> ○ Z<sub>k</sub> **:** = k-th PC ∈ ℝ<sup>p×1</sup>.

> ⑧ Characteristics of the goal orthogonal basis **:** Easily derived from linear algebra concepts.

<br>

![image](https://github.com/user-attachments/assets/37c61a4f-5048-4d0b-a22d-073835dcd87a)

<br>

>> ○ Var, Cov, etc., are specific values related to variance, belonging to ℝ<sup>1x1</sup>.

>> ○ Do not confuse with the variance-covariance matrix.

⑶ Definition of the PCA problem

<br>

![image](https://github.com/user-attachments/assets/724c5796-2854-43e5-833b-158fed21bd29)

<br>

⑷ Solution to the PCA problem

> ① **Method 1.** Eigen-decomposition of ∑.

>> ○ **Step 1:** Data Construction and Centering

>>> ○ **X** ∈ ℝ<sup>n×d</sup>: A data matrix with _n_ samples and _d_ features.

>>> ○ Center the data matrix **X**: **X**<sub>c</sub> = **X** - **μ** with **μ** ∈ ℝ<sup>d</sup> is the vector of column-wise means)

>> ○ **Step 2:** Covariance Matrix Calculation

>>> ○ Compute the sample covariance matrix **C** ∈ ℝ<sup>d×d</sup> from the centered data: 

<br>

<img width="99" alt="스크린샷 2024-11-16 오전 10 28 08" src="https://github.com/user-attachments/assets/ef233ea5-b06a-480b-8c9c-7738b79e98b5">

<br>

>>> ○ Since the given sample represents the entire population, the population mean is known, so the sample covariance is calculated by dividing by n instead of n−1.

>> ○ **Step 3:** Eigen-decomposition

>>> ○ Perform eigen-decomposition of the covariance matrix _C_ to compute eigenvalues and eigenvectors: **C** · **p**<sub>i</sub> = λ<sub>i</sub> · **p**<sub>i</sub>

>>> ○ Here, λ<sub>i</sub> represents the eigenvalue and **p**<sub>i</sub> ​∈ R<sup>d</sup> is the eigenvector.

>>> ○ Note: The symmetric matrix C can always be orthogonally diagonalized (by the [spectral theorem](https://jb243.github.io/pages/1928)).

>> ○ **Step 4:** Data Transformation

>>> ○ Define **P** = [**p**<sub>1</sub>, **p**<sub>2</sub>, …, **p**<sub>d</sub>] ∈ R<sup>d×d</sup>, a matrix where the eigenvectors are stored as columns.

>>> ○ Transform the data into a new coordinate system: **Y** = **X**<sub>c</sub>**P** where **Y** ∈ R<sup>n×d</sup> is the transformed data matrix.

>> ○ **Step 5:** Diagonalization of the Covariance Matrix

>>> ○ The covariance matrix of the transformed data **Y** becomes diagonal:

<br>

<img width="445" alt="스크린샷 2024-11-16 오전 10 32 40" src="https://github.com/user-attachments/assets/b6254144-078c-45ac-a26a-2f7999eeb9e8">

<br>

>> ○ **Step 6:** Principal Component Selection and Dimensionality Reduction

>>> ○ Select _k_ principal components (eigenvectors) corresponding to the largest eigenvalues λ<sub>i</sub> and reduce the dimensionality: **Y**<sub>k</sub> = **X**<sub>c</sub>**P**<sub>k</sub> with **P**<sub>k</sub> ∈ ℝ<sup>d×k</sup>, **Y**<sub>k</sub> ∈ ℝ<sup>n×k</sup> 

>>> ○ Alternatively, instead of fixing the number of principal components, eigenvalues smaller than a specific threshold η can be excluded.

>> ○ **Conclusion 1.** The d-th principal component PC<sub>d</sub> is the eigenvector corresponding to the d-th largest eigenvalue.

>> ○ **Conclusion 2.** The meaning of eigenvectors with the same eigenvalue **:** The magnitude of the corresponding principal component is the same.

> ② **Method 2.** Singular value decomposition (SVD) of the center matrix **X**<sub>c</sub>: The standard method currently used in PCA algorithms.

>> ○ **Step 1:** Center the data to obtain the centered data **X**<sub>c</sub>.

>> ○ **Step 2:** Define ​**C** = (1/n) **X**<sub>c</sub><sup>**T**</sup>**X**<sub>c</sub> = ∑ (covariance matrix)
​
>> ○ **Step 3:** Since **C** is symmetric, it can be orthogonally diagonalized as **C** = **VDV**<sup>T</sup> (where **V** is the same as the **V** in **X**<sub>c</sub> = **U**∑**V**<sup>T</sup>)

>> ○ **Step 4.** The 1<sup>st</sup>, 2<sup>nd</sup>, ··· th column vectors of **V** are the eigenvectors corresponding to the 1<sup>st</sup>, 2<sup>nd</sup>, ··· th largest eigenvalues and are the principal components.

> ③ **Method 3.** [Lagrange multiplier under equality constraint](https://jb243.github.io/pages/1813).

 ⑸ Determining the number of axes

> ① **Method 1.** Scree plot for the fraction of total variance.

>> ○ Definition of the fraction of total variance.

<br>

![image](https://github.com/user-attachments/assets/13b00d84-8318-4d3c-955c-c3ac61886aab)

<br>

>> ○ Graphical representation.

<br>

![image](https://github.com/user-attachments/assets/da97e824-00d1-4e02-a058-4d54b40e2d7f)

**Figure 1.** Example of axis setting.

<br>

![image](https://github.com/user-attachments/assets/888e17f2-c14b-4b9f-964d-97e2001fdb2c)

**Figure 2.** Scree plot **:** Shows the amount of variance explained by each principal component.

<br>

> ② **Method 2.** Scree plot for eigenvalues.

> ③ **Method 3.** Use in-sample 10-fold cross-validation to determine the number of axes p that minimizes MSPE.

<br>

![image](https://github.com/user-attachments/assets/d9cf92b1-9211-45bd-a984-c8d3c55a966b)

**Figure 3.** Determination of the number of axes.

<br>

> ④ Since information about the remaining axes is lost, PCA functions as a lossy compression algorithm.

⑹ Development of PCA

> ① Kernel PCA: Uses [kernels](https://jb243.github.io/pages/1897). Can either extend the given data to high dimensions or reduce it to lower dimensions through projection.

> ② MDS (Multidimensional Scaling)

> ③ SNE, t-SNE

> ④ UMAP

<br>

<br>

## 3\. Type 2.[SNE, symmetric-SNE, tSNE](https://jb243.github.io/pages/1730)

⑴ A dimension reduction algorithm that attempts to preserve local neighborhoods in the data.

⑵ Nonlinear and non-deterministic.

⑶ Requires knowledge of cost function, updating algorithm, etc., related to [deep learning](https://jb243.github.io/pages/1138).

<br>

<br>
 
## 4\. Type 3. [UMAP](https://jb243.github.io/pages/2203)

<br>

<br>

## **5\. Type 4.** Singular Value Decomposition (SVD)

⑴ A method that extracts singular values from an M × N dimensional matrix and reduces the dimensionality of the given data.

⑵ Formula **:** Let A be an m × n matrix with rank r.

> ① Then there exists an m × n matrix ∑ whose diagonal entries are the r nonzero singular values of A, σ<sub>1</sub> ≥ σ<sub>2</sub> ≥ ··· ≥ σ<sub>r</sub> > 0

<br>

<img width="393" alt="스크린샷 2024-10-22 오전 11 44 56" src="https://github.com/user-attachments/assets/3f9d04a4-797c-49ec-9152-4c62dfa74cae">

<br>

> ② and there exists an m × m orthogonal matrix U and an n × n orthogonal matrix V such that

>> ○ The SVD is not unique since we can switch the sign of a column of U and the sign of the corresponding column of V.

<br>

<img width="97" alt="스크린샷 2024-10-22 오전 11 34 47" src="https://github.com/user-attachments/assets/69e9e2b5-a161-46b1-b66d-228a7b78d62c">

<br>

> ③ Python code

<br>

```python
# np.linalg.svd computes the SVD of a matrix
A = np.array([[4,11,14],
            [8,7,-2]])
U, s, Vt=np.linalg.svd(A)
print(U)
print(s) # s is the vector of nonzero singular values
print(Vt)
```

<br>

⑶ **Application 1.** A<sup>T</sup>A = V(∑<sup>T</sup>∑)V<sup>T</sup> : The singular values of A are the square roots of the eigenvalues of A<sup>T</sup>A. However, some eigenvalues are set to 0 according to the rank.

⑷ **Application 2.** reduced SVD: Note that for the reduced version, both U and V have orthonormal columns. This means that U<sup>T</sup>U = I and V<sup>T</sup>V = I. However, U and V are not square in this version, so they are not orthogonal matrices.

<br>

![image](https://github.com/user-attachments/assets/ac360053-c3ee-48e0-aa9c-8df67c0de893)

<br>

```python
A = np.array([[4,11,14],
            [8,7,-2]])
U, s, Vt=np.linalg.svd(A, full_matrices=False)
print(U)
print(s)
print(Vt)
```

<br>

⑸ **Application 3.** rank-k approximation

> ① For k < rank(A), the m × n matrix A can be split into m × k + k × n matrices to achieve compression.

<br>

![image](https://github.com/user-attachments/assets/1c811701-96a9-4742-8fd9-754983d8fe7d)

<br>

> ② Given k, how to find the optimal rank-k approximation of A:

>> ○ A = UΣV<sup>T</sup> = σ<sub>1</sub>**u**<sub>1</sub>v<sub>1</sub><sup>T</sup> + ··· + σ<sub>r</sub>**u**<sub>r</sub>**v**<sub>r</sub><sup>T</sup>, where σ<sub>1</sub> ≥ ⋯ ≥ σ<sub>r</sub>, remove the smaller singular values σ<sub>r</sub>, σ<sub>r−1</sub>, ….

>> ○ By doing this, the Frobenius distance between A<sup>(k)</sup> and A is defined as follows:

<br>

![image](https://github.com/user-attachments/assets/570427b5-9fee-4986-8b08-a38539a9d781)

<br>

⑹ **Application 4.** PCA(principal component analysis)

> ① **Step 1.** Center the data

<br>

<img width="393" alt="스크린샷 2024-11-16 오전 8 45 10" src="https://github.com/user-attachments/assets/fc2361b9-742d-4697-a744-aee1af619126">

**Figure. 4.** Data centering of PCA

<br>

<img width="409" alt="스크린샷 2024-11-16 오전 8 45 28" src="https://github.com/user-attachments/assets/3e222a33-e7ae-4ccc-8560-3692470c786d">

<br>

> ② **Step 2.** **X**<sub>c</sub> is the centered data and **C** = (1/n) **X**<sub>c</sub><sup>T</sup>**X**<sub>c</sub>

> ③ **Step 3.** **C** is symmetric, so it is orthogonally diagonalizable as **C** = **VDV**<sup>T</sup>. **V** is the same as in **X**<sub>c</sub> = **U**∑**V**<sup>T</sup> in SVD. 

> ④ **Step 4.** The 1<sup>st</sup>, 2<sup>nd</sup>, ··· th column vectors of **V** are the eigenvectors corresponding to the 1<sup>st</sup>, 2<sup>nd</sup>, ··· th largest eigenvalues and are the principal components.

<br>

<br>
 
## **6\. Type 5.** Independent Component Analysis (ICA)

⑴ Unlike PCA, it reduces dimensionality by statistically separating multivariate signals into independent components.

⑵ The distribution of independent components follows a non-normal distribution.

<br>

<br>

## **7\. PPCA**

⑴ Formulation

> ① **Step 1.** Assume the observed data x is generated by the latent variable z: The following transformation itself becomes a dimensionality reduction technique.

<br>

<img width="145" alt="스크린샷 2024-11-07 오전 11 56 26" src="https://github.com/user-attachments/assets/f2c679ac-9f18-42c7-8930-3c83555d29c0">

<br>

>> ○ W: Transformation matrix of dimension D × d (for mapping from latent space to observed space)

>> ○ b: Mean vector of dimension D

>> ○ ϵ: Noise, following a normal distribution N(0,σ<sup>2</sup>I)

> ② **Step 2.** Assumption on the latent variable z

<br>

<img width="145" alt="스크린샷 2024-11-07 오전 11 56 18" src="https://github.com/user-attachments/assets/5d674833-40fb-4622-a7e1-aabde181ee86">

<br>

> ③ **Step 3.** Conditional distribution of x given z

<br>

<img width="264" alt="스크린샷 2024-11-07 오전 11 56 49" src="https://github.com/user-attachments/assets/cc266b17-e1e1-4ce6-b810-7fd3138368aa">

<br>

> ④ **Step 4.** Marginal distribution p(x)

<br>

<img width="264" alt="스크린샷 2024-11-07 오전 11 57 08" src="https://github.com/user-attachments/assets/f3a1fd86-f285-43d0-ae14-5f4c4c320074">

<br>

> ⑤ **Step 5.** MLE Estimation

>> ○ Maximum Likelihood Estimation (MLE) is used to estimate b, W, and σ from the given data.

>> ○ The MLE solution is obtained in closed form.

>> ○ When σ → 0, the method converges to standard PCA.

⑵ Code

<br>

```python
import numpy as np
from numpy import shape, isnan, nanmean, average, zeros, log, cov
from numpy import matmul as mm
from numpy.matlib import repmat
from numpy.random import normal
from numpy.linalg import inv, det, eig
from numpy import identity as eye
from numpy import trace as tr
from scipy.linalg import orth

def ppca(Y,d,dia):
    """
    Implements probabilistic PCA for data with missing values,
    using a factorizing distribution over hidden states and hidden observations.

    Args:
        Y:   (N by D ) input numpy ndarray of data vectors
        d:   (  int  ) dimension of latent space
        dia: (boolean) if True: print objective each step

    Returns:
        ss: ( float ) isotropic variance outside subspace
        C:  (D by d ) C*C' + I*ss is covariance model, C has scaled principal directions as cols
        M:  (D by 1 ) data mean
        X:  (N by d ) expected states
        Ye: (N by D ) expected complete observations (differs from Y if data is missing)

        Based on MATLAB code from J.J. VerBeek, 2006. http://lear.inrialpes.fr/~verbeek
    """
    N, D      = shape(Y) #N observations in D dimensions (i.e. D is number of features, N is samples)
    threshold = 1E-4 #minimal relative change in objective function to continue
    hidden    = isnan(Y)
    missing   = hidden.sum()

    if(missing > 0):
        M = nanmean(Y, axis=0)
    else:
        M = average(Y, axis=0)

    Ye = Y - repmat(M,N,1)

    if(missing > 0):
        Ye[hidden] = 0

    #initialize
    C = normal(loc=0.0, scale=1.0, size=(D,d))
    CtC = mm(C.T, C)
    X   = mm(mm(Ye, C), inv(CtC))
    recon = mm(X, C.T)
    recon[hidden] = 0
    ss = np.sum((recon-Ye)**2) / (N*D - missing)

    count = 1
    old = np.inf

    #EM Iterations
    while(count):
        Sx = inv(eye(d) + CtC/ss) #E-step, covariances
        ss_old = ss
        if(missing > 0):
            proj = mm(X,C.T)
            Ye[hidden] = proj[hidden]
        
        X = mm(mm(Ye, C), Sx/ss) #E-step: expected values

        SumXtX = mm(X.T, X) #M-step
        C = mm(mm(mm(Ye.T, X),(SumXtX + N*Sx).T), inv(mm((SumXtX + N*Sx), (SumXtX + N*Sx).T)))
        CtC = mm(C.T, C)
        ss = (np.sum((mm(X, C.T) - Ye)**2) + N*np.sum(CtC*Sx) + missing*ss_old) / (N*D)
        #transform Sx determinant into numpy float128 in order to deal with high dimensionality
        Sx_det = np.min(Sx).astype(np.float64)**shape(Sx)[0] * det(Sx / np.min(Sx))
        objective = N*D + N*(D*log(ss)+tr(Sx)-log(Sx_det)) + tr(SumXtX) - missing*log(ss_old)
        
        rel_ch = np.abs( 1 - objective / old )
        old = objective
        
        count = count + 1
        if( rel_ch < threshold and count > 5 ):
            count = 0
        if(dia == True):
            print('Objective: %.2f, Relative Change %.5f' %(objective, rel_ch))
    
    C = orth(C)
    covM = cov(mm(Ye, C).T)
    vals, vecs = eig( covM )
    ordr = np.argsort(vals)[::-1]
    vals = vals[ordr]
    vecs = vecs[:,ordr]
    
    C = mm(C, vecs)
    X = mm(Ye, C)
    
    #add data mean to expected complete data
    Ye = Ye + repmat(M,N,1)

    return C, ss, M, X, Ye
```

<br>

<br>
 
## **8\. Others**

⑴ Factor analysis

> ① Definition **:** A method of interpreting the structure within the data by grouping similar variables based on their correlations.

> ② Assumes the existence of latent variables that cannot be observed in the data.

> ③ Often used in social sciences or surveys.

⑵ Multi-dimensional scaling (MDS)

> ① Definition **:** A statistical technique for visualizing proximity and grouping of objects in a lower-dimensional space by identifying patterns and structures latent in the data.

> ② Measures the similarities and dissimilarities between objects and represents them as points in a 2D or 3D space.

⑶ Linear discriminant analysis (LDA)

> ① Definition **:** A method that reduces dimensionality by projecting data onto a lower-dimensional space, similar to PCA.

⑷ CCA (Canonical-correlation analysis) **:** Used in the [Seurat pipeline](https://jb243.github.io/pages/1788).

⑸ RPCA (Reciprocal principal component analysis) **:** Used in the [Seurat pipeline](https://jb243.github.io/pages/1788).

⑹ PHATE

⑺ ODA

⑻ LLE

⑼ LSA

⑽ [PLSA](https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis) (probabilistic latent semantic analysis)

> ① Used to decompose mixed components from the latent class model.

> ② Application : Used for dimension reduction in mass spectrometry data.

<br>

---

_Input: 2021.12.3 16:22_
