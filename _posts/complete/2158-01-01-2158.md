## **Chapter 7. Dimension Reduction Algorithm**

Recommended post: 【Algorithm】 [Algorithm Index](https://jb243.github.io/pages/1278)

---

**1.** [Overview](#1-overview)

**2.** **Type 1.** [Principal Component Analysis (PCA)](#2-type-1-principal-component-analysis-pca)

**3\. Type 2.** [SNE, symmetric-SNE, tSNE](#3-type-2-sne-symmetric--sne-tsne)

**4\. Type 3.** [UMAP](#4-type-3-umap)

**5\. Type 4.** [SVD](#5-type-4-singular-value-decomposition-svd)

**6\. Type 5.** [ICA](#6-type-5-ica)

**7. Type 6.** [PPCA](#7-type-6-ppca)

**8. Type 7.** [CCA](#8-type-7-cca)

**9\.** [Others](#8-others)

---

**a.** [SNE, symmetric-SNE, tSNE](https://jb243.github.io/pages/1730)

**b.** [UMAP](https://jb243.github.io/pages/2203) 

---

<br>

## **1\. Overview**

 ⑴ Definition of Dimension Reduction

> ① When given high-dimensional data X<sub>(1)</sub>, ···, X<sub>(n)</sub> ∈ ℝ<sup>p</sup>,

> ② Find an appropriate [subspace](https://jb243.github.io/pages/1897) V ⊂ ℝ<sup>p</sup> (where dim(V) = k),

> ③ Calculate the projection of X onto V, denoted as X̃.

> ④ It is an unsupervised algorithm.

⑵ Characteristics

> ① Information preservation

> ② Easier model training: In machine learning, it requires fewer parameters, so it is faster with lower computation.

> ③ Easier result interpretation: Visualization of high-dimensional data.

> ④ Noise reduction

> ⑤ It can reveal the true dimensionality of the given data.

 ⑶ Methods

> ① Feature selection

>> ○ Selecting a few important variables from the existing ones and discarding the rest.

>> ○ Choose one variable with a high correlation or high variance inflation factor (VIF).

> ② Feature extraction

>> ○ Combine all variables to derive new variables that best represent the data.

>> ○ Techniques that combine existing variables to create new ones.

<br>

<br>

## **2\. Type 1.** Principal Component Analysis (PCA)

⑴ Overview

> ① Definition: A dimension reduction technique that transforms correlated variables into orthogonal variables.

> ② In other words, it is a dimension reduction method that leaves only a few meaningful coordinate systems.

> ③ The orthogonal variables are called principal components, which can be used when they contain as much information as the original data.

> ④ It originates from the awareness that the given dimensionality and the true dimensionality of the data may differ.

> ⑤ Introduced by Pearson in 1901.

> ⑥ PCA can be defined not only in subspace (passing through the origin) but also in affine space.

> ⑦ **** Normalize by the mean and then perform PCA on the subspace.

> ⑧ PCA does not require multivariate normality of the given data.

> ⑨ It can be used when the data is linearly distributed.

> ⑩ Whitening: It is better to standardize features along the principal component directions when performing standardization.

⑵ Premises

> ① Represent the basis of the space where the data belongs as e<sub>1</sub>, ···, e<sub>p</sub> (the set of bases does not need to be orthogonal).

>> ○ e<sub>1</sub>, ···, e<sub>p</sub> can be considered as bases representing the given dimensions (no need for orthogonality).

> ② Goal: To find the orthogonal basis Z<sub>1</sub>, ···, Z<sub>k</sub> of such a subspace (k < n) (typical unsupervised problem setting).

>> ○ Z<sub>1</sub>, ···, Z<sub>k</sub> can be considered as bases representing the true dimensionality of the data.

<br>

![image](https://github.com/user-attachments/assets/a040256a-0f86-4033-a62e-5a0b740e92d4)

<br>

> ③ **Random vector**

>> ○ X represents a specific data point: x<sub>1</sub>, ···, x<sub>n</sub> are each component.

<br>

![image](https://github.com/user-attachments/assets/15c6f2f7-84d2-4c1e-980d-6b64528b5949)

<br>

> ④ **Mean vector**: Also called population mean.

<br>

![image](https://github.com/user-attachments/assets/a2a5e5c0-320a-4f1b-a3c0-c2f2d07055ef)

<br>

> ⑤ **Variance-covariance matrix**

<br>

![image](https://github.com/user-attachments/assets/0e9b988b-f0ef-4a41-b8df-e10caf819d54)

<br>

> ⑥ **Unbiased sample variance-covariance matrix**: With the design matrix **X**, and center matrix **X<sub>c</sub>**,

<br>

![image](https://github.com/user-attachments/assets/c937618b-0adc-486c-b16d-4eb67d43f50e)

<br>

>> ○ Used when the population variance-covariance matrix is unknown.

>> ○ Since **X** ∈ ℝ<sup>n×p</sup>, **X**<sup>T</sup>**X** ∈ ℝ<sup>p×n</sup> × ℝ<sup>n×p</sup> = ℝ<sup>p×p</sup>.

> ⑦ Expression of the goal orthogonal basis

<br>

![image](https://github.com/user-attachments/assets/08803bfd-bdfb-43bc-8f23-bed4bbf0e675)

<br>

>> ○ Z<sub>k</sub>: = k-th PC ∈ ℝ<sup>p×1</sup>.

> ⑧ Characteristics of the goal orthogonal basis: Easily derived from linear algebra concepts.

<br>

![image](https://github.com/user-attachments/assets/37c61a4f-5048-4d0b-a22d-073835dcd87a)

<br>

>> ○ Var, Cov, etc., are specific values related to variance, belonging to ℝ<sup>1x1</sup>.

>> ○ Do not confuse with the variance-covariance matrix.

⑶ Definition of the PCA problem

<br>

![image](https://github.com/user-attachments/assets/724c5796-2854-43e5-833b-158fed21bd29)

<br>

⑷ Solution to the PCA problem

> ① **Method 1.** Eigen-decomposition of ∑.

>> ○ **Step 1:** Data Construction and Centering

>>> ○ **X** ∈ ℝ<sup>n×d</sup>: A data matrix with _n_ samples and _d_ features.

>>> ○ Center the data matrix **X**: **X**<sub>c</sub> = **X** - **μ** with **μ** ∈ ℝ<sup>d</sup> is the vector of column-wise means)

>> ○ **Step 2:** Covariance Matrix Calculation

>>> ○ Compute the sample covariance matrix **C** ∈ ℝ<sup>d×d</sup> from the centered data: 

<br>

<img width="99" alt="스크린샷 2024-11-16 오전 10 28 08" src="https://github.com/user-attachments/assets/ef233ea5-b06a-480b-8c9c-7738b79e98b5">

<br>

>>> ○ Since the given sample represents the entire population, the population mean is known, so the sample covariance is calculated by dividing by n instead of n−1.

>> ○ **Step 3:** Eigen-decomposition

>>> ○ Perform eigen-decomposition of the covariance matrix _C_ to compute eigenvalues and eigenvectors: **C** · **p**<sub>i</sub> = λ<sub>i</sub> · **p**<sub>i</sub>

>>> ○ Here, λ<sub>i</sub> represents the eigenvalue and **p**<sub>i</sub> ​∈ R<sup>d</sup> is the eigenvector.

>>> ○ Note: The symmetric matrix C can always be orthogonally diagonalized (by the [spectral theorem](https://jb243.github.io/pages/1928)).

>> ○ **Step 4:** Data Transformation

>>> ○ Define **P** = [**p**<sub>1</sub>, **p**<sub>2</sub>, …, **p**<sub>d</sub>] ∈ R<sup>d×d</sup>, a matrix where the eigenvectors are stored as columns.

>>> ○ Transform the data into a new coordinate system: **Y** = **X**<sub>c</sub>**P** where **Y** ∈ R<sup>n×d</sup> is the transformed data matrix.

>> ○ **Step 5:** Diagonalization of the Covariance Matrix

>>> ○ The covariance matrix of the transformed data **Y** becomes diagonal:

<br>

<img width="445" alt="스크린샷 2024-11-16 오전 10 32 40" src="https://github.com/user-attachments/assets/b6254144-078c-45ac-a26a-2f7999eeb9e8">

<br>

>> ○ **Step 6:** Principal Component Selection and Dimensionality Reduction

>>> ○ Select _k_ principal components (eigenvectors) corresponding to the largest eigenvalues λ<sub>i</sub> and reduce the dimensionality: **Y**<sub>k</sub> = **X**<sub>c</sub>**P**<sub>k</sub> with **P**<sub>k</sub> ∈ ℝ<sup>d×k</sup>, **Y**<sub>k</sub> ∈ ℝ<sup>n×k</sup> 

>>> ○ Alternatively, instead of fixing the number of principal components, eigenvalues smaller than a specific threshold η can be excluded.

>> ○ **Conclusion 1.** The d-th principal component PC<sub>d</sub> is the eigenvector corresponding to the d-th largest eigenvalue.

>> ○ **Conclusion 2.** The meaning of eigenvectors with the same eigenvalue: The magnitude of the corresponding principal component is the same.

> ② **Method 2.** Singular value decomposition (SVD) of the center matrix **X**<sub>c</sub>: The standard method currently used in PCA algorithms.

>> ○ **Step 1:** Center the data to obtain the centered data **X**<sub>c</sub>.

>> ○ **Step 2:** Define ​**C** = (1/n) **X**<sub>c</sub><sup>**T**</sup>**X**<sub>c</sub> = ∑ (covariance matrix)
​
>> ○ **Step 3:** Since **C** is symmetric, it can be orthogonally diagonalized as **C** = **VDV**<sup>T</sup> (where **V** is the same as the **V** in **X**<sub>c</sub> = **U**∑**V**<sup>T</sup>)

>> ○ **Step 4.** The 1<sup>st</sup>, 2<sup>nd</sup>, ··· th column vectors of **V** are the eigenvectors corresponding to the 1<sup>st</sup>, 2<sup>nd</sup>, ··· th largest eigenvalues and are the principal components.

> ③ **Method 3.** [Lagrange multiplier under equality constraint](https://jb243.github.io/pages/1813).

>> ○ **Step 1.** The first principal component is obtained by solving for **w** that maximizes Var(**X**<sub>c</sub>**w**) subject to the constraint ㅣ**w**ㅣ = 1. Applying the Lagrange multiplier method leads to a condition of the form C**w** = λ**w**, which reduces to an eigenvalue problem.

>> ○ **Step 2.** The second and third principal components are then found sequentially in the same way, with additional constraints that they are orthogonal (or uncorrelated) to the previously found components.

 ⑸ Determining the number of axes

> ① **Method 1.** Scree plot for the fraction of total variance.

>> ○ Definition of the fraction of total variance.

<br>

![image](https://github.com/user-attachments/assets/13b00d84-8318-4d3c-955c-c3ac61886aab)

<br>

>> ○ Graphical representation.

<br>

![image](https://github.com/user-attachments/assets/da97e824-00d1-4e02-a058-4d54b40e2d7f)

**Figure 1.** Example of axis setting.

<br>

![image](https://github.com/user-attachments/assets/888e17f2-c14b-4b9f-964d-97e2001fdb2c)

**Figure 2.** Scree plot: Shows the amount of variance explained by each principal component.

<br>

> ② **Method 2.** Scree plot for eigenvalues.

> ③ **Method 3.** Use in-sample 10-fold cross-validation to determine the number of axes p that minimizes MSPE.

<br>

![image](https://github.com/user-attachments/assets/d9cf92b1-9211-45bd-a984-c8d3c55a966b)

**Figure 3.** Determination of the number of axes.

<br>

> ④ Since information about the remaining axes is lost, PCA functions as a lossy compression algorithm.

⑹ Python code example

<br>

```python
import numpy as np
import matplotlib as mpl
mpl.use('Agg')
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')
from ipywidgets import interact
%matplotlib inline
from sklearn.datasets import fetch_openml

def normalize(X):
    mu = np.mean(X, axis=0)
    std = np.std(X, axis=0)
    std_filled = std.copy()
    std_filled[std==0] = 1.
    Xbar = ((X-mu)/std_filled)
    return Xbar, mu, std

def eig(S):
    eigvals, eigvecs = np.linalg.eig(S)
    k = np.argsort(eigvals)[::-1]
    return eigvals[k], eigvecs[:,k]

def projection_matrix(B):
    return (B @ np.linalg.inv(B.T @ B) @ B.T)

def PCA(X, num_components=2):
    # reconstruction
    S = 1.0/len(X) * np.dot(X.T, X)
    eig_vals, eig_vecs = eig(S)
    eig_vals, eig_vecs = eig_vals[:num_components], eig_vecs[:, :num_components]
    B = np.real(eig_vecs)
    reconst = (projection_matrix(B) @ X.T)
    return reconst.T

def PC_coords(X, num_components=2):   
    # low-dimensional projection
    S = 1.0/len(X) * np.dot(X.T, X)
    eig_vals, eig_vecs = eig(S)
    eig_vals, eig_vecs = eig_vals[:num_components], eig_vecs[:, :num_components]
    B = np.real(eig_vecs)
    Xpca = X@B    
    return Xpca

images, labels = fetch_openml("mnist_784", version=1, return_X_y=True, as_frame=False)
labels=np.stack(labels).astype(None)
X = (images.reshape(-1, 28 * 28)) / 255.
subset_X = X[labels == 3]
Xbar, mu, std = normalize(subset_X)

num_components=2
X = Xbar
S = 1.0/len(X) * np.dot(X.T, X)
eig_vals, eig_vecs = eig(S)
eig_vals, eig_vecs = eig_vals[:num_components], eig_vecs[:, :num_components]
print(eig_vecs)

num_component = 2
reconst = PCA(Xbar, num_component)
reconstructions = reconst * std + mu # "unnormalize" the reconstructed image
```

<br>

⑺ Development of PCA

> ① Kernel PCA: Uses [kernels](https://jb243.github.io/pages/1897). Can either extend the given data to high dimensions or reduce it to lower dimensions through projection.

> ② MDS (Multidimensional Scaling)

> ③ SNE, t-SNE

> ④ UMAP

<br>

<br>

## 3. Type 2. [SNE, symmetric-SNE, tSNE](https://jb243.github.io/pages/1730)

⑴ A dimension reduction algorithm that attempts to preserve local neighborhoods in the data.

⑵ Nonlinear and non-deterministic.

⑶ Requires knowledge of cost function, updating algorithm, etc., related to [deep learning](https://jb243.github.io/pages/1138).

<br>

<br>
 
## 4\. Type 3. [UMAP](https://jb243.github.io/pages/2203)

<br>

<br>

## **5\. Type 4.** Singular Value Decomposition (SVD)

⑴ A method that extracts singular values from an M × N dimensional matrix and reduces the dimensionality of the given data.

⑵ Formula

> ① Summary: If A is an m × n matrix with rank r, then there exist ℝm×m matrix U and ℝn×n matrix V that satisfy the following conditions:

<br>

<img width="393" alt="스크린샷 2024-10-22 오전 11 44 56" src="https://github.com/user-attachments/assets/3f9d04a4-797c-49ec-9152-4c62dfa74cae">

<br>

>> ○ U and V are orthonormal matrices.

>> ○ Since changing the sign of each column of U and the corresponding column of V still satisfies the above equation, the SVD is not uniquely determined.

>> ○ Matrix ∑: An m × n matrix with r nonzero singular values (> 0) on the diagonal.

<br>

<img width="97" alt="스크린샷 2024-10-22 오전 11 34 47" src="https://github.com/user-attachments/assets/69e9e2b5-a161-46b1-b66d-228a7b78d62c">

<br>

> ② Whole process

<br>

![image](https://github.com/user-attachments/assets/4f94b661-115e-4360-9668-cb95dfd3df98)

<br>

> ③ Singular values are defined as the positive square roots of the eigenvalues of AA<sup>T</sup> or A<sup>T</sup>A.

>> ○ AA<sup>T</sup> and A<sup>T</sup>A may differ in the number of zero eigenvalues, but they yield the same non-zero eigenvalues.

>> ○ The number of non-zero singular values is equal to rank(A).

>> ○ The number of zero singular values is max(dim(A)[0], dim(A)[1])−rank(A).

<br>

<img width="402" alt="스크린샷 2024-11-29 오전 12 54 54" src="https://github.com/user-attachments/assets/982f6231-7509-4f4f-bd87-7a60fa55bf04">

<br>

> ④ Python code

<br>

```python
# np.linalg.svd computes the SVD of a matrix
A = np.array([[4,11,14],
            [8,7,-2]])
U, s, Vt=np.linalg.svd(A)
print(U)
print(s) # s is the vector of nonzero singular values
print(Vt)
```

<br>

⑶ **Application 1.** A<sup>T</sup>A = V(∑<sup>T</sup>∑)V<sup>T</sup> : The singular values of A are the square roots of the eigenvalues of A<sup>T</sup>A. However, some eigenvalues are set to 0 according to the rank.

⑷ **Application 2.** reduced SVD: Note that for the reduced version, both U and V have orthonormal columns. This means that U<sup>T</sup>U = I and V<sup>T</sup>V = I. However, U and V are not square in this version, so they are not orthogonal matrices.

<br>

![image](https://github.com/user-attachments/assets/ac360053-c3ee-48e0-aa9c-8df67c0de893)

<br>

```python
A = np.array([[4,11,14],
            [8,7,-2]])
U, s, Vt=np.linalg.svd(A, full_matrices=False)
print(U)
print(s)
print(Vt)
```

<br>

⑸ **Application 3.** rank-k approximation

> ① For k < rank(A), the m × n matrix A can be split into m × k + k × n matrices to achieve compression.

<br>

![image](https://github.com/user-attachments/assets/1c811701-96a9-4742-8fd9-754983d8fe7d)

<br>

> ② Given k, how to find the optimal rank-k approximation of A:

>> ○ A = UΣV<sup>T</sup> = σ<sub>1</sub>**u**<sub>1</sub>v<sub>1</sub><sup>T</sup> + ··· + σ<sub>r</sub>**u**<sub>r</sub>**v**<sub>r</sub><sup>T</sup>, where σ<sub>1</sub> ≥ ⋯ ≥ σ<sub>r</sub>, remove the smaller singular values σ<sub>r</sub>, σ<sub>r−1</sub>, ….

>> ○ By doing this, the Frobenius distance between A<sup>(k)</sup> and A is defined as follows:

<br>

![image](https://github.com/user-attachments/assets/570427b5-9fee-4986-8b08-a38539a9d781)

<br>

⑹ **Application 4.** PCA(principal component analysis)

> ① **Step 1.** Center the data

<br>

<img width="393" alt="스크린샷 2024-11-16 오전 8 45 10" src="https://github.com/user-attachments/assets/fc2361b9-742d-4697-a744-aee1af619126">

**Figure. 4.** Data centering of PCA

<br>

<img width="409" alt="스크린샷 2024-11-16 오전 8 45 28" src="https://github.com/user-attachments/assets/3e222a33-e7ae-4ccc-8560-3692470c786d">

<br>

> ② **Step 2.** **X**<sub>c</sub> is the centered data and **C** = (1/n) **X**<sub>c</sub><sup>T</sup>**X**<sub>c</sub>

> ③ **Step 3.** **C** is symmetric, so it is orthogonally diagonalizable as **C** = **VDV**<sup>T</sup>. **V** is the same as in **X**<sub>c</sub> = **U**∑**V**<sup>T</sup> in SVD. 

> ④ **Step 4.** The 1<sup>st</sup>, 2<sup>nd</sup>, ··· th column vectors of **V** are the eigenvectors corresponding to the 1<sup>st</sup>, 2<sup>nd</sup>, ··· th largest eigenvalues and are the principal components.

> ⑤ **Step 5.** If you want to get the new coordinates of the vectors in X based on just first 2 PCs, you simply project **x**<sub>i</sub> (a vector of length p) onto the first two PCs:

<br>

<img width="401" alt="스크린샷 2024-11-29 오전 12 03 43" src="https://github.com/user-attachments/assets/ad3b43af-cf73-4b8c-913c-3be17ae06063">

<br>

<br>
 
## **6\. Type 5.** Independent Component Analysis (ICA)

⑴ Unlike principal component analysis, it is a dimensionality reduction technique that statistically separates multivariate signals into independent subcomponents.

⑵ For example, a scenario where $N$ microphones distinguish $M$ conversations.

⑶ The distribution of independent components follows a non-Gaussian distribution.

⑷ It utilizes [matrix factorization](https://jb243.github.io/pages/956).

<br>

<br>

## **7\. Type 6. PPCA**

⑴ Formulation

> ① **Step 1.** Assume the observed data x is generated by the latent variable z: The following transformation itself becomes a dimensionality reduction technique.

<br>

<img width="145" alt="스크린샷 2024-11-07 오전 11 56 26" src="https://github.com/user-attachments/assets/f2c679ac-9f18-42c7-8930-3c83555d29c0">

<br>

>> ○ W: Transformation matrix of dimension D × d (for mapping from latent space to observed space)

>> ○ b: Mean vector of dimension D

>> ○ ϵ: Noise, following a normal distribution N(0,σ<sup>2</sup>I)

> ② **Step 2.** Assumption on the latent variable z

<br>

<img width="145" alt="스크린샷 2024-11-07 오전 11 56 18" src="https://github.com/user-attachments/assets/5d674833-40fb-4622-a7e1-aabde181ee86">

<br>

> ③ **Step 3.** Conditional distribution of x given z

<br>

<img width="264" alt="스크린샷 2024-11-07 오전 11 56 49" src="https://github.com/user-attachments/assets/cc266b17-e1e1-4ce6-b810-7fd3138368aa">

<br>

> ④ **Step 4.** Marginal distribution p(x)

<br>

<img width="264" alt="스크린샷 2024-11-07 오전 11 57 08" src="https://github.com/user-attachments/assets/f3a1fd86-f285-43d0-ae14-5f4c4c320074">

<br>

> ⑤ **Step 5.** MLE Estimation

>> ○ Maximum Likelihood Estimation (MLE) is used to estimate b, W, and σ from the given data.

>> ○ The MLE solution is obtained in closed form.

>> ○ When σ → 0, the method converges to standard PCA.

⑵ Code

<br>

```python
import numpy as np
from numpy import shape, isnan, nanmean, average, zeros, log, cov
from numpy import matmul as mm
from numpy.matlib import repmat
from numpy.random import normal
from numpy.linalg import inv, det, eig
from numpy import identity as eye
from numpy import trace as tr
from scipy.linalg import orth

def ppca(Y,d,dia):
    """
    Implements probabilistic PCA for data with missing values,
    using a factorizing distribution over hidden states and hidden observations.

    Args:
        Y:   (N by D ) input numpy ndarray of data vectors
        d:   (  int  ) dimension of latent space
        dia: (boolean) if True: print objective each step

    Returns:
        ss: ( float ) isotropic variance outside subspace
        C:  (D by d ) C*C' + I*ss is covariance model, C has scaled principal directions as cols
        M:  (D by 1 ) data mean
        X:  (N by d ) expected states
        Ye: (N by D ) expected complete observations (differs from Y if data is missing)

        Based on MATLAB code from J.J. VerBeek, 2006. http://lear.inrialpes.fr/~verbeek
    """
    N, D      = shape(Y) #N observations in D dimensions (i.e. D is number of features, N is samples)
    threshold = 1E-4 #minimal relative change in objective function to continue
    hidden    = isnan(Y)
    missing   = hidden.sum()

    if(missing > 0):
        M = nanmean(Y, axis=0)
    else:
        M = average(Y, axis=0)

    Ye = Y - repmat(M,N,1)

    if(missing > 0):
        Ye[hidden] = 0

    #initialize
    C = normal(loc=0.0, scale=1.0, size=(D,d))
    CtC = mm(C.T, C)
    X   = mm(mm(Ye, C), inv(CtC))
    recon = mm(X, C.T)
    recon[hidden] = 0
    ss = np.sum((recon-Ye)**2) / (N*D - missing)

    count = 1
    old = np.inf

    #EM Iterations
    while(count):
        Sx = inv(eye(d) + CtC/ss) #E-step, covariances
        ss_old = ss
        if(missing > 0):
            proj = mm(X,C.T)
            Ye[hidden] = proj[hidden]
        
        X = mm(mm(Ye, C), Sx/ss) #E-step: expected values

        SumXtX = mm(X.T, X) #M-step
        C = mm(mm(mm(Ye.T, X),(SumXtX + N*Sx).T), inv(mm((SumXtX + N*Sx), (SumXtX + N*Sx).T)))
        CtC = mm(C.T, C)
        ss = (np.sum((mm(X, C.T) - Ye)**2) + N*np.sum(CtC*Sx) + missing*ss_old) / (N*D)
        #transform Sx determinant into numpy float128 in order to deal with high dimensionality
        Sx_det = np.min(Sx).astype(np.float64)**shape(Sx)[0] * det(Sx / np.min(Sx))
        objective = N*D + N*(D*log(ss)+tr(Sx)-log(Sx_det)) + tr(SumXtX) - missing*log(ss_old)
        
        rel_ch = np.abs( 1 - objective / old )
        old = objective
        
        count = count + 1
        if( rel_ch < threshold and count > 5 ):
            count = 0
        if(dia == True):
            print('Objective: %.2f, Relative Change %.5f' %(objective, rel_ch))
    
    C = orth(C)
    covM = cov(mm(Ye, C).T)
    vals, vecs = eig( covM )
    ordr = np.argsort(vals)[::-1]
    vals = vals[ordr]
    vecs = vecs[:,ordr]
    
    C = mm(C, vecs)
    X = mm(Ye, C)
    
    #add data mean to expected complete data
    Ye = Ye + repmat(M,N,1)

    return C, ss, M, X, Ye
```

<br>

<br>

## **8\. Type 7. CCA**

⑴ CCA (Canonical Correlation Analysis)

> ① Problem Statement: Given data matrices X ∈ ℝ<sup>n×p</sup> and Y ∈ ℝ<sup>n×q</sup>, the goal is to maximize the correlation coefficient of the transformed variables.  

<br>

<img width="253" alt="스크린샷 2025-01-29 오후 11 17 08" src="https://github.com/user-attachments/assets/e186b29f-22ea-4805-b3c2-c13737edfa7e" />

<br>

>> ○ ∑<sub>xx</sub> = X<sup>T</sup>X: Covariance matrix of X ∈ ℝ<sup>p×p</sup>
   
>> ○ ∑<sub>yy</sub> = Y<sup>T</sup>Y: Covariance matrix of Y ∈ ℝ<sup>q×q</sup> 

>> ○ ∑<sub>xy</sub> = X<sup>T</sup>Y: Cross-covariance matrix between X and Y ∈ ℝ<sup>p×q</sup>
   
>> ○ ∑<sub>yx</sub> = (∑<sub>xy</sub>)<sup>T</sup> 

> ② Solution Approach: Reformulate the problem as an optimization problem under the constraints: w<sub>x</sub><sup>T</sup>∑<sub>xx</sub>w<sub>x</sub> = 1 and w<sub>y</sub><sup>T</sup>∑<sub>yy</sub>w<sub>y</sub> = 1, and apply the [Lagrange multiplier method](https://jb243.github.io/pages/1813).

<br>

<img width="639" alt="스크린샷 2025-01-29 오후 11 21 08" src="https://github.com/user-attachments/assets/300c8ecc-a72f-4782-98e5-dad6481509f7" />

<br>

> ③ Conclusion: The canonical correlation is given by the positive square root of the generalized eigenvalues of the matrix: ∑<sub>xx</sub><sup>-1</sup>∑<sub>xy</sub>∑<sub>yy</sub><sup>-1</sup>∑<sub>yx</sub>.

⑵ DCCA (Diagonal CCA)

> ① Problem Statement: Given data matrices X ∈ ℝ<sup>n×p</sup> and Y ∈ ℝ<sup>n×q</sup>

<br>

<img width="258" alt="스크린샷 2025-01-29 오후 11 23 34" src="https://github.com/user-attachments/assets/f206c8d8-534b-4dc3-a536-e3bd54fbd0e5" />

<br>
  
>> ○ D<sub>x</sub> = diag(∑<sub>xx</sub>): Using only the diagonal elements of the covariance matrix  

>> ○ D<sub>y</sub> = diag(∑<sub>yy</sub>): Using only the diagonal elements of the covariance matrix  

> ② Process: The singular values of [SVD](#5-type-4-singular-value-decomposition-svd) determine the canonical correlation values.  

<br>

<img width="401" alt="스크린샷 2025-01-29 오후 11 25 55" src="https://github.com/user-attachments/assets/8fe785c0-f693-40f7-81bc-ec00738235b8" />

<br>

⑶ Comparison of CCA and DCCA

<br>

| Method      | CCA                                    | DCCA                                 |
|-------------|----------------------------------------|---------------------------------------|
| Covariance Matrix | Full matrix used (\( \Sigma_{xx}, \Sigma_{yy} \)) | Diagonal elements only (\( D_x, D_y \)) |
| Solution    | Generalized eigenvalue problem        | SVD                                  |
| Computational Cost | Requires matrix inversion (high cost) | Diagonal matrix operations (low cost) |
| Result Interpretation | Eigenvectors, eigenvalues               | Singular vectors, singular values     |

**Table 1.** Comparison of CCA and DCCA

<br>

<br>
 
## **9\. Others**

⑴ Factor analysis

> ① Definition: A method of interpreting the structure within the data by grouping similar variables based on their correlations.

> ② Assumes the existence of latent variables that cannot be observed in the data.

> ③ Often used in social sciences or surveys.

⑵ Multi-dimensional scaling (MDS)

> ① Definition: A statistical technique for visualizing proximity and grouping of objects in a lower-dimensional space by identifying patterns and structures latent in the data.

> ② Measures the similarities and dissimilarities between objects and represents them as points in a 2D or 3D space.

⑶ Linear discriminant analysis (LDA)

> ① Definition: A method that reduces dimensionality by projecting data onto a lower-dimensional space, similar to PCA.

⑷ RPCA (Reciprocal principal component analysis)

⑸ [Autoencoder](https://jb243.github.io/pages/956) 

> ① [Variational Autoencoder (VAE)](https://jb243.github.io/pages/956#2-type-1-variational-autoencoder)

> ② [Graph Autoencoder (GAE)](https://jb243.github.io/pages/956#3-type-2-graph-autoencoder)

> ③ [Matrix Factorization and NMF](https://jb243.github.io/pages/956#4-type-3-matrix-factorization) 

> ④ [Latent Topic Model](https://jb243.github.io/pages/956#5-type-4-latent-topic-model): Latent Semantic Indexing (LSI), Latent Dirichlet Allocation (LDA), etc.

⑹ Signal Pattern Analysis (SPA) ([ref1](https://www.nature.com/articles/s43588-024-00734-0), [ref2](https://www.perplexity.ai/search/ieee-reviews-on-graph-signal-p-woIqh1tHR7u9oYzt5ODDmw#0))

> ① Instead of the traditional Laplacian matrix used in [spectral clustering](https://jb243.github.io/pages/616), SPA employs the power of the Markov transition matrix P<sup>t</sup>, derived from the graph, to capture information at various diffusion times t.

> ② Shorter diffusion times t are sensitive to local patterns, while longer times reflect more global structures, leveraging this property for analysis.

⑺ PHATE

⑻ ODA

⑼ LLE

⑽ LSA

⑾ [PLSA](https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis) (probabilistic latent semantic analysis)

> ① Used to decompose mixed components from the latent class model.

> ② Application : Used for dimension reduction in mass spectrometry data.

<br>

---

_Input: 2021.12.3 16:22_
