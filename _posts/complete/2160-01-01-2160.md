## **Chapter 23. MAB** (multi-armed bandits)

Recommended Post: „ÄêAlgorithm„Äë [Algorithm Table of Contents](https://jb243.github.io/pages/1278)

---

**1.** [Overview](#1-overview)

**2.** [UCB](#2-ucb)

**3.** [Thompson Sampling](#3-thompson-sampling)

**4.** [Early Stopping Problem](#4-early-stopping-problem)

---

<br>

## **1\. Overview**

‚ë¥ Overview

> ‚ë† Problem Definition: Selecting the optimal arm with the highest payoff

> ‚ë° Particularly necessary in the following situations

>> ‚óã When it is difficult to calculate the objective function (black-box algorithm)

>> ‚óã When there is no explicit analytic form

>> ‚óã When singular values exist

>> ‚óã When the function is not differentiable

>> ‚óã When the data is highly noisy

‚ëµ Trade-off of **two strategies**

> ‚ë† **Exploitation :** Maximizing reward using the empirical mean derived from current data

> ‚ë° **Exploration :** Improving the empirical mean to match the true mean

> ‚ë¢ If exploration is neglected when initial data deviates from the true distribution, incorrect judgments cannot be corrected

‚ë∂ **Type 1.** **Bayesian Optimization**

> ‚ë† [Bayes' theorem](https://jb243.github.io/pages/1623#:-,,-\(Bayestheroem\))

<br>

<img width="303" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2024-11-21 ·Ñã·Ö©·Ñí·ÖÆ 9 54 54" src="https://github.com/user-attachments/assets/609dbe5a-886f-4253-9ae2-47cddc97f47f">

<br>

>> ‚óã Prior distribution (surrogate model) is given for the reward distribution of each arm

>> ‚óã Gaussian Process is often used as the prior model

>> ‚óã Posterior distribution is obtained every time a reward is observed

> ‚ë° Acquisition Function: Sampling is performed to maximize the next reward

<br>

<img width="299" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2024-11-21 ·Ñã·Ö©·Ñí·ÖÆ 9 55 12" src="https://github.com/user-attachments/assets/e1c71643-1509-44af-809f-eeeadb5040a9">

<br>

>> ‚óã All acquisition functions calculate the mean Œº(x) and variance œÉ2(x) based on the posterior distribution, designing exploration strategies through this

>> ‚óã **Type 1.** EI (expected improvement): The second formula for EI(x) holds only when using a Gaussian Process

>>> ‚óã Note, Œ¶ and ùúô are the CDF and PDF of the standard normal distribution, and Z standardizes x

>>> ‚óã Higher Œª leads to more exploration

<br>

<img width="550" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2024-11-21 ·Ñã·Ö©·Ñí·ÖÆ 9 55 34" src="https://github.com/user-attachments/assets/d9bbf263-beb1-41e7-b78a-42e0dff9630b">

<br>

>> ‚óã **Type 2.** [MPI](https://en.wikipedia.org/wiki/Bayesian_optimization) (maximum probability of improvement)

<br>

<img width="277" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2024-11-26 ·Ñã·Ö©·Ñí·ÖÆ 1 41 21" src="https://github.com/user-attachments/assets/bedbeb8c-ac94-423e-bb44-d425bd9a524d">

<br>

>> ‚óã**Type 3.** [UCB](https://jb243.github.io/pages/2160)

<br>

<img width="332" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2024-11-26 ·Ñã·Ö©·Ñí·ÖÆ 1 41 44" src="https://github.com/user-attachments/assets/7f430921-3fa4-4277-a0cf-cd2f92c5ec56">

<br>

> ‚ë¢ Gaussian process: Assuming normal distribution during Bayesian optimization

<br>

<img width="496" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2024-11-21 ·Ñã·Ö©·Ñí·ÖÆ 9 55 53" src="https://github.com/user-attachments/assets/99009a47-3644-4d92-ae4a-545977802f9d">

<br>

```python
x <- c(-3, -1, 0, 2, 3)
f <- data.frame(x=x, y=cos(x))
x <- f$x
k_xx <- varCov(x,x)
k_xxs <- varCov(x, x_star)
k_xsx <- varCov(x_star, x)
k_xsxs <- varCov(x_star, x_star)

f_star_bar <- k_xsx %*% solve(k_xx)%*%f$y              # Mean
cov_f_star <- k_xsxs - k_xsx %*% solve(k_xx) %*% k_xxs # Var

plot_ly(type='scatter', mode="lines") %>%
  add_trace(x=x_star, y=f_star_bar, name="Function Estimate") %>%
  add_trace(x=x_star, y=f_star_bar-2*sqrt(diag(cov_f_star)), 
              name="Lower Confidence Band (Function Estimate)",
            line = list(width = 1, dash = 'dash')) %>%
  add_trace(x=x_star, y=f_star_bar+2*sqrt(diag(cov_f_star)), 
              name="Upper Confidence Band (Function Estimate)",
            line = list(width = 1, dash = 'dash')) %>%
  add_markers(x=f$x, y=f$y, name="Obs. Data", marker=list(size=20)) %>%
  add_trace(x=x_star, y=cos(x_star),line=list(width=5), name="True f()=cos(x)") %>%
  layout(title="Gaussian Process Simulation (5 observations)", legend = list(orientation='h'))
```

<br>

> ‚ë£ Hyperparameter Optimization

>> ‚óã Explore hyperparameters that maximize marginal likelihood, the probability that data X and y occur, using Maximum Likelihood Estimation (MLE)

>> ‚óã Marginal likelihood and Gaussian Process hyperparameters are defined as follows

<br>

<img width="602" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2024-11-21 ·Ñã·Ö©·Ñí·ÖÆ 9 57 39" src="https://github.com/user-attachments/assets/88190619-192d-4b20-b939-78f055e87461">

<br>

>> ‚óã Advantages: Improves underfitting and overfitting

> ‚ë§ References

>> ‚óã Osband, Russo, and Van Roy 2013

>> ‚óã Russo and Van Roy 2014, 2015, 2016

>> ‚óã Bubeck and Liu 2013

>> ‚óã [Data Science and Predictive Analytics](https://socr.umich.edu/DSPA2/DSPA2_notes/13_FunctionOptimization.html) (UMich HS650) (PI: Ivo Dinov)

‚ë∑ **Type 2.** Stochastic Multi-armed Bandit

> ‚ë† **Feature 1.** Online Decision: Select the it-th arm among N arms at each time t = 1, ¬∑¬∑¬∑, T

> ‚ë° **Feature 2.** Stochastic Feedback: The rewards provided by each arm follow a fixed but unknown distribution

> ‚ë¢ **Feature 3.** Bandit Feedback: At each time step, only the reward for the chosen arm is visible

> ‚ë£ Objective: Maximize cumulative expected rewards over T steps

<br>

<img width="295" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2024-11-21 ·Ñã·Ö©·Ñí·ÖÆ 9 58 06" src="https://github.com/user-attachments/assets/a684d9f4-8b4b-4fa4-b891-15b64dabdf66">

<br>

> ‚ë§ Maximizing rewards is equivalent to minimizing regret = Œº* - Œº<sub>t</sub>

<br>

<img width="415" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2024-11-21 ·Ñã·Ö©·Ñí·ÖÆ 9 58 40" src="https://github.com/user-attachments/assets/bb655e97-3a65-484e-8ba7-2eb4e1ba84bb">

<br>

> ‚ë• Bandit: Implies taking away when failed, like a robber

> ‚ë¶ Example: Thompson Sampling

>> ‚óã Along with UCB, one of the most widely used bandit algorithms

<br>

<br>

## **2\. UCB** 

‚ë¥ Along with Thompson Sampling, one of the most widely used bandit algorithms

‚ëµ Formulation

> ‚ë† Empirical mean for arm i during time t

<br>

<img width="177" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2024-11-21 ·Ñã·Ö©·Ñí·ÖÆ 9 58 56" src="https://github.com/user-attachments/assets/d05c8ae2-0732-43a3-b5ae-695d8e1bdb85">

<br>

>> ‚óã Numerator: Total reward obtained from arm i

>> ‚óã I{is = i}: Outputs 1 only when is = i, otherwise 0

>> ‚óã ni,t: Number of times i was chosen until time t

> ‚ë° UCB (upper confidence bound)

<br>

<img width="449" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2024-11-21 ·Ñã·Ö©·Ñí·ÖÆ 9 59 26" src="https://github.com/user-attachments/assets/07d13030-a008-47f5-b071-ab7d4bb63996">

<br>

>> ‚óã **Optimism :** Meaning to consider it higher than the actual expected value with high probability

>> ‚óã High probability: Over 99.9%

> ‚ë¢ Choose the arm with the maximum UCB at each step

<br>

<img width="158" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2024-11-21 ·Ñã·Ö©·Ñí·ÖÆ 9 59 44" src="https://github.com/user-attachments/assets/0ed3683f-9bde-48ab-a5c2-a063705199a6">

<br>

> ‚ë£ Update empirical mean and UCB at every time step

>> ‚óã Uncertainty represented by UCB decreases with each observation

‚ë∂ Proof of Convergence

> ‚ë† Definition of Regret: For arbitrary time T,

<br>

<img width="466" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2024-11-21 ·Ñã·Ö©·Ñí·ÖÆ 10 00 00" src="https://github.com/user-attachments/assets/6dcafeb9-1b6f-4e9b-be34-19260640f479">

<br>

> ‚ë° Convergence: Mathematically proven rigorously

<br>

<img width="300" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2024-11-21 ·Ñã·Ö©·Ñí·ÖÆ 10 00 17" src="https://github.com/user-attachments/assets/1a881646-2f4d-4c74-8bac-dd3dc92bf433">

<br>

> ‚ë¢ Conclusion

>> ‚óã Per time regret converges to 0: Meaning every choice becomes the best choice

>> ‚óã Using only empirical mean does not guarantee optimal arm selection

>> ‚óã May result in never attempting the optimal arm

‚ë∑ Comparison of UCB and Thompson Sampling

> ‚ë† **Commonality 1. **Total regret is O(log T): Per time regret converges to 0 as O(log T / T)

> ‚ë° **Difference 1.** Thompson Sampling often outperforms UCB: UCB is a slightly more conservative algorithm, converging slower

> ‚ë¢ **Difference 2.** UCB is deterministic while Thompson Sampling is probabilistic (randomized)

<br>

<br>

## **3\. Thompson Sampling**

‚ë¥ Overview

> ‚ë† Oldest bandit algorithm: Introduced by Thompson in 1933

> ‚ë° Natural and efficient heuristic algorithm

‚ëµ Process

> ‚ë† Maintain belief for parameters of each arm (e.g., mean reward)

>> ‚óã Use prior distributions such as [Beta distribution](https://jb243.github.io/pages/1627) or [Gaussian distribution](https://jb243.github.io/pages/1627)

> ‚ë° Extract estimated reward from each prior distribution

>> ‚óã Simple sampling, not extracting expected value

>> ‚óã Higher variance for fewer observations allows easier extraction

> ‚ë¢ Choose the arm with the maximum estimated reward and observe its posterior reward

>> ‚óã Posterior distribution must always be maintained by the researcher

> ‚ë£ Update prior belief with the posterior using Bayesian approach

> ‚ë§ Works well even with two modes

‚ë∂ **Example:** Thompson Sampling using Beta Distribution

> ‚ë† Start with Beta(1, 1) as the prior belief for all arms

> ‚ë° For each time t,

>> ‚óã Prior: Beta(Œ±, Œ≤)

>> ‚óã Independently sample Œ∏i,t for each arm

>> ‚óã Choose it = argmaxi Œ∏i,t

>> ‚óã Beta(Œ±+1, Œ≤): When 1 is observed

>> ‚óã Beta(Œ±, Œ≤+1): When 0 is observed

‚ë∑ **Example:** Thompson Sampling using Gaussian Distribution

> ‚ë† Start with N(0, ŒΩ2) as the prior belief for all arms

> ‚ë° For each time t,

>> ‚óã Prior: N(0, ŒΩ2)

>> ‚óã Independently sample Œ∏i,t for each arm

>> ‚óã Choose it = argmaxi Œ∏i,t

>> ‚óã Update the empirical mean ¬µÀÜ (posterior) for the chosen arm: Where n is the number of independent observations

<br>

<img width="114" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2024-11-21 ·Ñã·Ö©·Ñí·ÖÆ 10 00 35" src="https://github.com/user-attachments/assets/6ae24436-a6c0-4a1e-8795-0d40078cf299">

<br>

‚ë∏ Convergence

> ‚ë† After a certain number of steps, well-separation between two arms eventually creates the best arm

<br>

<img width="114" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2024-11-21 ·Ñã·Ö©·Ñí·ÖÆ 10 00 47" src="https://github.com/user-attachments/assets/f865cb8d-68ee-4b10-a20f-a61438b80f8b">

<br>

> ‚ë° Beta Distribution

<br>

<img width="312" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2024-11-21 ·Ñã·Ö©·Ñí·ÖÆ 10 01 07" src="https://github.com/user-attachments/assets/cee6cd7c-4bf3-48e1-a461-be7fec84baa5">

<br>

> ‚ë¢ Gaussian Distribution: Based on Azuma-Hoeffding inequality

<br>

<img width="177" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2024-11-21 ·Ñã·Ö©·Ñí·ÖÆ 10 01 21" src="https://github.com/user-attachments/assets/f269b2a6-213d-4642-adec-e49b9d8bd14c">

<br>

‚ëπ Comparison of UCB and Thompson Sampling

> ‚ë† **Commonality 1.** Total regret is O(log T): Per time regret converges to 0 as O(log T / T)

> ‚ë° **Difference 1.** Thompson Sampling often outperforms UCB: UCB is a slightly more conservative algorithm, converging slower

> ‚ë¢ **Difference 2.** UCB is deterministic while Thompson Sampling is probabilistic (randomized)

<br>

<br>

## **4\. Early Stopping Problem**

‚ë¥ Secretary Problem

> `The secretary problem is a fascinating puzzle that gained popularity among mathematicians in the 1940s and 50s. Here's the situation: you need to hire one secretary, and there are N candidates lined up. You interview them one by one and must decide immediately whether to accept or reject each person. Once rejected, a candidate cannot be reconsidered, and once someone is hired, you can't see any of the remaining applicants. So, as the hiring manager, what‚Äôs the best strategy to select the most qualified candidate? Surprisingly, there‚Äôs a clear answer. Assuming the number of applicants is large enough, the optimal approach is to automatically reject roughly the first 37% of candidates. For example, if there are 100 applicants, you reject the first 36. Then, among the remaining candidates, you hire the first person who‚Äôs better than anyone you‚Äôve seen so far. This strategy gives you about a 36.78% chance of picking the very best candidate ‚Äî the probability converges to 1/e. A beautiful bit of math magic.`

‚ëµ [Early stopping (optimal stopping) problem with Bayesian optimization not applied](https://jb243.github.io/pages/384)

<br>

---

_Input: 2021.12.10 00:52_

_Edited: 2024.11.21 15:30_
