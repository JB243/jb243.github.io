## **Chapter 11. Reinforcement Learning**

Recommended Reading: „ÄêAlgorithm„Äë [Algorithm Index](https://jb243.github.io/pages/1278)

---

**1.** [Overview](#1-overview)

**2.** [Markov Chain](#2-markov-chain)

**3.** [Markov Decision Process](#3-markov-decision-process)

**4.** [General Decision Process](#4-general-decision-process)

**5.** [Advanced Topics](#5-advanced-topics)

---

**a.** [Reinforcement Learning Problem Sets](https://jb243.github.io/pages/1130) 

---

I wrote this based on my experience taking UMich ECE567.

---

<br>

## **1\. Overview**

 ‚ë¥ Definition

> ‚ë† Supervised Learning

>> ‚óã Data: (x, y) (where x is a feature, y is a label)

>> ‚óã Goal: Compute the mapping function x ‚Üí y

> ‚ë° Unsupervised Learning

>> ‚óã Data: x (where x is a feature and there is no label)

>> ‚óã Goal: Learn the underlying structure of x

> ‚ë¢ Reinforcement Learning

>> ‚óã Data: (s, a, r, s') (where s is state, a is action, r is reward, s' is the next state)

>> ‚óã Goal: Maximize the total reward over multiple time steps

‚ëµ [Stochastic Control Theory](https://jb243.github.io/pages/895)

> ‚ë† **Element 1.** State

> ‚ë° **Element 2.** Reward

>> ‚óã Definition: The change in the state.

>> ‚óã **Value function:** The expected value of future rewards, expressed as lifetime value (VLT).

>>> ‚óã Formulation: For a state s, a policy œÄ, and a discount factor Œ≥ that adjusts future value to present value.

<br>

<img width="295" height="66" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2025-09-26 ·Ñã·Ö©·Ñí·ÖÆ 12 53 43" src="https://github.com/user-attachments/assets/ddb55021-a199-4bcc-ac27-f5864c114304" />

<br>

>>> ‚óã Provides the basis for choosing an action by evaluating whether a state is good or bad.

> ‚ë¢ **Element 3.** Action

> ‚ë£ **Element 4.** Policy

>> ‚óã Definition: The agent‚Äôs behavior; a mapping that takes a state as input and outputs an action.

>> ‚óã Decision process: A general framework for decision-making problems in which states, actions, and rewards unfold over a process.

>> ‚óã **Type 1:** Deterministic policy

>> ‚óã **Type 2:** Stochastic policy

>>> ‚óã **Reason 1:** In learning, the optimal behavior is unknown, so exploration is needed.

>>> ‚óã **Reason 2:** The optimal situation itself may be stochastic (e.g., rock‚Äìpaper‚Äìscissors, or when an opponent exploits determinism).

> ‚ë§ **Element 5.** Model

>> ‚óã Definition: The behavior/dynamics of the environment.

>> ‚óã Given a state and an action, the model determines the next state and the reward.

>> ‚óã Note the distinction between model-free and model-based methods.

‚ë∂ **Characteristics**: Different from supervised learning and unsupervised learning

> ‚ë† Implicitly receives correct answers: Provided in the form of rewards

> ‚ë° Needs to consider interaction with the environment: Delayed feedback can be an issue

> ‚ë¢ Previous decisions influence future interactions

> ‚ë£ Actively gathers information: Reinforcement learning includes the process of obtaining data

<br>

<br>

## **2\. Markov Chain** 

‚ë¥ Overview

> ‚ë† Definition: A system where the future state depends only on the current state and not on past states

<br>

<img width="577" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2025-02-25 ·Ñã·Ö©·Ñí·ÖÆ 10 30 26" src="https://github.com/user-attachments/assets/08615c69-63f0-4680-806d-ddd8a159569c" />

<br>

> ‚ë° A Markov chain refers to a Markov process whose state space is finite or countably infinite.

> ‚ë¢ **Lemma 1.** [Chapman-Kolmogorov decomposition](https://jb243.github.io/pages/895) 

> ‚ë£ **Lemma 2.** [Linear state-space model](https://jb243.github.io/pages/895) 

‚ëµ [Graph theory](https://jb243.github.io/pages/616) 

> ‚ë† Strongly Connected (= Irreducible): A state where any node i in the graph can reach any other node j

> ‚ë° Period: The greatest common divisor of all paths returning to node i

>> ‚óã Example: If two nodes A and B are connected as A=B with two edges, the period of each node is 2

> ‚ë¢ Aperiodic: When all nodes have a period of 1

>> ‚óã Aperiodic ‚äÇ Irreducible

>> ‚óã Example: If each node has a walk returning to itself, it is aperiodic

> ‚ë£ Stationary State: If $Pr(x_n \mid x_{n-1})$ is independent of $n$, the Markov process is stationary (time-invariant)

> ‚ë§ Regular

>> ‚óã Regular ‚äÇ Irreducible

>> ‚óã If there exists a natural number k such that all elements of the k-th power of the transition matrix M^k are positive (i.e., nonzero)

> ‚ë• **Lemma 1.** [Perron-Frobenius theorem](https://jb243.github.io/pages/895) 

> ‚ë¶ **Lemma 2.** [Lyapunov equation](https://jb243.github.io/pages/895) 

> ‚ëß **Lemma 3.** [Bellman equation](https://jb243.github.io/pages/895) 

 ‚ëµ **Type 1.** Two-State Markov Chain

<br>

<img width="388" height="202" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-01-23 ·Ñã·Ö©·Ñí·ÖÆ 5 23 16" src="https://github.com/user-attachments/assets/21757695-5270-46f5-8123-32e843f715c8" />

**Figure 1.** Two-Scale Markov Chain

<br>

> ‚ë† M: Transformation causing state transition in one step

> ‚ë° M<sup>n</sup>: Transformation causing state transition in n steps

> ‚ë¢ Steady-State Vector: A vector q satisfying Mq = q, i.e., an eigenvector with eigenvalue 1

‚ë∂ **Type 2.** HMM (Hidden Markov Model)

> ‚ë† œá = {X<sub>i</sub>} is a Markov process and Y<sub>i</sub> = œï(X<sub>i</sub>) (where œï is a deterministic function), then y = {Y<sub>i</sub>} is a Hidden Markov Model.

> ‚ë° **Baum-Welch Algorithm**

>> ‚óã Purpose: Learning HMM parameters

>> ‚óã Input: Observed data

>> ‚óã Output: State transition probabilities and emission probabilities of HMM

>> ‚óã Principle: A type of EM (Expectation Maximization) algorithm

>> ‚óã Formula

>>> ‚óã A<sub>kl</sub>: Number of transition from state k to l

<br>

<img width="376" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2025-03-14 ·Ñã·Ö©·Ñå·Ö•·Ü´ 8 40 21" src="https://github.com/user-attachments/assets/4881807e-d62c-4f7d-a97a-28e0ce8f8175" />

<br>

>>> ‚óã E<sub>k</sub>(b): Number of emissions of observation b from state k

<br>

<img width="338" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2025-03-14 ·Ñã·Ö©·Ñå·Ö•·Ü´ 8 40 38" src="https://github.com/user-attachments/assets/6fd17c4c-3fca-444d-b634-c72f04dbd0b9" />

<br>

>>> ‚óã B<sub>k</sub>: Initial probability for state k

<br>

<img width="256" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2025-03-14 ·Ñã·Ö©·Ñå·Ö•·Ü´ 8 40 55" src="https://github.com/user-attachments/assets/f4e86c0e-25c1-4f99-bc7a-6dd1e08e00c6" />

<br>

> ‚ë¢ **[Viterbi Algorithm](http://www.mcb111.org/w06/durbin_book.pdf)** 

>> ‚óã Purpose: Find the most likely hidden state sequence given an HMM

>> ‚óã Input: HMM parameters and observed data

>>> ‚óã N: Number of possible hidden states

>>> ‚óã T: Length of observed data

>>> ‚óã A: State transition probability, a<sub>kl</sub> = probability of transitioning from state k to state l

>>> ‚óã E: Emission probability, e<sub>k</sub>(x) = probability of observing x in state k

>>> ‚óã B: Initial state probability

>> ‚óã Output: Most probable state sequence

>> ‚óã Principle: Uses dynamic programming to compute the optimal path

<br>

![image](https://github.com/user-attachments/assets/e1fa8f67-d229-41db-bfa1-27320721e6b6)

<br>

>> ‚óã **Step 1.** Initialization

<br>

<img width="229" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2025-02-25 ·Ñã·Ö©·Ñí·ÖÆ 10 32 04" src="https://github.com/user-attachments/assets/7052ecde-8df0-400a-a125-7f4b29469018" />

<br>

>>> ‚óã b<sub>k</sub>: Initial probability of state $k$, $P(s_0 = k)$

>>> ‚óã $e_k(œÉ)$: Probability of observing the first observation œÉ in state $k$, $P(x_0 \mid s_0 = k)$

>> ‚óã **Step 2.** Recursion

>>> ‚óã Compute maximum probability from the previous state at each time step i = 1, ..., T

<br>

<img width="291" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2025-02-25 ·Ñã·Ö©·Ñí·ÖÆ 10 32 46" src="https://github.com/user-attachments/assets/9ef4d062-dddc-4496-ad1d-8dae4c2f05de" />

<br>

>>> ‚óã Compute backpointer (ptr) storing the most probable previous state

<br>

<img width="291" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2025-02-25 ·Ñã·Ö©·Ñí·ÖÆ 10 33 01" src="https://github.com/user-attachments/assets/351dfb0c-7f3d-4949-883f-d16a7f34af21" />

<br>

>>> ‚óã ptr<sub>i</sub>(l) serves to store the previous state k that has the highest probability of transitioning to the current state l.

>> ‚óã **Step 3.** Termination

>>> ‚óã Select the highest probability at the final time step

<br>

<img width="243" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2025-02-25 ·Ñã·Ö©·Ñí·ÖÆ 10 34 15" src="https://github.com/user-attachments/assets/d7211923-2c9d-499a-9917-7b9fee1b34b9" />

<br>

>>> ‚óã Determine the last state of the optimal sequence

<br>

<img width="223" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2025-02-25 ·Ñã·Ö©·Ñí·ÖÆ 10 34 32" src="https://github.com/user-attachments/assets/82218707-8776-4d83-b228-b55b1419a3b6" />

<br>

>>> ‚óã v<sub>k</sub>(i - 1): Optimal probability at previous time step i - 1 in state k

>>> ‚óã a<sub>kl</sub>: Probability of transitioning from state k to l

>> ‚óã **Step 4.** Traceback

>>> ‚óã Trace back through ptr array from i = T, ..., 1 to recover the optimal path

<br>

<img width="154" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2025-02-25 ·Ñã·Ö©·Ñí·ÖÆ 10 34 55" src="https://github.com/user-attachments/assets/516197ed-77f4-461e-98ce-f13fbbaf587f" />

<br>

>> ‚óã Example

<br>

![image](https://blog.kakaocdn.net/dn/nKnMO/btsMvUfkPcP/7YrFhrnIeBARENfxO3x1t1/img.gif)

 **Figure 2.** Example of Viterbi Algorithm

<br>

>> ‚óã Python Code

<br>

```python
class HMM(object):
    def __init__(self, alphabet, hidden_states, A=None, E=None, B=None):
        self._alphabet = set(alphabet)
        self._hidden_states = set(hidden_states)
        self._transitions = A
        self._emissions = E
        self._initial = B
        
    def _emit(self, cur_state, symbol):
        return self._emissions[cur_state][symbol]
    
    def _transition(self, cur_state, next_state):
        return self._transitions[cur_state][next_state]
    
    def _init(self, cur_state):
        return self._initial[cur_state]

    def _states(self):
        for k in self._hidden_states:
            yield k

    def draw(self, filename='hmm'):
        nodes = list(self._hidden_states) + ['Œ≤']

        def get_children(node):
            return self._initial.keys() if node == 'Œ≤' else self._transitions[node].keys()

        def get_edge_label(pred, succ):
            return (self._initial if pred == 'Œ≤' else self._transitions[pred])[succ]
        
        def get_node_shape(node):
            return 'circle' if node == 'Œ≤' else 'box'
            
        def get_node_label(node):
            if node == 'Œ≤':
                return 'Œ≤'
            else:
                return r'\n'.join([node, ''] + [
                    f"{e}: {p}" for e, p in self._emissions[node].items()
                ])

        graphviz(nodes, get_children, filename=filename,
                 get_edge_label=get_edge_label,
                 get_node_label=get_node_label,
                 get_node_shape=get_node_shape,
                 rankdir='LR')
        
    def viterbi(self, sequence):
        trellis = {} 
        traceback = [] 
        for state in self._states():
            trellis[state] = np.log10(self._init(state)) + np.log10(self._emit(state, sequence[0])) 
            
        for t in range(1, len(sequence)):
            trellis_next = {}
            traceback_next = {}

            for next_state in self._states():  
                k={}
                for cur_state in self._states():
                    k[cur_state] = trellis[cur_state] + np.log10(self._transition(cur_state, next_state)) 
                argmaxk = max(k, key=k.get)
                trellis_next[next_state] =  np.log10(self._emit(next_state, sequence[t])) + k[argmaxk] 
                traceback_next[next_state] = argmaxk
                
            trellis = trellis_next
            traceback.append(traceback_next)
            
        max_final_state = max(trellis, key=trellis.get)
        max_final_prob = trellis[max_final_state]
                
        result = [max_final_state]
        for t in reversed(range(len(sequence)-1)):
            result.append(traceback[t][max_final_state])
            max_final_state = traceback[t][max_final_state]
            
        return result[::-1]
```

<br>

> ‚ë£ **Type 1.** PSSM: Simpler HMM structure

> ‚ë§ **Type 2.** Profile HMM: It is advantageous over PSSMs regarding the following: 

>> ‚óã Diagram of profile HMM

<br>

![image](https://github.com/user-attachments/assets/7068b7b0-a363-4a77-941b-6b6ed71322bf)

**Figure 3.** Diagram of profile HMM 

<br>

>>> ‚óã M, I, and D represent match, insertion, and deletion, respectively.

>>> ‚óã M<sub>i</sub> can be transitioned to M<sub>i+1</sub>, I<sub>i</sub>, and D<sub>i+1</sub>.

>>> ‚óã I<sub>i</sub> can be transitioned to M<sub>i+1</sub>, I<sub>i</sub>, and D<sub>i+1</sub>.

>>> ‚óã D<sub>i</sub> can be transitioned to M<sub>i+1</sub>, I<sub>i</sub>, and D<sub>i+1</sub>.

>> ‚óã **Advantage 1.** The ability to model insertions and deletion

>> ‚óã **Advantage 2.** Transitions are restricted only between valid state traversal.

>> ‚óã **Advantage 3.** Boundaries between states are better defined.

‚ë∏ **Type 3.** Markov chain Monte Carlo (MCMC)

> ‚ë† Definition: A method for generating samples from a Markov chain following a complex probability distribution

> ‚ë° **Method 1.** Metropolis-Hastings

>> ‚óã Generate a new candidate sample from the current state ‚Üí Accept or reject the candidate sample ‚Üí If accepted, transition to the new state

> ‚ë¢ **Method 2.** Gibbs Sampling

<br>

<img width="616" height="236" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2025-12-13 ·Ñã·Ö©·Ñå·Ö•·Ü´ 1 36 59" src="https://github.com/user-attachments/assets/c61ca8fe-374f-4831-9de4-76547d9fa460" />

<br>

> ‚ë£ **Method 3.** Importance/Rejection Sampling

> ‚ë§ **Method 4.** Reversible Jump MCMC

>> ‚óã General MCMC methods like **Method 1** and **Method 2** sample from a probability distribution in a fixed-dimensional parameter space

>> ‚óã Reversible Jump MCMC operates in a variable-dimensional parameter space: The number of parameters dynamically changes during sampling

<br>

<br>

## **3\. Markov Decision Process** 

 ‚ë¥ Overview

> ‚ë† Definition: A decision process in which the future depends only on the current state.

> ‚ë° In practice, the vast majority of problem settings can be treated as a Markov decision process (MDP).

> ‚ë¢ Schematic: For transitions, the state at t+1 is determined solely as a function of the state at t.

<br>

<img width="541" height="301" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-01-23 ·Ñã·Ö©·Ñí·ÖÆ 5 25 11" src="https://github.com/user-attachments/assets/98bbcf50-619e-43e1-9c24-0ad37f519350" />

 **Figure 4.** Agent-Environment Interaction in MDP

<br>

>> ‚óã State: $s_t ‚àà S$

>> ‚óã Action: $a_t ‚àà A$

>> ‚óã Reward: $r_t ‚àà R(s_t, a_t)$

>> ‚óã Policy: $a_t ~ œÄ(¬∑ \mid s_t)$

>> ‚óã Transition: $(s_{t+1}, r_{t+1}) ~ P(¬∑ \mid s_t, a_t)$

> ‚ë£ Existence of optimal solution $V(s)$

<br>

<img width="159" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2025-02-25 ·Ñã·Ö©·Ñí·ÖÆ 10 39 24" src="https://github.com/user-attachments/assets/f48b935c-a53b-45a5-8d8b-7b5a71134b81" />

<br>

>> ‚óã **Premise 1.** Markov property

>> ‚óã **Premise 2.** Stationary assumption

>> ‚óã **Premise 3.** No distributional shift

 ‚ëµ **Type 1.** Q-learning (Watkins, 1989)

> ‚ë† Overview 

>> ‚óã Learn the Q-function (action-value function) directly from data.

>> ‚óã Model-free (off-policy), i.e., the transition dynamics $P(x' \mid x, u)$‚Äîis unknown.

>> ‚óã The reward function $R(s, a)$ is known.

>> ‚óã Example of [value iteration](https://jb243.github.io/pages/895#3-laws-of-stochastic-control-theory).

> ‚ë° Formula: Because the next state $j$ is fixed, the transition probability term drops out.

<br>

<img width="494" height="265" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-01-23 ·Ñã·Ö©·Ñí·ÖÆ 2 07 42" src="https://github.com/user-attachments/assets/11dbb75d-d664-4227-8859-5dc1047a649e" />

<br>

>> ‚óã **Step 1.** Initialize estimates for all state/action pairs: QÃÇ(x, u) = 0

>> ‚óã **Step 2.** Take a random action $u$

>> ‚óã **Step 3.** Observe the next state $x'$ and receive $R(x', u)$ from the environment: Note that this is the realized reward, not the expected reward.

>> ‚óã **Step 4.** Update $QÃÇ(x, u)$

> ‚ë¢ Supplements

>> ‚óã Learning the full model requires $\left\|\mathcal{S}\right\|^2\left\|\mathcal{A}\right\|$ memory, but Q-learning only needs $\left\|\mathcal{S}\right\|\times\left\|\mathcal{A}\right\|$ memory.

>> ‚óã It is also referred to as TD (temporal-difference) learning.

>> ‚óã **Proof on convergence of Q-learning:** Pseudo-contraction mapping

<br>

<img width="651" height="713" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-02-05 ·Ñã·Ö©·Ñí·ÖÆ 2 33 05" src="https://github.com/user-attachments/assets/1e1d2f8c-f09b-47b6-a181-3b48c86c4bc2" />

<br>

‚ë∂ **Type 2.** SARSA (State-Action-Reward-State-Action; Œµ-greedy, epsilon greedy) (Rummery & Niranjan, 1994)

> ‚ë† Overview

>> ‚óã Depends on policy (on-policy).

>> ‚óã When there is no data available, it chooses a policy and collects new data through interaction with the environment.

> ‚ë° Formula

>> ‚óã **Step 1.** Initialize estimates for all state/action pairs: QÃÇ(s, a) = 0

>> ‚óã **Step 2.** At each step $k$, with probability 1 - Œµ<sub>k</sub>, take u ‚àà arg max<sub>v</sub> QÃÇ(x, v); with probability Œµ<sub>k</sub>, take a random action

>> ‚óã **Step 3.** Observe the next state $x'$ and receive $R(x', u)$: Note that this is the realized reward, not the expected reward.

>> ‚óã **Step 4.** Update QÃÇ(x, a)

<br>

<img width="506" height="37" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-01-23 ·Ñã·Ö©·Ñí·ÖÆ 2 28 46" src="https://github.com/user-attachments/assets/0860934e-49e4-42a7-8e41-571e2ebd1a76" />

<br>

>> ‚óã **Step 5.** As $k \to \infty$, drive $\varepsilon_k \to 0$; in this case, one can also use a Boltzmann (softmax) distribution with temperature annealing $(T \to 0)$.

<br>

<img width="624" height="212" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-01-23 ·Ñã·Ö©·Ñí·ÖÆ 2 32 05" src="https://github.com/user-attachments/assets/f8f82aac-8a81-4df6-91f5-39bd8aa50a7e" />

<br>

> ‚ë¢ Comparison with Q-learning

>> ‚óã Target policy: the policy to be learned

>> ‚óã Behavior policy: the policy used to generate samples

>> ‚óã Q-learning: target policy = optimal policy; behavioral policy = any policy under which each action is taken infinitely often

>> ‚óã SARSA: target policy = Œµ-greedy policy; behavioral policy = Œµ-greedy policy

>> ‚óã **Supplement 1.** Using an Œµ-greedy behavior policy doesn‚Äôt automatically make it SARSA; what matters is the target policy used in the update.

>> ‚óã **Supplement 2.** ‚ÄúOff-policy‚Äù means that no matter which policy collected the data, the update uses a greedy target (i.e., the max over next actions).

>> ‚óã **Supplement 3.** If the behavior policy is fully greedy (Œµ = 0), then in SARSA $a' = \arg\max_{a} Q(s', a)$, so the targets match and the update equations become essentially the same.

‚ë∑ **Type 3.** Q-learning with Linear Function Approximation

> ‚ë† **Purpose:** The memory space of Q-learning is „Ö£ùíÆ„Ö£ √ó „Ö£ùíú„Ö£, while that of linear function approximation is M ‚â™ „Ö£ùíÆ„Ö£ √ó „Ö£ùíú„Ö£.

> ‚ë° **Formula:** Note that Œ¥<sub>k</sub><sup>2</sup>, which is the squared temporal difference, is always non-negative.

<br>

<img width="602" height="228" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-02-10 ·Ñã·Ö©·Ñå·Ö•·Ü´ 8 56 04" src="https://github.com/user-attachments/assets/66b0ded6-2e47-4982-9da5-5affd3bf43ef" />

<br>

> ‚ë¢ **Significance:** With a fixed policy and on-policy sampling, convergence is guaranteed (in an appropriate sense).

<br>

<img width="429" height="336" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-02-12 ·Ñã·Ö©·Ñå·Ö•·Ü´ 9 26 58" src="https://github.com/user-attachments/assets/e3c8231e-ba3d-4d73-ad6a-62793c00f7cb" />

<br>

> ‚ë£ **Problem:** If those conditions are violated (e.g., off-policy learning or updating with incorrect weighting), the algorithm can diverge.

<br>

<img width="609" height="776" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-02-12 ·Ñã·Ö©·Ñå·Ö•·Ü´ 9 30 47" src="https://github.com/user-attachments/assets/752c83c0-448e-40e8-aa0c-14b85f02c808" />

<br>

‚ë∏ **Type 4.** DQN

> ‚ë† Q-learning with nonlinear function approximation

>> ‚óã Formula: Constructs loss function based on Bellman equation.

<br>

<img width="461" height="134" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-02-15 ·Ñã·Ö©·Ñí·ÖÆ 3 22 08" src="https://github.com/user-attachments/assets/576d25ac-f3f1-44f2-9a9d-aa7aa8bafac8" />

<br>

>> ‚óã **Problem:** Local optima, Unstability during training 

> ‚ë° Experience Replay (Replay Buffer)

>> ‚óã Definition: A replay buffer is a memory that stores the agent's past experiences collected from the environment. A single time-step transition is usually stored as $(s, a, r, s', \text{done})$.

>> ‚óã Motivation: In reinforcement learning, data arrive as a sequence $(y_1,(x_1,u_1)) \rightarrow (y_2,(x_2,u_2)) \rightarrow (y_3,(x_3,u_3)) \rightarrow \cdots$, so samples are highly correlated and the data distribution keeps changing. Deep learning (SGD) is typically much more stable when samples are i.i.d. (independent and identically distributed). With highly correlated data, updates can ‚Äúswing‚Äù in one direction, often leading to divergence or oscillation during training.

>> ‚óã How it's used in training: Instead of updating the network using only the most recent experience, randomly sample a batch of transitions from the buffer and train the network on that mini-batch. This makes the training data closer to i.i.d., which helps improve stability and can reduce the chance of getting stuck in poor local solutions (i.e., improves training stability and robustness).

> ‚ë¢ DQN (Deep Q-Network)

>> ‚óã Definition: DQN implements Q-learning using deep neural networks. It is often implemented as an off-policy method.

>>> ‚óã Online network (Q-network): The network that is currently being trained. With parameters $\theta$, it outputs $Q(s,a;\theta)$. It is used both for action selection (e.g., $\epsilon$-greedy) and is updated continuously via SGD.

>>> ‚óã Target network: A slowly changing copy of the online network. With parameters $\theta^-$, it outputs $Q(s,a;\theta^-)$ and is used to compute the target value $y = r + \alpha \max_{a'} Q(s', a'; \theta^-)$. The target network is updated either by periodically copying the online parameters ($\theta^- \leftarrow \theta$, hard update) or by slowly tracking the online network, which stabilizes the targets.

>> ‚óã Comparison with experience replay: Both aim to stabilize training, but they are different mechanisms.

>>> ‚óã Experience replay: Reduces sample correlation and distribution shift by shuffling data (making it closer to i.i.d.) through random sampling from a replay buffer.

>>> ‚óã DQN (target network mechanism): If the $Q$ used to compute the target $y = r + \alpha \max Q(s', a')$ changes too rapidly, training can diverge. The target network updates slowly, reducing the ‚Äúmoving target‚Äù problem and improving stability.

>> ‚óã SGD detail: To treat $y_k$ as a label, DQN ignores the fact that $y_k$ is actually a function of the network parameters $w$.

<br>

<img width="513" height="128" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-02-15 ·Ñã·Ö©·Ñí·ÖÆ 4 30 28" src="https://github.com/user-attachments/assets/d63df82e-fe6c-48d5-b115-da741fd8dbc5" />

<br>

>> ‚óã Mini-batch SGD: Training is typically performed using mini-batches sampled from the replay buffer.

<br>

<img width="491" height="111" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-02-15 ·Ñã·Ö©·Ñí·ÖÆ 4 34 47" src="https://github.com/user-attachments/assets/72e147d0-8a62-4a17-9848-b559e89f7e38" />

<br>

>> ‚óã Implementation in Pytorch

<br>

<img width="654" height="222" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-02-15 ·Ñã·Ö©·Ñí·ÖÆ 4 47 42" src="https://github.com/user-attachments/assets/70bd436e-79ba-47c6-8dce-ae3e0edbe6a1" />

**Figure 5.** Implementation in Pytorch

<br>

>>> ‚óã Left (alternative form): `forward(state, action) -> q_scalar` with shape `(batch, 1)`. A drawback is that you must evaluate all possible actions separately‚Äîi.e., run multiple forward passes to compute $Q(s,1), Q(s,2), \ldots$ and then compare them.

>>> ‚óã Right (standard DQN form): `forward(state) -> q_values` with shape `(batch, num_actions)`. Since you only need a single forward pass and then take `argmax`, action selection is simple and computationally efficient. Also, because the early hidden layers are shared, the learning signal for one action can change the shared representation and indirectly affect the Q-values of other actions (i.e., *coupling*).

<br>

```python
random.sample(self.replay buffer, batch size)
next state values = target net(next states).max(1)[0]
target values= rewards + gamma √ó next state values.detach()
loss = nn.MSELoss()(state action values, target values)
optimizer.zerograd(); loss.backward(); optimizer.step()
```

<br>

>> ‚óã Applications: Applied to many games (e.g., DQN on Atari 2600 (2013), AlphaGo (2016), etc.).

> ‚ë£ DDQN(double Q-learning)

>> ‚óã Thrun and Schwartz (1993): Q-learning often overestimates the action value (Q-value) under certain states.

<br>

<img width="434" height="201" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-02-15 ·Ñã·Ö©·Ñí·ÖÆ 6 51 48" src="https://github.com/user-attachments/assets/34427d38-2055-4e7f-9f72-152cd40e81cf" />

<br>

>> ‚óã Double Q-learning (van Hasselt, 2010)

>>> ‚óã Definition: Train two Q-functions (with parameters $w$ and $w'$), then take their average to mitigate the overestimation problem. In this setup, the target network is used to select the action, and the online network is used to evaluate its value.

>>> ‚óã Principle: Double estimator is underestimator.

<br>

<img width="516" height="368" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-02-15 ·Ñã·Ö©·Ñí·ÖÆ 7 23 32" src="https://github.com/user-attachments/assets/65d36045-cd08-4a2b-9367-d647a78d8185" />

<br>

>>> ‚óã Formula

<br>

<img width="536" height="251" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-02-15 ·Ñã·Ö©·Ñí·ÖÆ 7 35 46" src="https://github.com/user-attachments/assets/30c3af4c-4493-421e-8b9c-ec813edd07a9" />

<br>

>>> ‚óã Q-learning vs. Double Q-learning (Weng et al., 2020): In Q-learning, the learning rate is $\beta_k = \frac{c}{k}$, whereas in Double Q-learning it is $\beta_k = \frac{2c}{k}$. Under linear function approximation, we can obtain the following. This also implies that Double Q-learning can converge faster because it uses a larger learning rate.

<br>

<img width="571" height="58" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-02-15 ·Ñã·Ö©·Ñí·ÖÆ 7 42 11" src="https://github.com/user-attachments/assets/7a85c1d1-50ea-4426-a8b5-d846e77415a3" />

<br>

>> ‚óã Clipped double Q-learning (Fujimoto, van Hoof, David Meger, 2018)

<br>

<img width="454" height="137" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-02-15 ·Ñã·Ö©·Ñí·ÖÆ 6 51 23" src="https://github.com/user-attachments/assets/fadf4c0e-924b-4ca9-9edc-82f0af7be849" />

<br>

> ‚ë§ Dueling DDQN (Wang et al, 2016)

<br>

<img width="447" height="377" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-02-19 ·Ñã·Ö©·Ñå·Ö•·Ü´ 9 23 51" src="https://github.com/user-attachments/assets/8e7691b4-2e90-4f6c-8f5d-dbda3a5566d1" />

**Figure 6.** Dueling DDQN

<br>

>> ‚óã Definition: For the advantage function $$A(s,a)=Q(s,a)-V(s),$$ we build a model that predicts $$V$$ and $$A$$ separately using $$Q(x,u;w,w_v,w_a)=V(x;w,w_v)+A(x,u;w,w_a),$$ and then combine them afterward.

>> ‚óã Motivation: Except for the optimal action, not every action is equally important. $$A(s,a)$$ can *contrast* actions under the same state $$s$$.

>> ‚óã **Unidentifiability problem:** The decomposition is not unique because $$Q(s,a)=V(s)+A(s,a)=(V(s)+c)+(A(s,a)-c),\quad c\in\mathbb{R},$$ so infinitely many $$ (V,A) $$ pairs yield the same $$Q$$.

>> ‚óã **Solution 1 (Subtract max):** $$Q(x,u;w)=V(x;w)+\big(A(x,u;w)-\max_v A(x,v;w)\big).$$ Under a greedy policy, $$Q(x,a^*)=V(x),$$ so it enforces the advantage to be $$0$$ for the selected optimal action.

>> ‚óã **Solution 2 (Subtract mean):** $$Q(x,u;w)=V(x;w)+\left(A(x,u;w)-\frac{1}{|\mathcal{A}|}\sum_v A(x,v;w)\right).$$

>>> ‚óã Pros: Improves optimization stability; resolves the unidentifiability issue.

>>> ‚óã Cons: With subtract-mean, $$V$$ becomes a different baseline (the average $$Q$$) rather than $$V^*$$, so the semantics of $$V$$ and $$A$$ no longer match the original $$V^*$$ and $$A^*$$ definitions.

> ‚ë• Multi-step target (Sutton & Barto, 1998)

>> ‚óã $$Q(x_k,u_k)=\mathbb{E}[r(x_k,u_k)] + \alpha, r(x_{k+1},\pi^*(x_{k+1}))+\cdots+\alpha^{n-1} r(x_{k+n-1},\pi^*(x_{k+n-1}))+\alpha^n \max_v Q(x_{k+n},v).$$

>> ‚óã SARSA with a multi-step target (sliding-window style): Use the loss function $$\big(r_k^{(n)}+\alpha^n \max_v Q(x_{k+n},v;w)-Q(x_k,u_k;w)\big)^2.$$

> ‚ë¶ Prioritized DDQN (Prioritized replay, Schaul et al., 2016) 

>> ‚óã Definition: Assign weights (priorities) to experiences using the information-theoretic concept of *surprisal*.

>> ‚óã **Step 1:** When a new experience is added, assign it the maximum priority $$z_{\max}$$.

>> ‚óã **Step 2:** Using $$m$$ experiences, compute the sampling probability $$p_k=\frac{z_k}{\sum_i z_i}.$$

>> ‚óã **Step 3:** If experience $$k$$ is sampled, define the update (change) in its weight $$z_k$$ as follows:

<br>

<img width="397" height="172" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-02-19 ·Ñã·Ö©·Ñå·Ö•·Ü´ 9 55 59" src="https://github.com/user-attachments/assets/0ddf3422-0631-4ebd-b01c-833557d8a7b2" />

<br>

> ‚ëß Distributed DQN (Bellemare et al., 2017)

<br>

<img width="655" height="213" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-02-19 ·Ñã·Ö©·Ñå·Ö•·Ü´ 9 58 34" src="https://github.com/user-attachments/assets/3ff52739-9659-4278-b185-a9843eef97df" />

<br>

> ‚ë® Noisy DQN

> ‚ë© Conclusion: Applying all of them will give you good performance

<br>

<img width="452" height="432" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-02-19 ·Ñã·Ö©·Ñå·Ö•·Ü´ 10 03 13" src="https://github.com/user-attachments/assets/c8ef6e3c-822e-4711-896f-8a32283c9f5d" />

**Figure 7.** Rainbow

<br>

‚ëπ **Type 5.** Deep O-network (DON)

‚ë∫ **Type 6.** Actor-critic

> ‚ë† Typical actor-critic

>> ‚óã Definition: Generalized policy iteration. Learning policy œÄ(s) directly

>> ‚óã Motivation

>>> ‚óã In Q-learning, states can be continuous, but actions must be discrete

>>> ‚óã In policy gradient, both states and actions can be continuous

>>> ‚óã Typically DQN is used if there are dozens of action spaces, and policy gradient is used if there are more than that.

>>> ‚óã In DQN, the network takes $$x$$ as input and outputs $$u$$ (i.e., Q-values for each discrete action), whereas in actor‚Äìcritic methods, the critic takes $$x$$ and $$u$$ as inputs and outputs $$Q(x,u)$$ (because the action space is continuous/infinite).

>> ‚óã Components

>>> ‚óã Critic: TD(Œª), double-Q, clipped double-Q

>>> ‚óã Actor: Œµ-greedy based on the current Q-function, policy-gradient

>> ‚óã An example process

>>> ‚óã **Step 1.** Receive frame

>>> ‚óã **Step 2.** Forward propagate to get P(action)

>>> ‚óã **Step 3.** Sample a from P(action)

>>> ‚óã **Step 4.** Play the rest of the game

>>> ‚óã **Step 5.** If the game is won, update in the ‚àáŒ∏ direction

>>> ‚óã **Step 6.** If the game is lost, update in the -‚àáŒ∏ direction

> ‚ë° A3C (Asynchronous Advantage Actor-Critic)

> ‚ë¢ Policy gradient theorem

<br>

<img width="600" height="726" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-02-22 ·Ñã·Ö©·Ñí·ÖÆ 9 37 00" src="https://github.com/user-attachments/assets/0b0b3395-b5d3-4683-acfa-765b9e4aaa96" />

<br>

>> ‚óã $\rho_w$: Discounted state distribution (= geometric distribution, discounted occupancy measure) 

>> ‚óã $\nabla_w \log \pi_w(u \mid x)$: Score function

>> ‚óã Gibbs policy:

<br>

$$
\nabla_w \log \pi_w(u \mid x) = \phi(x,u) - \mathbb{E}_{\pi_w}\left[\phi(x,\cdot)\right]
$$

<br>

>> ‚óã Gaussian policy:

<br>

$$
\nabla_w \log \pi_w(u \mid x) = \frac{(u-\mu(x))\phi(x)}{\sigma^2},
\qquad
\mu(x)=\phi(x)^{\top}w
$$

<br>

> ‚ë£ REINFORCE (Williams, 1988, 1992)

<br>

<img width="401" height="137" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-02-22 ·Ñã·Ö©·Ñí·ÖÆ 9 59 03" src="https://github.com/user-attachments/assets/49758321-163d-415c-9bb0-4232f8374f9c" />

<br>

>> ‚óã Use Monte Carlo methods to estimate the Q-function.

>> ‚óã On-policy.

>> ‚óã Drawback: high variance.

> ‚ë§ Variance Reduction Theorem

>> ‚óã Definition: For a given $F(x)$, a variance reduction method that uses a function $\phi(x)$ satisfying $\mathbb{E}[\phi(x)] = 0$ and having high correlation with $F(x)$, in order to reduce the variance of $\mathrm{Var}(F(x) - \phi(x))$.

>> ‚óã Application in the policy gradient theorem

<br>

<img width="601" height="495" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-02-27 ·Ñã·Ö©·Ñå·Ö•·Ü´ 10 50 21" src="https://github.com/user-attachments/assets/494a7118-028c-4a74-b99b-683396755381" />

<br>

>> ‚óã $G_w(x_k)$ must be action-independent. It may depend on $x_{k-1}$, but if it depends on $x_{k+1}$, it implicitly contains information about $u_k$, which makes it inappropriate.

> ‚ë• NPG (Kakade, 2001)

>> ‚óã Policy gradient theorem is a kind of optimization algorithm: Euclidean. Maximum, minimum in ¬±‚àû without quadratic terms.

<br>

<img width="451" height="96" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-02-27 ·Ñã·Ö©·Ñå·Ö•·Ü´ 11 12 23" src="https://github.com/user-attachments/assets/cb8ff269-81a7-48fe-a1be-ada1860ccb33" />

<br>

>> ‚óã Natural Policy Gradient (NPG)

<br>

<img width="662" height="268" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-02-28 ·Ñã·Ö©·Ñí·ÖÆ 6 29 38" src="https://github.com/user-attachments/assets/ec2f6e7d-f0cc-40ab-bc45-7d8bbe426702" />

<br>

>> ‚óã Convergence of PG (Mei et al., 2020): under the tabular setting where the state and action sets are finite,

<br>

<img width="321" height="54" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-02-28 ·Ñã·Ö©·Ñí·ÖÆ 7 58 02" src="https://github.com/user-attachments/assets/79287e14-a1e5-45a5-a8dd-d102fe084de4" />

<br>

>>> ‚óã $\beta = (1-\alpha)^3 / 8$

>>> ‚óã $S$: the number of states. Since it is typically very large, it becomes an issue in convergence arguments for PG.

>>> ‚óã $\rho$: the discounted state distribution

>>> ‚óã $c$: a constant

>>> ‚óã $d$: the distribution of the initial condition

>> ‚óã Convergence of NPG (Agarwal, 2020): under the tabular setting where the state and action sets are finite,

<br>

<img width="336" height="54" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-02-28 ·Ñã·Ö©·Ñí·ÖÆ 7 59 32" src="https://github.com/user-attachments/assets/0463c89b-c1d0-4458-ade6-d958af08f81a" />

<br>

>>> ‚óã $\beta = (1-\alpha)^2 \log \left\|\mathcal{A}\right\|$

>>> ‚óã $t$: the iteration index (number of iterations)

>> ‚óã NPG + entropy regularization

<br>

<img width="425" height="113" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-02-28 ·Ñã·Ö©·Ñí·ÖÆ 8 00 17" src="https://github.com/user-attachments/assets/f97242b5-9247-4ddc-ad62-5ff749b654d3" />

<br>

>>> ‚óã The policy $\pi$ is usually set as follows:

<br>

<img width="453" height="113" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-02-28 ·Ñã·Ö©·Ñí·ÖÆ 8 00 42" src="https://github.com/user-attachments/assets/bc1c1873-039a-4af5-8e15-874496cf82c4" />

<br>

>>> ‚óã If we set (\beta = (1-\alpha)/\tau), then the above policy becomes **soft policy iteration** ((\simeq) a **softmax policy**).

>>> ‚óã As (\tau \to 0), soft policy iteration reduces back to standard policy iteration.

> ‚ë¶ TRPO(trust region policy optimization)

>> ‚óã Performance difference lemma (Kakade & Langford, 2002)

<br>

<img width="668" height="470" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-02-28 ·Ñã·Ö©·Ñí·ÖÆ 10 59 20" src="https://github.com/user-attachments/assets/494dcaec-f3cf-4501-8505-f7a632b13314" />

<br>

‚ëª **Type 7.** [GA](https://jb243.github.io/pages/1146)(Genetic Algorithm)

<br>

<br>

## **4\. General Decision Process** 

‚ë¥ [MAB](https://jb243.github.io/pages/2160#1-overview)(multi-armed bandit)

‚ëµ [UCB](https://jb243.github.io/pages/2160#2-ucb) 

‚ë∂ [Thompson sampling](https://jb243.github.io/pages/2160#3-thompson-sampling) 

‚ë∑ [Early stopping problem](https://jb243.github.io/pages/2160#4-early-stopping-problem) 
 
<br>

<br>

## 5. Advanced Topics

‚ë¥ Unsupervised learning 

> ‚ë† Benchmarks: [URLB](https://github.com/facebookresearch/controllable_agent) 

> ‚ë° Baselines: [Diversity is all you need](https://arxiv.org/pdf/1802.06070), [Forward Backward Representation](https://arxiv.org/pdf/2103.07945) 

> ‚ë¢ Envs: [Dm_control](https://github.com/google-deepmind/dm_control), Maze, Hopper, Cheetah, Quadruped, Walker

‚ëµ Online goal-conditioned reinforcement learning 

> ‚ë† Benchmarks: [JaxGCRL](https://github.com/MichalBortkiewicz/JaxGCRL) 

> ‚ë° Baselines: [Contrastive Reinforcement Learning](https://arxiv.org/pdf/2206.07568), [SAC](https://arxiv.org/pdf/1801.01290), [PPO](https://arxiv.org/pdf/1707.06347), [TD3](https://arxiv.org/pdf/1802.09477) 

> ‚ë¢ Envs: [Brax](https://github.com/google/brax), Locomotion, Manipulation

‚ë∂ Offline goal-conditioned reinforcement learning 

> ‚ë† Benchmarks: [OGBench](https://github.com/seohongpark/ogbench) 

> ‚ë° Baselines: [CRL](https://arxiv.org/pdf/2206.07568), [HIQL](https://arxiv.org/pdf/2307.11949), [QRL](https://arxiv.org/pdf/2304.01203), [implicit Q/V learning](https://arxiv.org/pdf/2110.06169) 

> ‚ë¢ Envs: Locomotion, Manipulation, powderworld

> ‚ë£ example follow-up: [TMD](https://github.com/vivekmyers/tmd-release), [MQE](https://github.com/WJ2003B/mqe-release) 

‚ë∑ Continual reinforcement learning 

> ‚ë† Benchmarks: [CORA](https://github.com/AGI-Labs/continual_rl) 

> ‚ë° Baselines: [EWC](https://arxiv.org/pdf/1612.00796), [PC](https://arxiv.org/pdf/1805.06370), [CLEAR](https://proceedings.neurips.cc/paper_files/paper/2019/file/fa7cdfad1a5aaf8370ebeda47a1ff1c3-Paper.pdf) 

> ‚ë¢ Envs: Atari, Procgen, Minihack, CHORES, [Nethack](https://arxiv.org/abs/2006.13760) ([codebase](https://github.com/NetHack-LE/nle?tab=readme-ov-file))

> ‚ë£ Envs without benchmark: [AgarCL](https://github.com/machado-research/AgarCL-benchmark), [Jelly Bean World](https://github.com/eaplatanios/jelly-bean-world) 

‚ë∏ Open-ended reinforcement learning 

> ‚ë† Benchmarks: [Craftax baseline](https://github.com/MichaelTMatthews/Craftax_Baselines) 

> ‚ë° Baselines: [PPO](https://arxiv.org/pdf/1707.06347), [ICM](https://pathak22.github.io/noreward-rl/resources/icml17.pdf), [RND](https://arxiv.org/pdf/1810.12894) 

> ‚ë¢ Envs: [Craftax](https://github.com/MichaelTMatthews/Craftax) 

‚ëπ Safe reinforcement learning 

> ‚ë† Benchmarks: [Omnisafe](https://github.com/PKU-Alignment/omnisafe) 

> ‚ë° Baselines: [CPO](https://arxiv.org/pdf/1705.10528), [FOCOPS](https://arxiv.org/abs/2002.06506), [PPO-Lagrangian and TRPO-Lagrangian](https://cdn.openai.com/safexp-short.pdf) 

> ‚ë¢ Envs: [Safety-Gymnasium](https://safety-gymnasium.readthedocs.io/en/latest/introduction/about_safety_gymnasium.html), [Safe-Control-Gym](https://github.com/utiasDSL/safe-control-gym) 

‚ë∫ Multi-agent reinforcement learning 

> ‚ë† Benchmarks: [BenchMARL](https://github.com/facebookresearch/BenchMARL) 

> ‚ë° Baselines: [MAPPO](https://arxiv.org/abs/2103.01955), [IPPO](https://arxiv.org/abs/2011.09533) 

> ‚ë¢ Envs: [PettingZoo](https://github.com/Farama-Foundation/PettingZoo/tree/master), [VMAS](https://github.com/proroklab/VectorizedMultiAgentSimulator) 

‚ëª Queueing network control via reinforcement learning 

> ‚ë† Benchmarks: [QGym](https://arxiv.org/html/2410.06170v1) 

> ‚ë° Envs: [DiffDiscreteEventSystem](https://github.com/namkoong-lab/QGym)

<br>

---

_Input: 2021.12.13 15:20_

_Updated: 2024.10.08 22:43_
