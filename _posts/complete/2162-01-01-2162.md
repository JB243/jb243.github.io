## **Chapter 11. Reinforcement Learning**

Recommended Reading: 【Algorithm】 [Algorithm Index](https://jb243.github.io/pages/1278)

---

**1.** [Overview](#1-overview)

**2.** [Markov Chain](#2-markov-chain)

**3.** [Markov Decision Process](#3-markov-decision-process)

**4.** [General Decision Process](#4-general-decision-process)

---

**a.** [Stochastic Control Theory](https://jb243.github.io/pages/895) 

---

<br>

## **1\. Overview**

 ⑴ Definition

> ① Supervised Learning

>> ○ Data: (x, y) (where x is a feature, y is a label)

>> ○ Goal: Compute the mapping function x → y

> ② Unsupervised Learning

>> ○ Data: x (where x is a feature and there is no label)

>> ○ Goal: Learn the underlying structure of x

> ③ Reinforcement Learning

>> ○ Data: (s, a, r, s') (where s is state, a is action, r is reward, s' is the next state)

>> ○ Goal: Maximize the total reward over multiple time steps

⑵ [Stochastic Control Theory](https://jb243.github.io/pages/895)

> ① **Element 1.** State

> ② **Element 2.** Reward

>> ○ Definition: The change in the state.

>> ○ **Value function:** The expected value of future rewards, expressed as lifetime value (VLT).

>>> ○ Formulation: For a state s, a policy π, and a discount factor γ that adjusts future value to present value.

<br>

<img width="295" height="66" alt="스크린샷 2025-09-26 오후 12 53 43" src="https://github.com/user-attachments/assets/ddb55021-a199-4bcc-ac27-f5864c114304" />

<br>

>>> ○ Provides the basis for choosing an action by evaluating whether a state is good or bad.

> ③ **Element 3.** Action

> ④ **Element 4.** Policy

>> ○ Definition: The agent’s behavior; a mapping that takes a state as input and outputs an action.

>> ○ Decision process: A general framework for decision-making problems in which states, actions, and rewards unfold over a process.

>> ○ **Type 1:** Deterministic policy

>> ○ **Type 2:** Stochastic policy

>>> ○ **Reason 1:** In learning, the optimal behavior is unknown, so exploration is needed.

>>> ○ **Reason 2:** The optimal situation itself may be stochastic (e.g., rock–paper–scissors, or when an opponent exploits determinism).

> ⑤ **Element 5.** Model

>> ○ Definition: The behavior/dynamics of the environment.

>> ○ Given a state and an action, the model determines the next state and the reward.

>> ○ Note the distinction between model-free and model-based methods.

⑶ **Characteristics**: Different from supervised learning and unsupervised learning

> ① Implicitly receives correct answers: Provided in the form of rewards

> ② Needs to consider interaction with the environment: Delayed feedback can be an issue

> ③ Previous decisions influence future interactions

> ④ Actively gathers information: Reinforcement learning includes the process of obtaining data

<br>

<br>

## **2\. Markov Chain** 

⑴ Overview

> ① Definition: A system where the future state depends only on the current state and not on past states

<br>

<img width="577" alt="스크린샷 2025-02-25 오후 10 30 26" src="https://github.com/user-attachments/assets/08615c69-63f0-4680-806d-ddd8a159569c" />

<br>

> ② A Markov chain refers to a Markov process whose state space is finite or countably infinite.

> ③ **Lemma 1.** [Chapman-Kolmogorov decomposition](https://jb243.github.io/pages/895) 

> ④ **Lemma 2.** [Linear state-space model](https://jb243.github.io/pages/895) 

⑵ [Graph theory](https://jb243.github.io/pages/616) 

> ① Strongly Connected (= Irreducible): A state where any node i in the graph can reach any other node j

> ② Period: The greatest common divisor of all paths returning to node i

>> ○ Example: If two nodes A and B are connected as A=B with two edges, the period of each node is 2

> ③ Aperiodic: When all nodes have a period of 1

>> ○ Aperiodic ⊂ Irreducible

>> ○ Example: If each node has a walk returning to itself, it is aperiodic

> ④ Stationary State: If $Pr(x_n \mid x_{n-1})$ is independent of $n$, the Markov process is stationary (time-invariant)

> ⑤ Regular

>> ○ Regular ⊂ Irreducible

>> ○ If there exists a natural number k such that all elements of the k-th power of the transition matrix M^k are positive (i.e., nonzero)

> ⑥ **Lemma 1.** [Perron-Frobenius theorem](https://jb243.github.io/pages/895) 

> ⑦ **Lemma 2.** [Lyapunov equation](https://jb243.github.io/pages/895) 

> ⑧ **Lemma 3.** [Bellman equation](https://jb243.github.io/pages/895) 

 ⑵ **Type 1.** Two-State Markov Chain

<br>

<img width="388" height="202" alt="스크린샷 2026-01-23 오후 5 23 16" src="https://github.com/user-attachments/assets/21757695-5270-46f5-8123-32e843f715c8" />

**Figure 1.** Two-Scale Markov Chain

<br>

> ① M: Transformation causing state transition in one step

> ② M<sup>n</sup>: Transformation causing state transition in n steps

> ③ Steady-State Vector: A vector q satisfying Mq = q, i.e., an eigenvector with eigenvalue 1

⑶ **Type 2.** HMM (Hidden Markov Model)

> ① χ = {X<sub>i</sub>} is a Markov process and Y<sub>i</sub> = ϕ(X<sub>i</sub>) (where ϕ is a deterministic function), then y = {Y<sub>i</sub>} is a Hidden Markov Model.

> ② **Baum-Welch Algorithm**

>> ○ Purpose: Learning HMM parameters

>> ○ Input: Observed data

>> ○ Output: State transition probabilities and emission probabilities of HMM

>> ○ Principle: A type of EM (Expectation Maximization) algorithm

>> ○ Formula

>>> ○ A<sub>kl</sub>: Number of transition from state k to l

<br>

<img width="376" alt="스크린샷 2025-03-14 오전 8 40 21" src="https://github.com/user-attachments/assets/4881807e-d62c-4f7d-a97a-28e0ce8f8175" />

<br>

>>> ○ E<sub>k</sub>(b): Number of emissions of observation b from state k

<br>

<img width="338" alt="스크린샷 2025-03-14 오전 8 40 38" src="https://github.com/user-attachments/assets/6fd17c4c-3fca-444d-b634-c72f04dbd0b9" />

<br>

>>> ○ B<sub>k</sub>: Initial probability for state k

<br>

<img width="256" alt="스크린샷 2025-03-14 오전 8 40 55" src="https://github.com/user-attachments/assets/f4e86c0e-25c1-4f99-bc7a-6dd1e08e00c6" />

<br>

> ③ **[Viterbi Algorithm](http://www.mcb111.org/w06/durbin_book.pdf)** 

>> ○ Purpose: Find the most likely hidden state sequence given an HMM

>> ○ Input: HMM parameters and observed data

>>> ○ N: Number of possible hidden states

>>> ○ T: Length of observed data

>>> ○ A: State transition probability, a<sub>kl</sub> = probability of transitioning from state k to state l

>>> ○ E: Emission probability, e<sub>k</sub>(x) = probability of observing x in state k

>>> ○ B: Initial state probability

>> ○ Output: Most probable state sequence

>> ○ Principle: Uses dynamic programming to compute the optimal path

<br>

![image](https://github.com/user-attachments/assets/e1fa8f67-d229-41db-bfa1-27320721e6b6)

<br>

>> ○ **Step 1.** Initialization

<br>

<img width="229" alt="스크린샷 2025-02-25 오후 10 32 04" src="https://github.com/user-attachments/assets/7052ecde-8df0-400a-a125-7f4b29469018" />

<br>

>>> ○ b<sub>k</sub>: Initial probability of state $k$, $P(s_0 = k)$

>>> ○ $e_k(σ)$: Probability of observing the first observation σ in state $k$, $P(x_0 \mid s_0 = k)$

>> ○ **Step 2.** Recursion

>>> ○ Compute maximum probability from the previous state at each time step i = 1, ..., T

<br>

<img width="291" alt="스크린샷 2025-02-25 오후 10 32 46" src="https://github.com/user-attachments/assets/9ef4d062-dddc-4496-ad1d-8dae4c2f05de" />

<br>

>>> ○ Compute backpointer (ptr) storing the most probable previous state

<br>

<img width="291" alt="스크린샷 2025-02-25 오후 10 33 01" src="https://github.com/user-attachments/assets/351dfb0c-7f3d-4949-883f-d16a7f34af21" />

<br>

>>> ○ ptr<sub>i</sub>(l) serves to store the previous state k that has the highest probability of transitioning to the current state l.

>> ○ **Step 3.** Termination

>>> ○ Select the highest probability at the final time step

<br>

<img width="243" alt="스크린샷 2025-02-25 오후 10 34 15" src="https://github.com/user-attachments/assets/d7211923-2c9d-499a-9917-7b9fee1b34b9" />

<br>

>>> ○ Determine the last state of the optimal sequence

<br>

<img width="223" alt="스크린샷 2025-02-25 오후 10 34 32" src="https://github.com/user-attachments/assets/82218707-8776-4d83-b228-b55b1419a3b6" />

<br>

>>> ○ v<sub>k</sub>(i - 1): Optimal probability at previous time step i - 1 in state k

>>> ○ a<sub>kl</sub>: Probability of transitioning from state k to l

>> ○ **Step 4.** Traceback

>>> ○ Trace back through ptr array from i = T, ..., 1 to recover the optimal path

<br>

<img width="154" alt="스크린샷 2025-02-25 오후 10 34 55" src="https://github.com/user-attachments/assets/516197ed-77f4-461e-98ce-f13fbbaf587f" />

<br>

>> ○ Example

<br>

![image](https://blog.kakaocdn.net/dn/nKnMO/btsMvUfkPcP/7YrFhrnIeBARENfxO3x1t1/img.gif)

 **Figure 2.** Example of Viterbi Algorithm

<br>

>> ○ Python Code

<br>

```python
class HMM(object):
    def __init__(self, alphabet, hidden_states, A=None, E=None, B=None):
        self._alphabet = set(alphabet)
        self._hidden_states = set(hidden_states)
        self._transitions = A
        self._emissions = E
        self._initial = B
        
    def _emit(self, cur_state, symbol):
        return self._emissions[cur_state][symbol]
    
    def _transition(self, cur_state, next_state):
        return self._transitions[cur_state][next_state]
    
    def _init(self, cur_state):
        return self._initial[cur_state]

    def _states(self):
        for k in self._hidden_states:
            yield k

    def draw(self, filename='hmm'):
        nodes = list(self._hidden_states) + ['β']

        def get_children(node):
            return self._initial.keys() if node == 'β' else self._transitions[node].keys()

        def get_edge_label(pred, succ):
            return (self._initial if pred == 'β' else self._transitions[pred])[succ]
        
        def get_node_shape(node):
            return 'circle' if node == 'β' else 'box'
            
        def get_node_label(node):
            if node == 'β':
                return 'β'
            else:
                return r'\n'.join([node, ''] + [
                    f"{e}: {p}" for e, p in self._emissions[node].items()
                ])

        graphviz(nodes, get_children, filename=filename,
                 get_edge_label=get_edge_label,
                 get_node_label=get_node_label,
                 get_node_shape=get_node_shape,
                 rankdir='LR')
        
    def viterbi(self, sequence):
        trellis = {} 
        traceback = [] 
        for state in self._states():
            trellis[state] = np.log10(self._init(state)) + np.log10(self._emit(state, sequence[0])) 
            
        for t in range(1, len(sequence)):
            trellis_next = {}
            traceback_next = {}

            for next_state in self._states():  
                k={}
                for cur_state in self._states():
                    k[cur_state] = trellis[cur_state] + np.log10(self._transition(cur_state, next_state)) 
                argmaxk = max(k, key=k.get)
                trellis_next[next_state] =  np.log10(self._emit(next_state, sequence[t])) + k[argmaxk] 
                traceback_next[next_state] = argmaxk
                
            trellis = trellis_next
            traceback.append(traceback_next)
            
        max_final_state = max(trellis, key=trellis.get)
        max_final_prob = trellis[max_final_state]
                
        result = [max_final_state]
        for t in reversed(range(len(sequence)-1)):
            result.append(traceback[t][max_final_state])
            max_final_state = traceback[t][max_final_state]
            
        return result[::-1]
```

<br>

> ④ **Type 1.** PSSM: Simpler HMM structure

> ⑤ **Type 2.** Profile HMM: It is advantageous over PSSMs regarding the following: 

>> ○ Diagram of profile HMM

<br>

![image](https://github.com/user-attachments/assets/7068b7b0-a363-4a77-941b-6b6ed71322bf)

**Figure 3.** Diagram of profile HMM 

<br>

>>> ○ M, I, and D represent match, insertion, and deletion, respectively.

>>> ○ M<sub>i</sub> can be transitioned to M<sub>i+1</sub>, I<sub>i</sub>, and D<sub>i+1</sub>.

>>> ○ I<sub>i</sub> can be transitioned to M<sub>i+1</sub>, I<sub>i</sub>, and D<sub>i+1</sub>.

>>> ○ D<sub>i</sub> can be transitioned to M<sub>i+1</sub>, I<sub>i</sub>, and D<sub>i+1</sub>.

>> ○ **Advantage 1.** The ability to model insertions and deletion

>> ○ **Advantage 2.** Transitions are restricted only between valid state traversal.

>> ○ **Advantage 3.** Boundaries between states are better defined.

⑸ **Type 3.** Markov chain Monte Carlo (MCMC)

> ① Definition: A method for generating samples from a Markov chain following a complex probability distribution

> ② **Method 1.** Metropolis-Hastings

>> ○ Generate a new candidate sample from the current state → Accept or reject the candidate sample → If accepted, transition to the new state

> ③ **Method 2.** Gibbs Sampling

<br>

<img width="616" height="236" alt="스크린샷 2025-12-13 오전 1 36 59" src="https://github.com/user-attachments/assets/c61ca8fe-374f-4831-9de4-76547d9fa460" />

<br>

> ④ **Method 3.** Importance/Rejection Sampling

> ⑤ **Method 4.** Reversible Jump MCMC

>> ○ General MCMC methods like **Method 1** and **Method 2** sample from a probability distribution in a fixed-dimensional parameter space

>> ○ Reversible Jump MCMC operates in a variable-dimensional parameter space: The number of parameters dynamically changes during sampling

<br>

<br>

## **3\. Markov Decision Process** 

 ⑴ Overview

> ① Definition: A decision process in which the future depends only on the current state.

> ② In practice, the vast majority of problem settings can be treated as a Markov decision process (MDP).

> ③ Schematic: For transitions, the state at t+1 is determined solely as a function of the state at t.

<br>

<img width="541" height="301" alt="스크린샷 2026-01-23 오후 5 25 11" src="https://github.com/user-attachments/assets/98bbcf50-619e-43e1-9c24-0ad37f519350" />

 **Figure 4.** Agent-Environment Interaction in MDP

<br>

>> ○ State: $s_t ∈ S$

>> ○ Action: $a_t ∈ A$

>> ○ Reward: $r_t ∈ R(s_t, a_t)$

>> ○ Policy: $a_t ~ π(· \mid s_t)$

>> ○ Transition: $(s_{t+1}, r_{t+1}) ~ P(· \mid s_t, a_t)$

> ④ Existence of optimal solution $V(s)$

<br>

<img width="159" alt="스크린샷 2025-02-25 오후 10 39 24" src="https://github.com/user-attachments/assets/f48b935c-a53b-45a5-8d8b-7b5a71134b81" />

<br>

>> ○ **Premise 1.** Markov property

>> ○ **Premise 2.** Stationary assumption

>> ○ **Premise 3.** No distributional shift

 ⑵ **Type 1.** Q-learning (Watkins, 1989)

> ① Overview 

>> ○ Learn the Q-function (action-value function) directly from data.

>> ○ Model-free (off-policy), i.e., the transition dynamics $P(x' \mid x, u)$—is unknown.

>> ○ The reward function $R(s, a)$ is known.

>> ○ Example of [value iteration](https://jb243.github.io/pages/895#3-laws-of-stochastic-control-theory).

> ② Formula: Because the next state $j$ is fixed, the transition probability term drops out.

<br>

<img width="494" height="265" alt="스크린샷 2026-01-23 오후 2 07 42" src="https://github.com/user-attachments/assets/11dbb75d-d664-4227-8859-5dc1047a649e" />

<br>

>> ○ **Step 1.** Initialize estimates for all state/action pairs: Q̂(x, u) = 0

>> ○ **Step 2.** Take a random action $u$

>> ○ **Step 3.** Observe the next state $x'$ and receive $R(x', u)$ from the environment: Note that this is the realized reward, not the expected reward.

>> ○ **Step 4.** Update $Q̂(x, u)$

> ③ Supplements

>> ○ Learning the full model requires $\left\|\mathcal{S}\right\|^2\left\|\mathcal{A}\right\|$ memory, but Q-learning only needs $\left\|\mathcal{S}\right\|\times\left\|\mathcal{A}\right\|$ memory.

>> ○ It is also referred to as TD (temporal-difference) learning.

>> ○ **Proof on convergence of Q-learning:** Pseudo-contraction mapping

<br>

<img width="652" height="712" alt="스크린샷 2026-02-04 오후 1 53 30" src="https://github.com/user-attachments/assets/2075116e-2053-4181-96ab-ca6684a0a95c" />

<br>

⑶ **Type 2.** SARSA (State-Action-Reward-State-Action; ε-greedy, epsilon greedy) (Rummery & Niranjan, 1994)

> ① Overview

>> ○ Depends on policy (on-policy).

>> ○ When there is no data available, it chooses a policy and collects new data through interaction with the environment.

> ② Formula

>> ○ **Step 1.** Initialize estimates for all state/action pairs: Q̂(s, a) = 0

>> ○ **Step 2.** At each step $k$, with probability 1 - ε<sub>k</sub>, take u ∈ arg max<sub>v</sub> Q̂(x, v); with probability ε<sub>k</sub>, take a random action

>> ○ **Step 3.** Observe the next state $x'$ and receive $R(x', u)$: Note that this is the realized reward, not the expected reward.

>> ○ **Step 4.** Update Q̂(x, a)

<br>

<img width="506" height="37" alt="스크린샷 2026-01-23 오후 2 28 46" src="https://github.com/user-attachments/assets/0860934e-49e4-42a7-8e41-571e2ebd1a76" />

<br>

>> ○ **Step 5.** As $k \to \infty$, drive $\varepsilon_k \to 0$; in this case, one can also use a Boltzmann (softmax) distribution with temperature annealing $(T \to 0)$.

<br>

<img width="624" height="212" alt="스크린샷 2026-01-23 오후 2 32 05" src="https://github.com/user-attachments/assets/f8f82aac-8a81-4df6-91f5-39bd8aa50a7e" />

<br>

> ③ Comparison with Q-learning

>> ○ Target policy: the policy to be learned

>> ○ Behavior policy: the policy used to generate samples

>> ○ Q-learning: target policy = optimal policy; behavioral policy = any policy under which each action is taken infinitely often

>> ○ SARSA: target policy = ε-greedy policy; behavioral policy = ε-greedy policy

>> ○ **Supplement 1.** Using an ε-greedy behavior policy doesn’t automatically make it SARSA; what matters is the target policy used in the update.

>> ○ **Supplement 2.** “Off-policy” means that no matter which policy collected the data, the update uses a greedy target (i.e., the max over next actions).

>> ○ **Supplement 3.** If the behavior policy is fully greedy (ε = 0), then in SARSA $a' = \arg\max_{a} Q(s', a)$, so the targets match and the update equations become essentially the same.

⑷ **Type 3.** DQN (Deep Q-Network)

> ① Definition: Implementation of Q-learning using deep learning

> ② Applied to many games: DQN on Atari 2600 (2013), AlphaGo (2016), etc.

⑸ **Type 4.** Policy Gradient

> ① Definition: Learning policy π(s) directly

> ② Characteristics

>> ○ In Q-learning, states can be continuous, but actions must be discrete

>> ○ In policy gradient, both states and actions can be continuous

> ③ Algorithm

>> ○ **Step 1.** Receive frame

>> ○ **Step 2.** Forward propagate to get P(action)

>> ○ **Step 3.** Sample a from P(action)

>> ○ **Step 4.** Play the rest of the game

>> ○ **Step 5.** If the game is won, update in the ∇θ direction

>> ○ **Step 6.** If the game is lost, update in the -∇θ direction

⑸ Other Types

> ① Deep O-Network (DON)

> ② A3C (Asynchronous Advantage Actor-Critic)

> ③ [GA](https://jb243.github.io/pages/1146)(Genetic Algorithm)

<br>

<br>

## **4\. General Decision Process** 

⑴ [MAB](https://jb243.github.io/pages/2160#1-overview)(multi-armed bandit)

⑵ [UCB](https://jb243.github.io/pages/2160#2-ucb) 

⑶ [Thompson sampling](https://jb243.github.io/pages/2160#3-thompson-sampling) 

⑷ [Early stopping problem](https://jb243.github.io/pages/2160#4-early-stopping-problem) 
 
<br>

---

_Input: 2021.12.13 15:20_

_Updated: 2024.10.08 22:43_
