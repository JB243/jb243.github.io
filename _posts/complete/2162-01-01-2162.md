## **Chapter 11. Reinforcement Learning**

Recommended Reading: „ÄêAlgorithm„Äë [Algorithm Index](https://jb243.github.io/pages/1278)

---

**1.** [Overview](#1-overview)

**2.** [Markov Chain](#2-markov-chain)

**3.** [Markov Decision Process](#3-markov-decision-process)

**4.** [General Decision Process](#4-general-decision-process)

---

**a.** [Stochastic Control Theory](https://jb243.github.io/pages/895) 

---

<br>

## **1\. Overview**

 ‚ë¥ Definition

> ‚ë† Supervised Learning

>> ‚óã Data: (x, y) (where x is a feature, y is a label)

>> ‚óã Goal: Compute the mapping function x ‚Üí y

> ‚ë° Unsupervised Learning

>> ‚óã Data: x (where x is a feature and there is no label)

>> ‚óã Goal: Learn the underlying structure of x

> ‚ë¢ Reinforcement Learning

>> ‚óã Data: (s, a, r, s') (where s is state, a is action, r is reward, s' is the next state)

>> ‚óã Goal: Maximize the total reward over multiple time steps

‚ëµ [Stochastic Control Theory](https://jb243.github.io/pages/895)

> ‚ë† **Element 1.** State

> ‚ë° **Element 2.** Reward

>> ‚óã Definition: The change in the state.

>> ‚óã **Value function:** The expected value of future rewards, expressed as lifetime value (VLT).

>>> ‚óã Formulation: For a state s, a policy œÄ, and a discount factor Œ≥ that adjusts future value to present value.

<br>

<img width="295" height="66" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2025-09-26 ·Ñã·Ö©·Ñí·ÖÆ 12 53 43" src="https://github.com/user-attachments/assets/ddb55021-a199-4bcc-ac27-f5864c114304" />

<br>

>>> ‚óã Provides the basis for choosing an action by evaluating whether a state is good or bad.

> ‚ë¢ **Element 3.** Action

> ‚ë£ **Element 4.** Policy

>> ‚óã Definition: The agent‚Äôs behavior; a mapping that takes a state as input and outputs an action.

>> ‚óã Decision process: A general framework for decision-making problems in which states, actions, and rewards unfold over a process.

>> ‚óã **Type 1:** Deterministic policy

>> ‚óã **Type 2:** Stochastic policy

>>> ‚óã **Reason 1:** In learning, the optimal behavior is unknown, so exploration is needed.

>>> ‚óã **Reason 2:** The optimal situation itself may be stochastic (e.g., rock‚Äìpaper‚Äìscissors, or when an opponent exploits determinism).

> ‚ë§ **Element 5.** Model

>> ‚óã Definition: The behavior/dynamics of the environment.

>> ‚óã Given a state and an action, the model determines the next state and the reward.

>> ‚óã Note the distinction between model-free and model-based methods.

‚ë∂ **Characteristics**: Different from supervised learning and unsupervised learning

> ‚ë† Implicitly receives correct answers: Provided in the form of rewards

> ‚ë° Needs to consider interaction with the environment: Delayed feedback can be an issue

> ‚ë¢ Previous decisions influence future interactions

> ‚ë£ Actively gathers information: Reinforcement learning includes the process of obtaining data

<br>

<br>

## **2\. Markov Chain** 

‚ë¥ Overview

> ‚ë† Definition: A system where the future state depends only on the current state and not on past states

<br>

<img width="577" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2025-02-25 ·Ñã·Ö©·Ñí·ÖÆ 10 30 26" src="https://github.com/user-attachments/assets/08615c69-63f0-4680-806d-ddd8a159569c" />

<br>

> ‚ë° A Markov chain refers to a Markov process whose state space is finite or countably infinite.

> ‚ë¢ **Lemma 1.** [Chapman-Kolmogorov decomposition](https://jb243.github.io/pages/895) 

> ‚ë£ **Lemma 2.** [Linear state-space model](https://jb243.github.io/pages/895) 

‚ëµ [Graph theory](https://jb243.github.io/pages/616) 

> ‚ë† Strongly Connected (= Irreducible): A state where any node i in the graph can reach any other node j

> ‚ë° Period: The greatest common divisor of all paths returning to node i

>> ‚óã Example: If two nodes A and B are connected as A=B with two edges, the period of each node is 2

> ‚ë¢ Aperiodic: When all nodes have a period of 1

>> ‚óã Aperiodic ‚äÇ Irreducible

>> ‚óã Example: If each node has a walk returning to itself, it is aperiodic

> ‚ë£ Stationary State: If $Pr(x_n \mid x_{n-1})$ is independent of $n$, the Markov process is stationary (time-invariant)

> ‚ë§ Regular

>> ‚óã Regular ‚äÇ Irreducible

>> ‚óã If there exists a natural number k such that all elements of the k-th power of the transition matrix M^k are positive (i.e., nonzero)

> ‚ë• **Lemma 1.** [Perron-Frobenius theorem](https://jb243.github.io/pages/895) 

> ‚ë¶ **Lemma 2.** [Lyapunov equation](https://jb243.github.io/pages/895) 

> ‚ëß **Lemma 3.** [Bellman equation](https://jb243.github.io/pages/895) 

 ‚ëµ **Type 1.** Two-State Markov Chain

<br>

<img width="388" height="202" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-01-23 ·Ñã·Ö©·Ñí·ÖÆ 5 23 16" src="https://github.com/user-attachments/assets/21757695-5270-46f5-8123-32e843f715c8" />

**Figure 1.** Two-Scale Markov Chain

<br>

> ‚ë† M: Transformation causing state transition in one step

> ‚ë° M<sup>n</sup>: Transformation causing state transition in n steps

> ‚ë¢ Steady-State Vector: A vector q satisfying Mq = q, i.e., an eigenvector with eigenvalue 1

‚ë∂ **Type 2.** HMM (Hidden Markov Model)

> ‚ë† œá = {X<sub>i</sub>} is a Markov process and Y<sub>i</sub> = œï(X<sub>i</sub>) (where œï is a deterministic function), then y = {Y<sub>i</sub>} is a Hidden Markov Model.

> ‚ë° **Baum-Welch Algorithm**

>> ‚óã Purpose: Learning HMM parameters

>> ‚óã Input: Observed data

>> ‚óã Output: State transition probabilities and emission probabilities of HMM

>> ‚óã Principle: A type of EM (Expectation Maximization) algorithm

>> ‚óã Formula

>>> ‚óã A<sub>kl</sub>: Number of transition from state k to l

<br>

<img width="376" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2025-03-14 ·Ñã·Ö©·Ñå·Ö•·Ü´ 8 40 21" src="https://github.com/user-attachments/assets/4881807e-d62c-4f7d-a97a-28e0ce8f8175" />

<br>

>>> ‚óã E<sub>k</sub>(b): Number of emissions of observation b from state k

<br>

<img width="338" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2025-03-14 ·Ñã·Ö©·Ñå·Ö•·Ü´ 8 40 38" src="https://github.com/user-attachments/assets/6fd17c4c-3fca-444d-b634-c72f04dbd0b9" />

<br>

>>> ‚óã B<sub>k</sub>: Initial probability for state k

<br>

<img width="256" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2025-03-14 ·Ñã·Ö©·Ñå·Ö•·Ü´ 8 40 55" src="https://github.com/user-attachments/assets/f4e86c0e-25c1-4f99-bc7a-6dd1e08e00c6" />

<br>

> ‚ë¢ **[Viterbi Algorithm](http://www.mcb111.org/w06/durbin_book.pdf)** 

>> ‚óã Purpose: Find the most likely hidden state sequence given an HMM

>> ‚óã Input: HMM parameters and observed data

>>> ‚óã N: Number of possible hidden states

>>> ‚óã T: Length of observed data

>>> ‚óã A: State transition probability, a<sub>kl</sub> = probability of transitioning from state k to state l

>>> ‚óã E: Emission probability, e<sub>k</sub>(x) = probability of observing x in state k

>>> ‚óã B: Initial state probability

>> ‚óã Output: Most probable state sequence

>> ‚óã Principle: Uses dynamic programming to compute the optimal path

<br>

![image](https://github.com/user-attachments/assets/e1fa8f67-d229-41db-bfa1-27320721e6b6)

<br>

>> ‚óã **Step 1.** Initialization

<br>

<img width="229" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2025-02-25 ·Ñã·Ö©·Ñí·ÖÆ 10 32 04" src="https://github.com/user-attachments/assets/7052ecde-8df0-400a-a125-7f4b29469018" />

<br>

>>> ‚óã b<sub>k</sub>: Initial probability of state $k$, $P(s_0 = k)$

>>> ‚óã $e_k(œÉ)$: Probability of observing the first observation œÉ in state $k$, $P(x_0 \mid s_0 = k)$

>> ‚óã **Step 2.** Recursion

>>> ‚óã Compute maximum probability from the previous state at each time step i = 1, ..., T

<br>

<img width="291" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2025-02-25 ·Ñã·Ö©·Ñí·ÖÆ 10 32 46" src="https://github.com/user-attachments/assets/9ef4d062-dddc-4496-ad1d-8dae4c2f05de" />

<br>

>>> ‚óã Compute backpointer (ptr) storing the most probable previous state

<br>

<img width="291" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2025-02-25 ·Ñã·Ö©·Ñí·ÖÆ 10 33 01" src="https://github.com/user-attachments/assets/351dfb0c-7f3d-4949-883f-d16a7f34af21" />

<br>

>>> ‚óã ptr<sub>i</sub>(l) serves to store the previous state k that has the highest probability of transitioning to the current state l.

>> ‚óã **Step 3.** Termination

>>> ‚óã Select the highest probability at the final time step

<br>

<img width="243" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2025-02-25 ·Ñã·Ö©·Ñí·ÖÆ 10 34 15" src="https://github.com/user-attachments/assets/d7211923-2c9d-499a-9917-7b9fee1b34b9" />

<br>

>>> ‚óã Determine the last state of the optimal sequence

<br>

<img width="223" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2025-02-25 ·Ñã·Ö©·Ñí·ÖÆ 10 34 32" src="https://github.com/user-attachments/assets/82218707-8776-4d83-b228-b55b1419a3b6" />

<br>

>>> ‚óã v<sub>k</sub>(i - 1): Optimal probability at previous time step i - 1 in state k

>>> ‚óã a<sub>kl</sub>: Probability of transitioning from state k to l

>> ‚óã **Step 4.** Traceback

>>> ‚óã Trace back through ptr array from i = T, ..., 1 to recover the optimal path

<br>

<img width="154" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2025-02-25 ·Ñã·Ö©·Ñí·ÖÆ 10 34 55" src="https://github.com/user-attachments/assets/516197ed-77f4-461e-98ce-f13fbbaf587f" />

<br>

>> ‚óã Example

<br>

![image](https://blog.kakaocdn.net/dn/nKnMO/btsMvUfkPcP/7YrFhrnIeBARENfxO3x1t1/img.gif)

 **Figure 2.** Example of Viterbi Algorithm

<br>

>> ‚óã Python Code

<br>

```python
class HMM(object):
    def __init__(self, alphabet, hidden_states, A=None, E=None, B=None):
        self._alphabet = set(alphabet)
        self._hidden_states = set(hidden_states)
        self._transitions = A
        self._emissions = E
        self._initial = B
        
    def _emit(self, cur_state, symbol):
        return self._emissions[cur_state][symbol]
    
    def _transition(self, cur_state, next_state):
        return self._transitions[cur_state][next_state]
    
    def _init(self, cur_state):
        return self._initial[cur_state]

    def _states(self):
        for k in self._hidden_states:
            yield k

    def draw(self, filename='hmm'):
        nodes = list(self._hidden_states) + ['Œ≤']

        def get_children(node):
            return self._initial.keys() if node == 'Œ≤' else self._transitions[node].keys()

        def get_edge_label(pred, succ):
            return (self._initial if pred == 'Œ≤' else self._transitions[pred])[succ]
        
        def get_node_shape(node):
            return 'circle' if node == 'Œ≤' else 'box'
            
        def get_node_label(node):
            if node == 'Œ≤':
                return 'Œ≤'
            else:
                return r'\n'.join([node, ''] + [
                    f"{e}: {p}" for e, p in self._emissions[node].items()
                ])

        graphviz(nodes, get_children, filename=filename,
                 get_edge_label=get_edge_label,
                 get_node_label=get_node_label,
                 get_node_shape=get_node_shape,
                 rankdir='LR')
        
    def viterbi(self, sequence):
        trellis = {} 
        traceback = [] 
        for state in self._states():
            trellis[state] = np.log10(self._init(state)) + np.log10(self._emit(state, sequence[0])) 
            
        for t in range(1, len(sequence)):
            trellis_next = {}
            traceback_next = {}

            for next_state in self._states():  
                k={}
                for cur_state in self._states():
                    k[cur_state] = trellis[cur_state] + np.log10(self._transition(cur_state, next_state)) 
                argmaxk = max(k, key=k.get)
                trellis_next[next_state] =  np.log10(self._emit(next_state, sequence[t])) + k[argmaxk] 
                traceback_next[next_state] = argmaxk
                
            trellis = trellis_next
            traceback.append(traceback_next)
            
        max_final_state = max(trellis, key=trellis.get)
        max_final_prob = trellis[max_final_state]
                
        result = [max_final_state]
        for t in reversed(range(len(sequence)-1)):
            result.append(traceback[t][max_final_state])
            max_final_state = traceback[t][max_final_state]
            
        return result[::-1]
```

<br>

> ‚ë£ **Type 1.** PSSM: Simpler HMM structure

> ‚ë§ **Type 2.** Profile HMM: It is advantageous over PSSMs regarding the following: 

>> ‚óã Diagram of profile HMM

<br>

![image](https://github.com/user-attachments/assets/7068b7b0-a363-4a77-941b-6b6ed71322bf)

**Figure 3.** Diagram of profile HMM 

<br>

>>> ‚óã M, I, and D represent match, insertion, and deletion, respectively.

>>> ‚óã M<sub>i</sub> can be transitioned to M<sub>i+1</sub>, I<sub>i</sub>, and D<sub>i+1</sub>.

>>> ‚óã I<sub>i</sub> can be transitioned to M<sub>i+1</sub>, I<sub>i</sub>, and D<sub>i+1</sub>.

>>> ‚óã D<sub>i</sub> can be transitioned to M<sub>i+1</sub>, I<sub>i</sub>, and D<sub>i+1</sub>.

>> ‚óã **Advantage 1.** The ability to model insertions and deletion

>> ‚óã **Advantage 2.** Transitions are restricted only between valid state traversal.

>> ‚óã **Advantage 3.** Boundaries between states are better defined.

‚ë∏ **Type 3.** Markov chain Monte Carlo (MCMC)

> ‚ë† Definition: A method for generating samples from a Markov chain following a complex probability distribution

> ‚ë° **Method 1.** Metropolis-Hastings

>> ‚óã Generate a new candidate sample from the current state ‚Üí Accept or reject the candidate sample ‚Üí If accepted, transition to the new state

> ‚ë¢ **Method 2.** Gibbs Sampling

<br>

<img width="616" height="236" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2025-12-13 ·Ñã·Ö©·Ñå·Ö•·Ü´ 1 36 59" src="https://github.com/user-attachments/assets/c61ca8fe-374f-4831-9de4-76547d9fa460" />

<br>

> ‚ë£ **Method 3.** Importance/Rejection Sampling

> ‚ë§ **Method 4.** Reversible Jump MCMC

>> ‚óã General MCMC methods like **Method 1** and **Method 2** sample from a probability distribution in a fixed-dimensional parameter space

>> ‚óã Reversible Jump MCMC operates in a variable-dimensional parameter space: The number of parameters dynamically changes during sampling

<br>

<br>

## **3\. Markov Decision Process** 

 ‚ë¥ Overview

> ‚ë† Definition: A decision process in which the future depends only on the current state.

> ‚ë° In practice, the vast majority of problem settings can be treated as a Markov decision process (MDP).

> ‚ë¢ Schematic: For transitions, the state at t+1 is determined solely as a function of the state at t.

<br>

<img width="541" height="301" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-01-23 ·Ñã·Ö©·Ñí·ÖÆ 5 25 11" src="https://github.com/user-attachments/assets/98bbcf50-619e-43e1-9c24-0ad37f519350" />

 **Figure 4.** Agent-Environment Interaction in MDP

<br>

>> ‚óã State: $s_t ‚àà S$

>> ‚óã Action: $a_t ‚àà A$

>> ‚óã Reward: $r_t ‚àà R(s_t, a_t)$

>> ‚óã Policy: $a_t ~ œÄ(¬∑ \mid s_t)$

>> ‚óã Transition: $(s_{t+1}, r_{t+1}) ~ P(¬∑ \mid s_t, a_t)$

> ‚ë£ Existence of optimal solution $V(s)$

<br>

<img width="159" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2025-02-25 ·Ñã·Ö©·Ñí·ÖÆ 10 39 24" src="https://github.com/user-attachments/assets/f48b935c-a53b-45a5-8d8b-7b5a71134b81" />

<br>

>> ‚óã **Premise 1.** Markov property

>> ‚óã **Premise 2.** Stationary assumption

>> ‚óã **Premise 3.** No distributional shift

 ‚ëµ **Type 1.** Q-learning (Watkins, 1989)

> ‚ë† Overview 

>> ‚óã Learn the Q-function (action-value function) directly from data.

>> ‚óã Model-free (off-policy), i.e., the transition dynamics $P(x' \mid x, u)$‚Äîis unknown.

>> ‚óã The reward function $R(s, a)$ is known.

>> ‚óã Example of [value iteration](https://jb243.github.io/pages/895#3-laws-of-stochastic-control-theory).

> ‚ë° Formula: Because the next state $j$ is fixed, the transition probability term drops out.

<br>

<img width="494" height="265" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-01-23 ·Ñã·Ö©·Ñí·ÖÆ 2 07 42" src="https://github.com/user-attachments/assets/11dbb75d-d664-4227-8859-5dc1047a649e" />

<br>

>> ‚óã **Step 1.** Initialize estimates for all state/action pairs: QÃÇ(x, u) = 0

>> ‚óã **Step 2.** Take a random action $u$

>> ‚óã **Step 3.** Observe the next state $x'$ and receive $R(x', u)$ from the environment: Note that this is the realized reward, not the expected reward.

>> ‚óã **Step 4.** Update $QÃÇ(x, u)$

> ‚ë¢ Supplements

>> ‚óã Learning the full model requires $\left\|\mathcal{S}\right\|^2\left\|\mathcal{A}\right\|$ memory, but Q-learning only needs $\left\|\mathcal{S}\right\|\times\left\|\mathcal{A}\right\|$ memory.

>> ‚óã It is also referred to as TD (temporal-difference) learning.

>> ‚óã **Proof on convergence of Q-learning:** Pseudo-contraction mapping

<br>

<img width="651" height="713" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-02-05 ·Ñã·Ö©·Ñí·ÖÆ 2 33 05" src="https://github.com/user-attachments/assets/1e1d2f8c-f09b-47b6-a181-3b48c86c4bc2" />

<br>

‚ë∂ **Type 2.** SARSA (State-Action-Reward-State-Action; Œµ-greedy, epsilon greedy) (Rummery & Niranjan, 1994)

> ‚ë† Overview

>> ‚óã Depends on policy (on-policy).

>> ‚óã When there is no data available, it chooses a policy and collects new data through interaction with the environment.

> ‚ë° Formula

>> ‚óã **Step 1.** Initialize estimates for all state/action pairs: QÃÇ(s, a) = 0

>> ‚óã **Step 2.** At each step $k$, with probability 1 - Œµ<sub>k</sub>, take u ‚àà arg max<sub>v</sub> QÃÇ(x, v); with probability Œµ<sub>k</sub>, take a random action

>> ‚óã **Step 3.** Observe the next state $x'$ and receive $R(x', u)$: Note that this is the realized reward, not the expected reward.

>> ‚óã **Step 4.** Update QÃÇ(x, a)

<br>

<img width="506" height="37" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-01-23 ·Ñã·Ö©·Ñí·ÖÆ 2 28 46" src="https://github.com/user-attachments/assets/0860934e-49e4-42a7-8e41-571e2ebd1a76" />

<br>

>> ‚óã **Step 5.** As $k \to \infty$, drive $\varepsilon_k \to 0$; in this case, one can also use a Boltzmann (softmax) distribution with temperature annealing $(T \to 0)$.

<br>

<img width="624" height="212" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-01-23 ·Ñã·Ö©·Ñí·ÖÆ 2 32 05" src="https://github.com/user-attachments/assets/f8f82aac-8a81-4df6-91f5-39bd8aa50a7e" />

<br>

> ‚ë¢ Comparison with Q-learning

>> ‚óã Target policy: the policy to be learned

>> ‚óã Behavior policy: the policy used to generate samples

>> ‚óã Q-learning: target policy = optimal policy; behavioral policy = any policy under which each action is taken infinitely often

>> ‚óã SARSA: target policy = Œµ-greedy policy; behavioral policy = Œµ-greedy policy

>> ‚óã **Supplement 1.** Using an Œµ-greedy behavior policy doesn‚Äôt automatically make it SARSA; what matters is the target policy used in the update.

>> ‚óã **Supplement 2.** ‚ÄúOff-policy‚Äù means that no matter which policy collected the data, the update uses a greedy target (i.e., the max over next actions).

>> ‚óã **Supplement 3.** If the behavior policy is fully greedy (Œµ = 0), then in SARSA $a' = \arg\max_{a} Q(s', a)$, so the targets match and the update equations become essentially the same.

‚ë∑ **Type 3.** Q-learning with Linear Function Approximation

> ‚ë† **Purpose:** The memory space of Q-learning is „Ö£ùíÆ„Ö£ √ó „Ö£ùíú„Ö£, while that of linear function approximation is M ‚â™ „Ö£ùíÆ„Ö£ √ó „Ö£ùíú„Ö£.

> ‚ë° **Formula:** Note that Œ¥<sub>k</sub><sup>2</sup>, which is the squared temporal difference, is always non-negative.

<br>

<img width="602" height="228" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-02-10 ·Ñã·Ö©·Ñå·Ö•·Ü´ 8 56 04" src="https://github.com/user-attachments/assets/66b0ded6-2e47-4982-9da5-5affd3bf43ef" />

<br>

> ‚ë¢ **Significance:** With a fixed policy and on-policy sampling, convergence is guaranteed (in an appropriate sense).

<br>

<img width="429" height="336" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-02-12 ·Ñã·Ö©·Ñå·Ö•·Ü´ 9 26 58" src="https://github.com/user-attachments/assets/e3c8231e-ba3d-4d73-ad6a-62793c00f7cb" />

<br>

> ‚ë£ **Problem:** If those conditions are violated (e.g., off-policy learning or updating with incorrect weighting), the algorithm can diverge.

<br>

<img width="609" height="776" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-02-12 ·Ñã·Ö©·Ñå·Ö•·Ü´ 9 30 47" src="https://github.com/user-attachments/assets/752c83c0-448e-40e8-aa0c-14b85f02c808" />

<br>

‚ë∏ **Type 4.** DQN

> ‚ë† Q-learning with nonlinear function approximation

>> ‚óã Formula

<br>

<img width="461" height="134" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-02-15 ·Ñã·Ö©·Ñí·ÖÆ 3 22 08" src="https://github.com/user-attachments/assets/576d25ac-f3f1-44f2-9a9d-aa7aa8bafac8" />

<br>

>> ‚óã **Problem:** Local optima, Unstability during training 

> ‚ë° Experience Replay (Replay Buffer)

>> ‚óã Definition: A replay buffer is a memory that stores the agent's past experiences collected from the environment. A single time-step transition is usually stored as $(s, a, r, s', \text{done})$.

>> ‚óã Motivation: In reinforcement learning, data arrive as a sequence $(y_1,(x_1,u_1)) \rightarrow (y_2,(x_2,u_2)) \rightarrow (y_3,(x_3,u_3)) \rightarrow \cdots$, so samples are highly correlated and the data distribution keeps changing. Deep learning (SGD) is typically much more stable when samples are i.i.d. (independent and identically distributed). With highly correlated data, updates can ‚Äúswing‚Äù in one direction, often leading to divergence or oscillation during training.

>> ‚óã How it's used in training: Instead of updating the network using only the most recent experience, randomly sample a batch of transitions from the buffer and train the network on that mini-batch. This makes the training data closer to i.i.d., which helps improve stability and can reduce the chance of getting stuck in poor local solutions (i.e., improves training stability and robustness).

> ‚ë¢ DQN (Deep Q-Network)

>> ‚óã Definition: DQN implements Q-learning using deep neural networks. It is often implemented as an off-policy method.

>>> ‚óã Online network (Q-network): The network that is currently being trained. With parameters $\theta$, it outputs $Q(s,a;\theta)$. It is used both for action selection (e.g., $\epsilon$-greedy) and is updated continuously via SGD.

>>> ‚óã Target network: A slowly changing copy of the online network. With parameters $\theta^-$, it outputs $Q(s,a;\theta^-)$ and is used to compute the target value $y = r + \alpha \max_{a'} Q(s', a'; \theta^-)$. The target network is updated either by periodically copying the online parameters ($\theta^- \leftarrow \theta$, hard update) or by slowly tracking the online network, which stabilizes the targets.

>> ‚óã Comparison with experience replay: Both aim to stabilize training, but they are different mechanisms.

>>> ‚óã Experience replay: Reduces sample correlation and distribution shift by shuffling data (making it closer to i.i.d.) through random sampling from a replay buffer.

>>> ‚óã DQN (target network mechanism): If the $Q$ used to compute the target $y = r + \alpha \max Q(s', a')$ changes too rapidly, training can diverge. The target network updates slowly, reducing the ‚Äúmoving target‚Äù problem and improving stability.

>> ‚óã SGD detail: To treat $y_k$ as a label, DQN ignores the fact that $y_k$ is actually a function of the network parameters $w$.

<br>

<img width="513" height="128" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-02-15 ·Ñã·Ö©·Ñí·ÖÆ 4 30 28" src="https://github.com/user-attachments/assets/d63df82e-fe6c-48d5-b115-da741fd8dbc5" />

<br>

>> ‚óã Mini-batch SGD: Training is typically performed using mini-batches sampled from the replay buffer.

<br>

<img width="491" height="111" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-02-15 ·Ñã·Ö©·Ñí·ÖÆ 4 34 47" src="https://github.com/user-attachments/assets/72e147d0-8a62-4a17-9848-b559e89f7e38" />

<br>

>> ‚óã Implementation in Pytorch

<br>

<img width="654" height="222" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-02-15 ·Ñã·Ö©·Ñí·ÖÆ 4 47 42" src="https://github.com/user-attachments/assets/70bd436e-79ba-47c6-8dce-ae3e0edbe6a1" />

**Figure 5.** Implementation in Pytorch

<br>

>>> ‚óã Left (alternative form): `forward(state, action) -> q_scalar` with shape `(batch, 1)`. A drawback is that you must evaluate all possible actions separately‚Äîi.e., run multiple forward passes to compute $Q(s,1), Q(s,2), \ldots$ and then compare them.

>>> ‚óã Right (standard DQN form): `forward(state) -> q_values` with shape `(batch, num_actions)`. Since you only need a single forward pass and then take `argmax`, action selection is simple and computationally efficient. Also, because the early hidden layers are shared, the learning signal for one action can change the shared representation and indirectly affect the Q-values of other actions (i.e., *coupling*).

<br>

```python
random.sample(self.replay buffer, batch size)
next state values = target net(next states).max(1)[0]
target values= rewards + gamma √ó next state values.detach()
loss = nn.MSELoss()(state action values, target values)
optimizer.zerograd(); loss.backward(); optimizer.step()
```

<br>

>> ‚óã Applications: Applied to many games (e.g., DQN on Atari 2600 (2013), AlphaGo (2016), etc.).

> ‚ë£ DDQN(double Q-learning)

>> ‚óã Thrun and Schwartz (1993): Q-learning often overestimates the action value (Q-value) under certain states.

<br>

<img width="434" height="201" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-02-15 ·Ñã·Ö©·Ñí·ÖÆ 6 51 48" src="https://github.com/user-attachments/assets/34427d38-2055-4e7f-9f72-152cd40e81cf" />

<br>

>> ‚óã Double Q-learning (van Hasselt, 2010)

>>> ‚óã Definition: Train two Q-functions (with parameters $w$ and $w'$), then take their average to mitigate the overestimation problem. In this setup, the target network is used to select the action, and the online network is used to evaluate its value.

>>> ‚óã Principle: Double estimator is underestimator.

<br>

<img width="516" height="368" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-02-15 ·Ñã·Ö©·Ñí·ÖÆ 7 23 32" src="https://github.com/user-attachments/assets/65d36045-cd08-4a2b-9367-d647a78d8185" />

<br>

>>> ‚óã Formula

<br>

<img width="536" height="251" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-02-15 ·Ñã·Ö©·Ñí·ÖÆ 7 35 46" src="https://github.com/user-attachments/assets/30c3af4c-4493-421e-8b9c-ec813edd07a9" />

<br>

>>> ‚óã Q-learning vs. Double Q-learning (Weng et al., 2020): In Q-learning, the learning rate is $\beta_k = \frac{c}{k}$, whereas in Double Q-learning it is $\beta_k = \frac{2c}{k}$. Under linear function approximation, we can obtain the following. This also implies that Double Q-learning can converge faster because it uses a larger learning rate.

<br>

<img width="571" height="58" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-02-15 ·Ñã·Ö©·Ñí·ÖÆ 7 42 11" src="https://github.com/user-attachments/assets/7a85c1d1-50ea-4426-a8b5-d846e77415a3" />

<br>

>> ‚óã Clipped double Q-learning (Fujimoto, van Hoof, David Meger, 2018)

<br>

<img width="454" height="137" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2026-02-15 ·Ñã·Ö©·Ñí·ÖÆ 6 51 23" src="https://github.com/user-attachments/assets/fadf4c0e-924b-4ca9-9edc-82f0af7be849" />

<br>

‚ëπ **Type 5.** Policy Gradient

> ‚ë† Overview

>> ‚óã Definition: Learning policy œÄ(s) directly

>> ‚óã **Characteristics 1.** In Q-learning, states can be continuous, but actions must be discrete

>> ‚óã **Characteristics 2.** In policy gradient, both states and actions can be continuous

> ‚ë° Algorithm

>> ‚óã **Step 1.** Receive frame

>> ‚óã **Step 2.** Forward propagate to get P(action)

>> ‚óã **Step 3.** Sample a from P(action)

>> ‚óã **Step 4.** Play the rest of the game

>> ‚óã **Step 5.** If the game is won, update in the ‚àáŒ∏ direction

>> ‚óã **Step 6.** If the game is lost, update in the -‚àáŒ∏ direction

‚ë∫ Other Types

> ‚ë† Deep O-Network (DON)

> ‚ë° A3C (Asynchronous Advantage Actor-Critic)

> ‚ë¢ [GA](https://jb243.github.io/pages/1146)(Genetic Algorithm)

<br>

<br>

## **4\. General Decision Process** 

‚ë¥ [MAB](https://jb243.github.io/pages/2160#1-overview)(multi-armed bandit)

‚ëµ [UCB](https://jb243.github.io/pages/2160#2-ucb) 

‚ë∂ [Thompson sampling](https://jb243.github.io/pages/2160#3-thompson-sampling) 

‚ë∑ [Early stopping problem](https://jb243.github.io/pages/2160#4-early-stopping-problem) 
 
<br>

---

_Input: 2021.12.13 15:20_

_Updated: 2024.10.08 22:43_
