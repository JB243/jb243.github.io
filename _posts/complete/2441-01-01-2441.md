## **2025 National Civil Service Exam (Technical) Question 2]**

Recommended Post: ã€Artificial Intelligenceã€‘ [AI Technical Exam Table of Contents](https://jb243.github.io/pages/714)

---

**a.** [Stochastic Control Theory](https://jb243.github.io/pages/895)

**b.** [Reinforcement Learning](https://jb243.github.io/pages/2162)

---

<br>

## **Q.** 

The following is a 4 Ã— 4 grid board on which a robot can move around. Cells 1 and 16, marked in gray, are terminal points. The robot can move in four directionsâ€”east, west, south, and northâ€”from its current position. If it chooses a direction that leads outside the board, it stays in place and receives a reward of â€˜-1â€™. If it chooses a direction that stays within the board, it moves in that direction and receives a reward of â€˜0â€™. When it enters a terminal point, it receives a reward of â€˜5â€™. Answer the following questions.

<br>

<img width="536" height="262" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-20 á„‹á…©á„’á…® 2 22 03" src="https://github.com/user-attachments/assets/a24b3c46-9dd0-435e-9a1e-20894f6826c9" />

<br>

### **Q1.** 

(b) shows an example of a movement path from the starting position, cell 10 (marked with a square), to the terminal point, cell 1. Find the state set, action set, and reward set for this movement problem.

### **A1.** 

> State set: {Move, Terminal}

> Action set: {East, West, South, North, Wait} (Note: East, West, South, North correspond to the â€˜Moveâ€™ state, while Wait corresponds to the â€˜Terminalâ€™ state.)

> Reward set: {-1, 0, 5}

### **Q2.** 

Calculate the cumulative reward (total reward) of â’.

### **A2.**

> 0 â†’ 0 â†’ 0 â†’ 0 â†’ 0 â†’ -1 â†’ -1 â†’ 4

> Therefore, the cumulative reward (total reward) is 4.

<br>

---

<br>

## **Practice Problem.** 

Controlled Markov Chain Formulation (Source: ECE 558, UMich)

### **Problem.**

> Suppose a system evolves as discrete-time Markov chain on finite state space S = {1, 2, . . . , I } evolves according to a fixed transition law P<sub>t</sub>(j|i) at time t = 0,1,2,..., and it generates costs c<sub>t</sub>(i) if it is in state i. At any stage, the decision maker may either let the system evolve uninterrupted or instead intervene and choose an action u âˆˆ U where U is finite, which leads to transition law P<sub>t</sub>(j|i, u) at time t and generates cost c<sub>t</sub>(i,u) at time t. Formulate this as a Controlled Markov Chain (aka Markov Decision Process). Clearly identify the action sets, costs and the transition probabilities.

### **Answer.**

> Denote the choice of the decision-maker to not interrupt the system as action 0, and then append to action set ğ’° to get new action set _Å¨_ = {0} âˆª ğ’°; when the decision-maker chooses to interrupt the system, then they do so by choosing a specific action u âˆˆ ğ’°. Then, define P<sub>t</sub>(j|i,0) = P<sub>t</sub>(j|i) for all i,j âˆˆ S and t âˆˆ â„¤<sub>+</sub>, and c<sub>t</sub>(i,0) = c<sub>t</sub>(i) for all i âˆˆ S and t âˆˆ â„¤<sub>+</sub>. Then, the new problem with states space S, action space _Å¨_, transition kernels {P<sub>t</sub>(j|i,u) : i,j âˆˆ S andu âˆˆ _Å¨_, âˆ€t âˆˆ â„¤<sub>+</sub>}, and cost functions {c<sub>t</sub>(i,u) : i âˆˆ S and u âˆˆ _Å¨_, âˆ€t âˆˆ â„¤<sub>+</sub>} is a Markov Decision Process (MDP) formulation of the original problem.

<br>

---

_Input: 2025.07.14 01:32_
