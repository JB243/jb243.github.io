## **Chapter 22. Image Generative Models**

Recommended Reading: 【Algorithm】 [Algorithm Index](https://jb243.github.io/pages/1278)

---

**1.** [DIP](#1-dip)

**2.** [Vision Transformer](#2-vision-transformer)

**3.** [Image Generative Model](#3-image-generative-model)

**4.** [Vision-Language Model](#4-vision-language-model)

**5.** [Video Generative Model](#5-video-generative-model)

**6.** [Explainability Model](#6-explainability-model)

---

<br>

## **1. Convolutional Neural Network (CNN)**

⑴ [Concepts of CNN](https://jb243.github.io/pages/2152)

⑵ **Example 1.** [DIP](https://jb243.github.io/pages/2164) (deep image prior): Overfits the CNN architecture to the input image without training data to generate new images

⑶ **Example 2.** Detectron2: Developed by Facebook AI.

⑷ **Example 3.** Functions provided by OpenCV 

<br>

<br>

## **2. Computer Vision Foundation Models**

⑴ Vision Transformer (ViT)

> ① ViT uses only the [transformer encoder](https://jb243.github.io/pages/325#:1.-,,-\(transformerencoder\)) structure

> ② **Step 1.** Split the image into multiple small patches and treat each patch as a token for input into the transformer

> ③ **Step 2.** Embed each patch using the transformer encoder

> ④ **Step 3.** Just like embedding words in a sentence and outputting the sentence embedding that represents the sentence's meaning, ViT learns the relationships between patches and outputs features representing the whole image.

> ⑤ **Limitation:** The computation of self-attention is proportional to the square of the number of patches that make up the image, making it difficult to input high-resolution images at once.

>> ○ **Solution 1:** Divide the given image into smaller patches and apply ViT independently to each patch (e.g., [iSTAR](https://www.nature.com/articles/s41587-023-02019-9)).

>> ○ **Solution 2:** Introduce an extended self-attention mechanism, such as dilated self-attention using models like LongNet (e.g., [Prov-GigaPath](https://www.nature.com/articles/s41586-024-07441-w)).

⑵ Types

> ① DINO(self-distillation with no labels)

> ② IBOT(image BERT pretraining with online tokenizer)

> ③ [BEiT](https://nate9389.tistory.com/325#:~:text=%E2%91%A0-,BERT,-%2C%20RoBERTa%2C): A ViT variant that adopts the idea of the BERT model, trained similarly to masked language modeling.

>> ○ [iSTAR](https://www.nature.com/articles/s41587-023-02019-9): Used to enhance the resolution of spatial transcriptomics. It utilizes a BEiT-based model trained with the DINO method.

<br>

![image](https://github.com/user-attachments/assets/f17eb8ba-feb8-4957-a564-3edae5a381c5)

**Figure 1.** Diagram for data preparation step in iSTAR

<br>

>>> ○ **Step 1.** Divide the given image into 256 × 256 patches.  

>>> ○ **Step 2.** Further divide each patch into 16 × 16 sub-patches.  

>>> ○ **Step 3.** Apply ViT (denoted as f2) to each sub-patch to obtain a 384-dimensional vector.  

>>> ○ **Step 4.** Aggregate the 384-dimensional vectors to form a 16 × 16 × 384 data structure, then apply another ViT (denoted as f1) to obtain a 192-dimensional vector.  

>>> ○ **Step 5.** Gather the 192-dimensional vectors and apply ViT (denoted as f0).  

>>> ○ Feature extraction and loss function formulation.

<br>

<img width="583" alt="스크린샷 2025-02-24 오후 12 46 10" src="https://github.com/user-attachments/assets/ac4aeea6-b9ee-4e9a-9a9b-d9c5ff831c28" />

<br>

> ④ Swin Transformer: A ViT variant that uses window-based local self-attention.

> ⑤ [CTransPath](https://www.sciencedirect.com/science/article/pii/S1361841522002043) **:** Wang et al., Medical Image Analysis (2022)

> ⑥ [UNI](https://www.nature.com/articles/s41591-024-02857-3) **:** Chen et al., Nature Medicine (2024)

> ⑦ [CONCH](https://www.nature.com/articles/s41591-024-02856-4) (CONtrastive learning from Captions for Histopathology) **:** Lu et al., Nature Medicine (2024)

> ⑧ [Virchow](https://arxiv.org/abs/2309.07778) **:** Vorontsov et al., arxiv (2023)

> ⑨ [RudolfV](https://arxiv.org/abs/2401.04079) **:** Dippel et al., arxiv (2024)

> ⑩ [Campanella](https://arxiv.org/abs/2310.07033) **:** Campanella et al., arxiv (2023)

> ⑪ [Prov-GigaPath](https://www.nature.com/articles/s41586-024-07441-w): Announced by Microsoft, it is a vision foundation model trained on 170,000 pathology images (1.3 billion tiles) (2024).

> ⑫ PRISM

<br>

<br>

## **3. Image Generation Model** 

⑴ Types

> ① DALL·E3 (OpenAI)

> ② Midjourney

> ③ Stable Diffusion

> ④ Sora (OpenAI)

> ⑤ Video LLM

<br>

<br>

## **4. Vision-Language Model** 

⑴ Types

> ① [Stable diffusion](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb): AI algorithm to generate digital images from natural language

> ② [MedGemma](https://github.com/google-health/medgemma) 

> ③ TITAN: digital pathology 

<br>

<br>

## **5. Video Generative Model**

⑴ Types

> ① XVFI: A kind of optical flow.

> ② FILM(Frame Interpolation for Large Motion): Encoder + U-Net like decoder  

<br>

<br>

## **6. Explainability Model**

⑴ Types

> ① LIME: Explanation based on local approximation (surrogate models). Visualizes each feature's local contribution. Can explain any computer vision model.

> ② SHAP: Decomposes predictions using game-theoretic Shapley values to derive each feature's fair contribution.

<br>

---

_Input: 2024.04.22 14:08_
