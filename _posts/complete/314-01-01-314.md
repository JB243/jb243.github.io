## **Chapter 22. Image Generative Models**

Recommended Reading **:** 【Algorithm】 [Algorithm Index](https://jb243.github.io/pages/1278)

---

**1.** [DIP](#1-dip)

**2.** [Vision Transformer](#2-vision-transformer)

**3.** [Image Generative Models](#3-image-generative-models)

---

<br>

## **1. [DIP](https://jb243.github.io/pages/2164)** (deep image prior)

 ⑴ Features **:** Overfits the CNN architecture to the input image without training data to generate new images

<br>

<br>

## **2. Computer Vision Foundation Models**

⑴ Vision Transformer (ViT)

> ① ViT uses only the [transformer encoder](https://jb243.github.io/pages/325#:1.-,,-\(transformerencoder\)) structure

> ② **Step 1.** Split the image into multiple small patches and treat each patch as a token for input into the transformer

> ③ **Step 2.** Embed each patch using the transformer encoder

> ④ **Step 3.** Just like embedding words in a sentence and outputting the sentence embedding that represents the sentence's meaning, ViT learns the relationships between patches and outputs features representing the whole image.

> ⑤ **Limitation:** The computation of self-attention is proportional to the square of the number of patches that make up the image, making it difficult to input high-resolution images at once.

>> ○ **Solution 1:** Divide the given image into smaller patches and apply ViT independently to each patch (e.g., [iSTAR](https://www.nature.com/articles/s41587-023-02019-9)).

>> ○ **Solution 2:** Introduce an extended self-attention mechanism, such as dilated self-attention using models like LongNet (e.g., [Prov-GigaPath](https://www.nature.com/articles/s41586-024-07441-w)).

⑵ Types

> ① [BEiT](https://nate9389.tistory.com/325#:~:text=%E2%91%A0-,BERT,-%2C%20RoBERTa%2C): A ViT variant that adopts the idea of the BERT model, trained similarly to masked language modeling.

>> ○ [iSTAR](https://www.nature.com/articles/s41587-023-02019-9): Used to enhance the resolution of spatial transcriptomics. It utilizes a BEiT-based model trained with the DINO method.

> ② Swin Transformer: A ViT variant that uses window-based local self-attention.

> ③ [CTransPath](https://www.sciencedirect.com/science/article/pii/S1361841522002043) **:** Wang et al., Medical Image Analysis (2022)

> ④ [UNI](https://www.nature.com/articles/s41591-024-02857-3) **:** Chen et al., Nature Medicine (2024)

> ⑤ [CONCH](https://www.nature.com/articles/s41591-024-02856-4) (CONtrastive learning from Captions for Histopathology) **:** Lu et al., Nature Medicine (2024)

> ⑥ [Virchow](https://arxiv.org/abs/2309.07778) **:** Vorontsov et al., arxiv (2023)

> ⑦ [RudolfV](https://arxiv.org/abs/2401.04079) **:** Dippel et al., arxiv (2024)

> ⑧ [Campanella](https://arxiv.org/abs/2310.07033) **:** Campanella et al., arxiv (2023)

> ⑨ [Prov-GigaPath](https://www.nature.com/articles/s41586-024-07441-w): Announced by Microsoft, it is a vision foundation model trained on 170,000 pathology images (1.3 billion tiles) (2024).

<br>

<br>

## **3. Image Generation Models** 

⑴ Types

> ① DALL·E3 (OpenAI)

> ② Midjourney

> ③ Stable Diffusion

> ④ Sora (OpenAI)

> ⑤ Video LLM

<br>

---

_Input: 2024.04.22 14:08_
