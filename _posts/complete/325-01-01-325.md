## **Chapter 21.** Large Language Models (LLM)

Recommended Readings: 【Algorithms】 [Table of Algorithm Contents](https://jb243.github.io/pages/1278)

---

**1.** [Natural Language Processing](#1-natrual-language-processing-nlp)

**2.** [Large Language Models](#2-large-language-models-llm)

**3.** [Bioinformatics and Language Models](#3-bioinformatics-and-language-models)

---

**a.** [Prompt Engineering](https://jb243.github.io/pages/2317)

**b.** [Natural Language Processing and LLM Useful Function Collection](https://jb243.github.io/pages/2404)

**c.** [Research Topics related to LLM](https://jb243.github.io/pages/194)

---

<br>

## **1\. Natural Language Processing** (NLP)

⑴ Definition: AI models based on text

⑵ Text Preprocessing: Preprocessing to make unstructured text recognizable to computers

> ① Tokenization

>> ○ Dividing sentences or corpora into minimum meaning units, tokens, for computer recognition

>> ○ **English**: Mainly divided by spaces

>>> ○ 75 English words ≃ 100 tokens

>>> ○ Example: I / ate / noodle / very / deliciously

>> ○ Example: [OpenAI Tokenizer](https://platform.openai.com/tokenizer)

> ② Part-of-Speech Tagging (POS tagging)

>> ○ Technique to tag the parts of speech of morphemes

> ③ Lemmatization

>> ○ Technique to find lemmas (base words) from words

>> ○ Example: am, are, is → be

> ④ Stemming

>> ○ Technique to obtain stems by removing prefixes and suffixes from words

> ⑤ Stopword Removal

>> ○ Technique to handle words that contribute little to actual meaning analysis, such as particles and suffixes

⑶ Text Mining 

> ① Topic Modeling

>> ○ One of the statistical models in the fields of machine learning and natural language processing used to discover abstract topics, referred to as 'topics', within a collection of documents.

>> ○ Used to uncover the hidden meaning structure in the text body.

> ② Word Cloud

>> ○ A visualization technique that uses natural language processing to simply count and visualize people's interests or frequency.

> ③ Social Network Analysis (SNA)

>> ○ An analytical technique for analyzing and visualizing the network characteristics and structure among people within a group.

> ④ TF-IDF (Term Frequency-Inverse Document Frequency)

>> ○ A technique used to extract how important a word is within a specific document, in a collection of multiple documents.

⑷ Types

> ① **Type 1.** Latent Semantic Analysis (LSA)

> ② **Type 2.** Probabilistic Latent Semantic Analysis (PLSA)

> ③ **Type 3.** Latent Dirichlet Allocation (LDA): Generative probabilistic models. Can be used even for deconvolution without reference ([ref](https://www.nature.com/articles/s41467-022-30033-z#Sec11))

> ④ **Type 4.** Word2Vec: Skip-gram is a component of the Word2Vec model.

⑸ Evaluation of Language Models

> ① Parallel text datasets: Canadian Parliament (English ← Spanish), European Parliament (multiple languages ​​supported)

> ② SNS

> ③ Bleu score ([ref](https://dl.acm.org/doi/10.3115/1073083.1073135))

> ④ perplexity

<br>

<br>

## **2\. Large Language Models** (LLM)

⑴ Definition: Natural language processing models with billions of parameters

> ① Counting parameters method: [here](https://jb243.github.io/pages/2152#:~:text=%E2%97%8B-,Counting%20the%20Number%20of%20Parameters,-%3D%20Goal%20of%20the)

> ② Terms 1: Meaning of 7B, 13B, 30B, 65B: The number of parameters in natural language models is 7 billion, 13 billion, 30 billion, and 65 billion

> ③ Term 2: Token: The unit of text processed by the model

>> ○ Word-level tokenization: [**ChatGPT**, **is**, **an**, **AI**, **language**, **model**, **.**]

>> ○ Subword-level tokenization: [**Chat**, **G**, **PT**, **is**, **an**, **AI**, **language**, **model**, **.**]

>> ○ Character-level tokenization: [**C**, **h**, **a**, **t**, **G**, **P**, **T**, **i**, **s**, **a**, **n**, **A**, **I**, **l**, **a**, **n**, **g**, **u**, **a**, **g**, **e**, **m**, **o**, **d**, **e**, **l**]

> ④ Term 3: Meaning of 0-shot, 1-shot, etc.: The number of example inputs given per task

>> ○ 0-shot prompting

<br>

```python
Q: <Question>?
A:
```

<br>

>> ○ Few-shot prompting

<br>

```python
Q: <Question>?
A: <Answer>
Q: <Question>?
A: <Answer>
Q: <Question>?
A: <Answer>
Q: <Question>?
A:
```

<br>

⑵ **Transformer**

> ① Overview

>> ○ The etymology of Transformer: What are "they" in the sentence of "The students emailed the professor because they were confused."?

>> ○ Transformer Encoder: It tokenizes the input text, converts it into numbers (embeddings), and encodes it into contextual representations (feature vectors).

>> ○ Transformer Decoder: Using those representations (or previously generated tokens), it predicts a probability distribution over the next token, selects one, and decodes it back into text.

>> ○ Because a transformer has to pick one token, hallucinations can occur during that selection process.

> ② Attention model ([`Attention Is All You Need`](https://arxiv.org/abs/1706.03762))

>> ○ Definition: A self-regression mechanism used in both the Transformer encoder and decoder.

>> ○ Components

>>> ○ $Q$ (query): the entity that needs information.

>>> ○ $K$ (key): a retrieval label that indicates what each token/word represents.

>>> ○ $V$ (value): the actual content carried by that information.

>> ○ Formalization (using a spatial transcriptomics example)

<br>

<img width="520" height="460" alt="스크린샷 2026-02-04 오후 2 59 42" src="https://github.com/user-attachments/assets/b376d373-bde3-480d-8282-210c22a398f0" />

**Figure 1.** Example of an attention model used in spatial transcriptomics

<br>

>>> ○ H′<sub>gene</sub> = CrossAttn(Q = H<sub>gene</sub>, K = H<sub>img</sub>, V = H<sub>img</sub>): the gene representation retrieves visual cues it needs from the image (e.g., tissue boundaries, cell density).

>>> ○ H′<sub>img</sub> = CrossAttn(Q = H<sub>img</sub>, K = H<sub>gene</sub>, V = H<sub>gene</sub>): the image representation integrates gene-expression pattern information for that location.

>> ○ Applications

>>> ○ FlashAttention: Applying FlashAttention, which optimizes memory access at the hardware (GPU) level, can speed up training by about 2–3×.

>>> ○ Positional encoding: Inject relative positional encoding (relative bias) into the attention map via bias addition.

> ③ **Stage 1.** Transformer Encoder: Converts the input sequence into a higher-level representation

>> ○ Training Methods: NSP, MLM

>>> ○ Next Sentence Prediction (NSP): Composes pairs of sentences A and B from the input data, and determines whether B follows A

>>>> ○ First Sentence: "The quick brown fox jumps over the lazy dog."

>>>> ○ Second Sentence: "The dog is not amused."

>>>> ○ Prediction Result: Whether the second sentence follows the first sentence (e.g., "No")

>>> ○ Masked Language Modeling (MLM): Masks some words in the sentence and predicts the masked words

>>> ○ In transformers like BERT, the encoder is trained.

>> ○ **1-1.** Self-Attention Mechanism

>>> ○ **1-1-1.** Splits the sentence into multiple tokens

>>> ○ **1-1-2.** For each pair of tokens in the input sequence, generates "query", "key", and "value" to calculate the attention score, which indicates how much each token should attend to other tokens

<br>

![image](https://github.com/user-attachments/assets/ee5afe38-a592-49e5-9e7d-1e0fca8a975b)

**Figure 2.** Attention score of a pair of tokens

<br>

>>> ○ **1-1-3.** Positional Encoding: The transformer cannot directly know the order of the input words, so positional encoding is added to each word's position information. 

>>>> ○ This allows the model to understand the sequence's order information.

>>>> ○ For example, when defined by sine and cosine functions, the closer the values, the larger the inner product becomes, allowing us to understand the adjacency relationship. 

>>>> ○ If the input has no inherent order, we can omit positional encodings from it.

>>> ○ **1-1-4.** Each token (word) within the encoder learns the relationships with all other tokens.

>>> ○ **1-1-5.** Token Embedding: Based on the attention score, the weighted sum of each token is calculated and each token is transformed into a new representation. The result embedded with positional encoding is called positional embedding.

>>> ○ **1-1-6.** When there are multiple attention heads, it is called multi-head attention.

>> ○ **1-2.** FFN (Feed-Forward Neural Network): Refines the representations generated through self-attention

>>> ○ Individually tunes each token embedding, using non-linear activation functions

>> ○ **1-3.** Add & Norm (LayerNorm): Performs layer normalization after self-attention and FFN

>> ○ **1-4.** The structure of "self-attention → Add & Norm → FFN → Add & Norm" is repeated across several layers

>>> ○ This gradually transforms each token embedding into a more context-aware vector.

>>> ○ **Type 1.** Initial Layers:

>>>> ○ Attention heads in each layer have significant variance in focus areas within the input features

>>>> ○ During this stage, broad exploration helps to distinguish between important and less important information

>>> ○ **Type 2.** Intermediate Layers:

>>>> ○ Attention heads in each layer consistently focus on lower-ranked information

>>>> ○ Identifies noise inherent in the data and understands details

>>> ○ **Type 3.** Final Layers:

>>>> ○ Attention heads in each layer consistently focus on higher-ranked information

>>>> ○ Extracts critical information and contributes to making final decisions

>> ○ **1-5.** Sentence embedding: Ultimately, it synthesizes each token embedding to convert it into a single vector that represents the overall meaning of the sentence. The Transformer decoder does not take the sentence embedding created by the Transformer encoder as input. Instead, the Transformer decoder receives the contextualized token embeddings from the encoder as input.

> ④ **Stage 2.** Transformer Decoder: Takes the representation generated by the encoder and ultimately generates the desired output

>> ○ Training Method: NWP

<br>

![image](https://github.com/user-attachments/assets/c29c0e0d-43a6-4fc5-9037-dcbdf925790a)

**Figure 3.** Defining Transformer Decoder Problems

<br>

>>> ○ Next Word Prediction (NWP): The task of predicting the next word within a given context

>>> ○ In transformers like GPT, the decoder is trained.

>> ○ **2-1.** Masked Self-Attention Mechanism

>>> ○ **2-1-1.** The decoder refers to the token embeddings generated by the encoder

>>> ○ **2-1-2.** Each token within the decoder refers only to the previous tokens to predict the next token: Applies masking to ensure the model does not see future information. The figure below shows that each token is embedded into vector representations

<br>

![image](https://github.com/user-attachments/assets/8d0f4dcd-6b22-4fd1-8bd7-3432d7c25cd3)

**Figure 4.** Token embedding results and next word prediction situation

<br>

>> ○ **2-2.** FFN (Feed-Forward Neural Network)

>>> ○ Used to refine the representations generated by the decoder to produce the final output

>>> ○ Similar to the encoder, the FFN non-linearly transforms each token embedding

<br>

![image](https://github.com/user-attachments/assets/d9782ec5-5b39-4a28-9441-4a89d1ecf43b)

**Figure 5.** attention multilayer perceptron

<br>

>> ○ **2-3.** Multilayer Decoder: Stacks multiple decoder layers to ultimately generate the output sequence

>>> ○ By serially attaching attention and multilayer perceptron, it is possible to continuously generate sentences

>>> ○ The decoder combines the information from the encoder with the information of the currently generated sequence to predict the next token

>>> ○ To date, the cleverest thinker of all time was ___ → undoubtedly

>>> ○ To date, the cleverest thinker of all time was undoubtedly ___ → Einstein

>>> ○ To date, the cleverest thinker of all time was undoubtedly Einstein, ___ → for

<br>

![image](https://github.com/user-attachments/assets/3a93a276-8e3f-45a4-9306-14403d3ebbff)

**Figure 6.** Multiple decoders

<br>

⑶ Important parameters

> ① temperature = 0: leading to very deterministic outputs 

> ② max_tokens API Call settings = 100: response is capped at 100 tokens

> ③ top_p = 1.0: uses all possible tokens for generating the response

> ④ frequency_penalty = 0.0: does not avoide repeating tokens more than it naturally would

> ⑤ presence_penalty = 0.0: no penalty applied for reusing tokens

⑷ Types

> ① [BERT](https://arxiv.org/abs/1810.04805), [RoBERTa](https://arxiv.org/abs/1907.11692), [ALBERT](https://arxiv.org/abs/1909.11942)

>> ○ Using a bidirectional transformer, which is advantageous for understanding text as it considers the context of both the left and right sides of the input sequence.

>> ○ The input for BERT is a maximum of 512 tokens, but the input can be increased by extending the positional embedding: 2<sup>n</sup> is commonly used as the maximum input.

>> ○ A function that loads BERT or BioBERT from Hugging Face to create an attention matrix for a given sentence.

<br>

```
import torch
from transformers import BertTokenizer, BertModel
import matplotlib.pyplot as plt
import seaborn as sns

# Load BERT model and tokenizer (OPTION 1)
'''
model_name = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertModel.from_pretrained(model_name, output_attentions=True)
'''

# Load BioBERT model and tokenizer (OPTION 2)
model_name = 'dmis-lab/biobert-base-cased-v1.1'
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertModel.from_pretrained(model_name, output_attentions=True)

# Input sentence
sentence = "Find diseases associated with glucose"

# Tokenization
inputs = tokenizer(sentence, return_tensors='pt')

# Calculate outputs and attention weights through the model
outputs = model(**inputs)
attentions = outputs.attentions  # These are the attention weights for each layer

# Visualize the attention weights from the first head of the first layer
attention = attentions[0][0][0].detach().numpy()

# Token list
tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])

# Visualize Attention weights
plt.figure(figsize=(10, 10))
sns.heatmap(attention, xticklabels=tokens, yticklabels=tokens, cmap='viridis')
plt.title('Attention Weights')
plt.show()
```

<br>

>> ○ Used in Google Search, etc.

> ② [GPT](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf), [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), [GPT-3](https://arxiv.org/abs/2005.14165), GPT-J, GPT-Neo, GPT-3.5, GPT-4

>> ○ Focuses on predicting the next word based on the past context only by sequentially considering the context from left to right in the input sequence.

>> ○ Uses an autoregressive model.

<br>

<img width="396" alt="스크린샷 2024-10-28 오후 10 13 49" src="https://github.com/user-attachments/assets/dcda95fa-51a9-408f-9a49-e5d3eb5a8c31">

<br>

>> ○ **GPT-1:** Released in 2018. 150 million parameters. Trained on about 4 GB of web data (corpus). Introduced the concepts of transfer learning and fine-tuning.

>> ○ **GPT-2:** Released in 2019. 117 million to 1.542 billion parameters. Trained on GPT-1 data plus additional data (40 GB). Introduced the concept of meta-learning.

>> ○ **GPT-3:** Released in 2020. 1.75 billion parameters. Trained on 570 GB of data. Raised awareness of ethical issues. Trained at the scale of a Microsoft Azure supercomputer (285,000 CPUs, 10,000 GPUs, 400G network).

>> ○ **GPT-3.5:** Released in 2022. Fine-tuned from GPT-3. Trained on data up to September 2021. Reinforcement learning.

>> ○ **GPT-4:** Released in 2023. Fine-tuned from GPT-3. 1.7 trillion parameters. Image input (multimodal) and extended context length.

>> ○ **ChatGPT:** Released in 2023.

> ③ Comparison between BERT and GPT

<br>

|         | BERT       | GPT-4      |
|---------|------------|------------|
| Developer  | Google AI  | OpenAI     |
| Input Data | Considers both left and right context of input sequences | Considers input sequences sequentially from left to right |
| Parameters | 1.5 B      | 340 M (= 0.34 B) |
| Training Method | Transformer Encoder (MLM, NSP)        | Transformer Decoder (NWP)   |
| Training Data | 3 TB       | 45 TB      |
| Main Application Area | Text Understanding | Text Generation |

**Table. 1.** Comparison between BERT and GPT

<br>

> ④ Gopher

> ⑤ [Chinchilla](https://arxiv.org/abs/2203.15556)

> ⑥ Flan, PaLM, Flan-PaLM

> ⑦ OPT-IML

> ⑧ LLaMA, LLaMA2: Installed models. Developed by Meta. Specialized in question answering.

> ⑨ Alpaca

> ⑩ [XLNet](https://arxiv.org/abs/1906.08237), [T5](https://arxiv.org/abs/1910.10683), [CTRL](https://arxiv.org/abs/1909.05858), [BART](https://arxiv.org/abs/1910.13461): Specialized in text generation.

> ⑪ Gemma 2/3

>> ○ multi-query attention

>> ○ RoPE embedding

>> ○ GeGLU activation

>> ○ RMSNorm

> ⑫ Mistral

>> ○ group-query attention

>> ○ sliding window attention

>> ○ rolling buffer cache

>> ○ pre-fill and chunking

> ⑬ [ollama](https://github.com/ollama/ollama): Supports the following LLMs.

>> ○ Llama2 (7B)

>> ○ Mistral (7B)

>> ○ Dolphin Phi (2.7B)

>> ○ Phi-2 (2.7B)

>> ○ Neural Chat (7B)

>> ○ Starling (7B)

>> ○ Code Llama (7B)

>> ○ Llama2 Uncensored (7B)

>> ○ Llama2 (13B)

>> ○ Llama2 (70B)

>> ○ Orca Mini (3B)

>> ○ Vicuna (7B)

>> ○ LLaVA (7B)

> ⑭ [AlphaGeometry](https://jb243.github.io/pages/1213): Implements symbolic deduction to solve geometry problems at the International Mathematical Olympiad (IMO) level.

> ⑮ MiniLM

>> ○ Function that transforms arbitrary variable-length natural language sentences into 384-dimensional vectors by considering their meaning ([ref](https://github.com/portrai-io/CELLama/tree/main))

<br>

<br>

> ⑯ Other useful generative AI proprietary tools

>> ○ [GitHub Copilot](https://copilot.microsoft.com)

>> ○ [Perplexity AI](https://www.perplexity.ai)

>> ○ [Consensus](https://consensus.app)

>> ○ [Scite](https://scite.ai/assistant)

>> ○ SciSpace / typeset.io 

>> ○ Elicit.com 

>> ○ [Claude AI](https://claude.ai/new)

>> ○ Elicit AI

>> ○ Research Rabbit

>> ○ [Gemini](https://gemini.google.com/) 

>> ○ [Mistral AI](https://chat.mistral.ai/chat)

>> ○ Tabnine 

>> ○ CodiumAI

>> ○ AWS Code Whisperer

>> ○ Sourcegraph Cody

>> ○ NotebookLM

>> ○ Grok 

>> ○ DeepSeek 

>> ○ Qwen

>> ○ LSTM: Specialized in text classification.

>> ○ Falcon: Specialized in question answering.

>> ○ StableLM

>> ○ Rasa

<br>

<img width="1280" height="948" alt="image" src="https://github.com/user-attachments/assets/39d16c9e-1b5c-469c-851b-10d54a7d598f" />

**Figure 7.** Various LLM models

<br>

<br>

## **3\. Bioinformatics and Language Models**

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/44dcb503-67d5-409d-8382-45a3d1ac0083)

![image](https://github.com/JB243/jb243.github.io/assets/55747737/31c4f27a-63a8-4399-9ed9-c92a837ed67b)

**Figure 8.** Bioinformatics and language models 

<br>

⑴ BioBERT

⑵ BioNER

⑶ SQuAD

⑷ BioASQ

⑸ [PubMedGPT](https://www.mosaicml.com/blog/introducing-pubmed-gpt) (BioMedLM)

⑹ BioGPT 

⑺ [scBERT](https://www.nature.com/articles/s42256-022-00534-z)

⑻ GPT-Neo

⑼ PubMedBERT

⑽ BioLinkBERT

⑾ DRAGON

⑿ BioMedLM

⒀ [Med-PaLM](https://www.nature.com/articles/s41586-023-06291-2), [Med-PaLM M](https://ai.nejm.org/doi/full/10.1056/AIoa2300138)

⒁ [BioMedGPT](https://arxiv.org/abs/2305.17100)

⒂ [tGPT](https://www.sciencedirect.com/science/article/pii/S2589004223006132) 

⒃ [CellLM](https://arxiv.org/abs/2306.04371) 

⒄ [Geneformer](https://pubmed.ncbi.nlm.nih.gov/37258680/): Based on BERT. Uses a transformer encoder-based architecture. Utilized with a pretraining → finetuning approach. Zero-shot capabilities are practically useless.

⒅ [scGPT](https://github.com/bowang-lab/scGPT): Based on GPT. Uses a transformer decoder-based architecture. Utilized with a pretraining → finetuning approach. The zero-shot performance of the pretraining model is also quite excellent.

⒆ [scFoundation](https://www.biorxiv.org/content/10.1101/2023.05.29.542705v2) 

⒇ [SCimilarity](https://www.biorxiv.org/content/10.1101/2023.07.18.549537v1)

⒇ [CellPLM](https://www.biorxiv.org/content/10.1101/2023.10.03.560734v1.full.pdf) 

⒇ [Nicheformer](https://www.biorxiv.org/content/10.1101/2024.04.15.589472v1) 

⒇ [Evo2](https://arcinstitute.org/manuscripts/Evo2): An alignment-free foundation model built by compiling the DNA of almost all living organisms, totaling 8.8 trillion nucleotides.

⒇ GenePT

⒇ Concerto

⒇ scTrans

⒇ [scConcept](https://www.biorxiv.org/content/10.1101/2025.10.14.682419v1.full.pdf): Employs contrastive learning for model training instead of masked language modeling (MLM)

<br>

---

_Input**:** 2021-12-11 17:34_

_Modified**:** 2023-05-18 11:36_




