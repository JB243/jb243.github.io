## **Chapter 21.** Natural Language Processing (NLP) and Large Language Models (LLM)

Recommended Readings **:** 【Algorithms】 [Table of Algorithm Contents](https://jb243.github.io/pages/1278)

---

**1.** [Natural Language Processing](#1-natrual-language-processing-nlp)

**2.** [Large Language Models](#2-large-language-models-llm)

**3.** [Bioinformatics and Language Models](#3-bioinformatics-and-language-models)

---

**a.** [Prompt Engineering](https://jb243.github.io/pages/2317)

**b.** [Natural Language Processing and LLM Useful Function Collection](https://jb243.github.io/pages/2404)

**c.** [Research Topics related to LLM](https://jb243.github.io/pages/194)

---

<br>

## **1\. Natural Language Processing** (NLP)

⑴ Definition **:** AI models based on text

⑵ Text Preprocessing **:** Preprocessing to make unstructured text recognizable to computers

> ① Tokenization

>> ○ Dividing sentences or corpora into minimum meaning units, tokens, for computer recognition

>> ○ **English** **:** Mainly divided by spaces

>>> ○ 75 English words ≃ 100 tokens

>>> ○ Example **:** I / ate / noodle / very / deliciously

>> ○ Example **:** [OpenAI Tokenizer](https://platform.openai.com/tokenizer)

> ② Part-of-Speech Tagging (POS tagging)

>> ○ Technique to tag the parts of speech of morphemes

> ③ Lemmatization

>> ○ Technique to find lemmas (base words) from words

>> ○ Example **:** am, are, is → be

> ④ Stemming

>> ○ Technique to obtain stems by removing prefixes and suffixes from words

> ⑤ Stopword Removal

>> ○ Technique to handle words that contribute little to actual meaning analysis, such as particles and suffixes

⑶ Text Mining 

> ① Topic Modeling

>> ○ One of the statistical models in the fields of machine learning and natural language processing used to discover abstract topics, referred to as 'topics', within a collection of documents.

>> ○ Used to uncover the hidden meaning structure in the text body.

> ② Word Cloud

>> ○ A visualization technique that uses natural language processing to simply count and visualize people's interests or frequency.

> ③ Social Network Analysis (SNA)

>> ○ An analytical technique for analyzing and visualizing the network characteristics and structure among people within a group.

> ④ TF-IDF (Term Frequency-Inverse Document Frequency)

>> ○ A technique used to extract how important a word is within a specific document, in a collection of multiple documents.

⑷ Types

> ① **Type 1:** Latent Semantic Analysis (LSA)

> ② **Type 2:** Probabilistic Latent Semantic Analysis (PLSA)

> ③ **Type 3:** Latent Dirichlet Allocation (LDA): Generative probabilistic models. Can be used even for deconvolution without reference ([ref](https://www.nature.com/articles/s41467-022-30033-z#Sec11))

⑸ Evaluation of Language Models

> ① Parallel text datasets **:** Canadian Parliament (English ← Spanish), European Parliament (multiple languages ​​supported)

> ② SNS

> ③ Bleu score ([ref](https://dl.acm.org/doi/10.3115/1073083.1073135))

> ④ perplexity

<br>

<br>

## **2\. Large Language Models** (LLM)

⑴ Definition **:** Natural language processing models with billions of parameters

> ① Counting parameters method: [here](https://jb243.github.io/pages/2152#:~:text=%E2%97%8B-,Counting%20the%20Number%20of%20Parameters,-%3D%20Goal%20of%20the)

> ② Terms 1: Meaning of 7B, 13B, 30B, 65B **:** The number of parameters in natural language models is 7 billion, 13 billion, 30 billion, and 65 billion

> ③ Term 2: Token **:** The unit of text processed by the model

>> ○ Word-level tokenization **:** [**ChatGPT**, **is**, **an**, **AI**, **language**, **model**, **.**]

>> ○ Subword-level tokenization **:** [**Chat**, **G**, **PT**, **is**, **an**, **AI**, **language**, **model**, **.**]

>> ○ Character-level tokenization **:** [**C**, **h**, **a**, **t**, **G**, **P**, **T**, **i**, **s**, **a**, **n**, **A**, **I**, **l**, **a**, **n**, **g**, **u**, **a**, **g**, **e**, **m**, **o**, **d**, **e**, **l**]

> ④ Term 3: Meaning of 0-shot, 1-shot, etc. **:** The number of example inputs given per task

>> ○ 0-shot prompting

<br>

```python
Q: <Question>?
A:
```

<br>

>> ○ Few-shot prompting

<br>

```python
Q: <Question>?
A: <Answer>
Q: <Question>?
A: <Answer>
Q: <Question>?
A: <Answer>
Q: <Question>?
A:
```

<br>

⑵ **Transformer**

> ① **Stage 1.** Transformer Encoder: Converts the input sequence into a higher-level representation

>> ○ Training Methods: NSP, MLM

>>> ○ Next Sentence Prediction (NSP): Composes pairs of sentences A and B from the input data, and determines whether B follows A

>>>> ○ First Sentence: "The quick brown fox jumps over the lazy dog."

>>>> ○ Second Sentence: "The dog is not amused."

>>>> ○ Prediction Result: Whether the second sentence follows the first sentence (e.g., "No")

>>> ○ Masked Language Modeling (MLM): Masks some words in the sentence and predicts the masked words

>> ○ **1-1.** Self-Attention Mechanism

>>> ○ **1-1-1.** Splits the sentence into multiple tokens

>>> ○ **1-1-2.** For each pair of tokens in the input sequence, generates "query", "key", and "value" to calculate the attention score, which indicates how much each token should attend to other tokens

<br>

![image](https://github.com/user-attachments/assets/ee5afe38-a592-49e5-9e7d-1e0fca8a975b)

**Figure 1.** Attention score of a pair of tokens

<br>

>>> ○ **1-1-3.** Each token (word) within the encoder learns the relationships with all other tokens

>>> ○ **1-1-4.** Token Embedding: Calculates the weighted sum of each token based on the attention score to transform each token into a new representation. When treating an entire sentence as a token, this is referred to as sentence embedding

>>> ○ **1-1-5.** Positional Encoding: As the transformer cannot directly understand the order of the input words, positional encoding adds positional information for each word. This allows the model to understand the order within the sequence. The resulting embedding is called positional embedding

>>> ○ **1-1-6.** When there are multiple attention heads, it is called multi-head attention

>> ○ **1-2.** FFN (Feed-Forward Neural Network): Refines the representations generated through self-attention

>>> ○ Individually tunes each token embedding, using non-linear activation functions

>> ○ **1-3.** Add & Norm (LayerNorm): Performs layer normalization after self-attention and FFN

>> ○ **1-4.** The structure of "self-attention → Add & Norm → FFN → Add & Norm" is repeated across several layers

>>> ○ **Type 1.** Initial Layers:

>>>> ○ Attention heads in each layer have significant variance in focus areas within the input features

>>>> ○ During this stage, broad exploration helps to distinguish between important and less important information

>>> ○ **Type 2.** Intermediate Layers:

>>>> ○ Attention heads in each layer consistently focus on lower-ranked information

>>>> ○ Identifies noise inherent in the data and understands details

>>> ○ **Type 3.** Final Layers:

>>>> ○ Attention heads in each layer consistently focus on higher-ranked information

>>>> ○ Extracts critical information and contributes to making final decisions

> ② **Stage 2.** Transformer Decoder: Takes the representation generated by the encoder and ultimately generates the desired output

>> ○ Training Method: NWP

<br>

![image](https://github.com/user-attachments/assets/c29c0e0d-43a6-4fc5-9037-dcbdf925790a)

**Figure 2.** Defining Transformer Decoder Problems

<br>

>>> ○ Next Word Prediction (NWP): The task of predicting the next word within a given context

>> ○ **2-1.** Masked Self-Attention Mechanism

>>> ○ **2-1-1.** The decoder refers to the token embeddings generated by the encoder

>>> ○ **2-1-2.** Each token within the decoder refers only to the previous tokens to predict the next token: Applies masking to ensure the model does not see future information. The figure below shows that each token is embedded into vector representations

<br>

![image](https://github.com/user-attachments/assets/8d0f4dcd-6b22-4fd1-8bd7-3432d7c25cd3)

**Figure 3.** Token embedding results and next word prediction situation

<br>

>> ○ **2-2.** FFN (Feed-Forward Neural Network)

>>> ○ Used to refine the representations generated by the decoder to produce the final output

>>> ○ Similar to the encoder, the FFN non-linearly transforms each token embedding

<br>

![image](https://github.com/user-attachments/assets/d9782ec5-5b39-4a28-9441-4a89d1ecf43b)

**Figure 4.** attention multilayer perceptron

<br>

>> ○ **2-3.** Multilayer Decoder: Stacks multiple decoder layers to ultimately generate the output sequence

>>> ○ By serially attaching attention and multilayer perceptron, it is possible to continuously generate sentences

>>> ○ The decoder combines the information from the encoder with the information of the currently generated sequence to predict the next token

>>> ○ To date, the cleverest thinker of all time was ___ → undoubtedly

>>> ○ To date, the cleverest thinker of all time was undoubtedly ___ → Einstein

>>> ○ To date, the cleverest thinker of all time was undoubtedly Einstein, ___ → for

⑶ Important parameters

> ① temperature = 0: leading to very deterministic outputs 

> ② max_tokens API Call settings = 100: response is capped at 100 tokens

> ③ top_p = 1.0: uses all possible tokens for generating the response

> ④ frequency_penalty = 0.0: does not avoide repeating tokens more than it naturally would

> ⑤ presence_penalty = 0.0: no penalty applied for reusing tokens

⑷ Types

> ① [BERT](https://arxiv.org/abs/1810.04805), [RoBERTa](https://arxiv.org/abs/1907.11692), [ALBERT](https://arxiv.org/abs/1909.11942)

>> ○ Using a bidirectional transformer, which is advantageous for understanding text as it considers the context of both the left and right sides of the input sequence.

>> ○ The input for BERT is a maximum of 512 tokens, but the input can be increased by extending the positional embedding: 2<sup>n</sup> is commonly used as the maximum input.

>> ○ A function that loads BERT or BioBERT from Hugging Face to create an attention matrix for a given sentence.

<br>

```
import torch
from transformers import BertTokenizer, BertModel
import matplotlib.pyplot as plt
import seaborn as sns

# Load BERT model and tokenizer (OPTION 1)
'''
model_name = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertModel.from_pretrained(model_name, output_attentions=True)
'''

# Load BioBERT model and tokenizer (OPTION 2)
model_name = 'dmis-lab/biobert-base-cased-v1.1'
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertModel.from_pretrained(model_name, output_attentions=True)

# Input sentence
sentence = "Find diseases associated with glucose"

# Tokenization
inputs = tokenizer(sentence, return_tensors='pt')

# Calculate outputs and attention weights through the model
outputs = model(**inputs)
attentions = outputs.attentions  # These are the attention weights for each layer

# Visualize the attention weights from the first head of the first layer
attention = attentions[0][0][0].detach().numpy()

# Token list
tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])

# Visualize Attention weights
plt.figure(figsize=(10, 10))
sns.heatmap(attention, xticklabels=tokens, yticklabels=tokens, cmap='viridis')
plt.title('Attention Weights')
plt.show()
```

<br>

> ② [GPT](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf), [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), [GPT-3](https://arxiv.org/abs/2005.14165), GPT-J, GPT-Neo, GPT-3.5, GPT-4

>> ○ Focuses on predicting the next word by sequentially considering the context from left to right in the input sequence.

>> ○ Uses an autoregressive model.

> ③ Comparison between BERT and GPT

<br>

|         | BERT       | GPT-4      |
|---------|------------|------------|
| Developer  | Google AI  | OpenAI     |
| Input Data | Considers both left and right context of input sequences | Considers input sequences sequentially from left to right |
| Parameters | 1.5 B      | 340 M (= 0.34 B) |
| Training Method | NWP        | MLM, NSP   |
| Training Data | 3 TB       | 45 TB      |
| Main Application Area | Text Understanding | Text Generation |

**Table. 1.** Comparison between BERT and GPT

<br>

> ④ Gopher

> ⑤ [Chinchilla](https://arxiv.org/abs/2203.15556)

> ⑥ Flan, PaLM, Flan-PaLM

> ⑦ OPT-IML

> ⑧ LLaMA, **LLaMA2** **:** Installed models. Developed by Meta

> ⑨ Alpaca

> ⑩ [XLNet](https://arxiv.org/abs/1906.08237), [T5](https://arxiv.org/abs/1910.10683), [CTRL](https://arxiv.org/abs/1909.05858), [BART](https://arxiv.org/abs/1910.13461)

> ⑪ [ollama](https://github.com/ollama/ollama) **:** Supports the following LLMs.

>> ○ Llama2 (7B)

>> ○ Mistral (7B)

>> ○ Dolphin Phi (2.7B)

>> ○ Phi-2 (2.7B)

>> ○ Neural Chat (7B)

>> ○ Starling (7B)

>> ○ Code Llama (7B)

>> ○ Llama2 Uncensored (7B)

>> ○ Llama2 (13B)

>> ○ Llama2 (70B)

>> ○ Orca Mini (3B)

>> ○ Vicuna (7B)

>> ○ LLaVA (7B)

> ⑫ [AlphaGeometry](https://jb243.github.io/pages/1213): Implements symbolic deduction to solve geometry problems at the International Mathematical Olympiad (IMO) level.

> ⑬ MiniLM

>> ○ Function that transforms arbitrary variable-length natural language sentences into 384-dimensional vectors by considering their meaning ([ref](https://github.com/portrai-io/CELLama/tree/main))

<br>

<br>

> ⑭ Other useful generative AI proprietary tools

>> ○ [GitHub Copilot](https://copilot.microsoft.com)

>> ○ [Perplexity AI](https://www.perplexity.ai)

>> ○ [Consensus](https://consensus.app)

>> ○ [Scite](https://scite.ai/assistant)

>> ○ SciSpace / typeset.io 

>> ○ Elicit.com 

>> ○ [Claude AI](https://claude.ai/new)

>> ○ Elicit AI

>> ○ Research Rabbit

>> ○ [Gemini](https://gemini.google.com/) 

>> ○ [Mistral AI](https://chat.mistral.ai/chat)

>> ○ Tabnine 

>> ○ CodiumAI

>> ○ AWS Code Whisperer

>> ○ Sourcegraph Cody

>> ○ NotebookLM

<br>

<br>

## **3\. Bioinformatics and Language Models**

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/44dcb503-67d5-409d-8382-45a3d1ac0083)

![image](https://github.com/JB243/jb243.github.io/assets/55747737/31c4f27a-63a8-4399-9ed9-c92a837ed67b)

<br>

⑴ BioBERT

⑵ BioNER

⑶ SQuAD

⑷ BioASQ

⑸ [PubMedGPT](https://www.mosaicml.com/blog/introducing-pubmed-gpt) (BioMedLM)

⑹ BioGPT 

⑺ [scBERT](https://www.nature.com/articles/s42256-022-00534-z)

⑻ GPT-Neo

⑼ PubMedBERT

⑽ BioLinkBERT

⑾ DRAGON

⑿ BioMedLM

⒀ [Med-PaLM](https://www.nature.com/articles/s41586-023-06291-2), [Med-PaLM M](https://ai.nejm.org/doi/full/10.1056/AIoa2300138)

⒁ [BioMedGPT](https://arxiv.org/abs/2305.17100)

⒂ [tGPT](https://www.sciencedirect.com/science/article/pii/S2589004223006132) 

⒃ [CellLM](https://arxiv.org/abs/2306.04371) 

⒄ [Geneformer](https://pubmed.ncbi.nlm.nih.gov/37258680/): Based on BERT. Uses a transformer encoder-based architecture. Utilized with a pretraining → finetuning approach. Zero-shot capabilities are practically useless.

⒅ [scGPT](https://github.com/bowang-lab/scGPT): Based on GPT. Uses a transformer decoder-based architecture. Utilized with a pretraining → finetuning approach. The zero-shot performance of the pretraining model is also quite excellent.

⒆ [scFoundation](https://www.biorxiv.org/content/10.1101/2023.05.29.542705v2) 

⒇ [SCimilarity](https://www.biorxiv.org/content/10.1101/2023.07.18.549537v1)

⒇ [CellPLM](https://www.biorxiv.org/content/10.1101/2023.10.03.560734v1.full.pdf) 

<br>

---

_Input**:** 2021-12-11 17:34_

_Modified**:** 2023-05-18 11:36_
