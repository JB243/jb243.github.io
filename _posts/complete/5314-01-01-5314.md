## **第 22 章. 图像生成模型**

推荐阅读：【算法】【算法索引】(https://jb243.github.io/pages/1278)

---

**1.** [DIP](#1-浸)

**2.** [视觉变压器](#2-vision-transformer)

**3.** [图像生成模型](#3-image-generative-model)

**4.** [视觉语言模型](#4-视觉语言模型)

**5.** [视频生成模型](#5-video-generative-model)

**6.** [可解释性模型](#6-可解释性-模型)

---

<br>

## **1.卷积神经网络 (CNN)**

⑴【CNN的概念】(https://jb243.github.io/pages/2152)

⑵ **示例1.** [DIP](https://jb243.github.io/pages/2164)（深度图像先验）：在没有训练数据的情况下将CNN架构过度拟合到输入图像以生成新图像

⑶ **示例2.** Detectron2：由Facebook AI开发。

⑷ **示例3.** OpenCV提供的函数 

<br>

<br>

## **2.计算机视觉基础模型**

⑴ 视觉转换器（ViT）

> ① ViT仅使用[transformer编码器](https://jb243.github.io/pages/325#:1.-,,-\(transformerencoder\))结构

> ② **步骤1.** 将图像分割成多个小patch，并将每个patch视为输入到transformer的token

> ③ **步骤2.** 使用变压器编码器嵌入每个补丁

> ④ **步骤3.** 就像在句子中嵌入单词并输出代表句子含义的句子嵌入一样，ViT 学习块之间的关系并输出代表整个图像的特征。

> ⑤ **限制：** self-attention的计算量与组成图像的patch数量的平方成正比，导致很难一次性输入高分辨率图像。

>> ○ **解决方案 1：** 将给定图像分成更小的补丁，并将 ViT 独立应用于每个补丁（例如，[iSTAR](https://www.nature.com/articles/s41587-023-02019-9)）。

>> ○ **解决方案 2：** 引入扩展的自注意力机制，例如使用 LongNet 等模型的扩张自注意力（例如，[Prov-GigaPath](https://www.nature.com/articles/s41586-024-07441-w)）。

⑵ 种类

> ① DINO（自蒸馏无标签）

> ② IBOT（使用在线分词器进行图像 BERT 预训练）

> ③ [BEiT](https://nate9389.tistory.com/325#:~:text=%E2%91%A0-,BERT,-%2C%20RoBERTa%2C)：采用BERT模型思想的ViT变体，与掩码语言建模类似地进行训练。

>> ○ [iSTAR](https://www.nature.com/articles/s41587-023-02019-9)：用于增强空间转录组学的分辨率。它利用使用 DINO 方法训练的基于 BEiT 的模型。

<br>

![图片](https://github.com/user-attachments/assets/f17eb8ba-feb8-4957-a564-3edae5a381c5)

**图 1.** iSTAR 中的数据准备步骤图

<br>

>>> ○ **步骤 1.** 将给定图像划分为 256 × 256 块。  

>>> ○ **步骤 2.** 将每个补丁进一步划分为 16 × 16 子补丁。  

>>> ○ **步骤3.** 对每个子补丁应用ViT（表示为f2）以获得384维向量。  

>>> ○ **步骤4.** 将384维向量聚合形成16 × 16 × 384数据结构，然后应用另一个ViT（表示为f1）以获得192维向量。  

>>> ○ **步骤 5.** 收集 192 维向量并应用 ViT（表示为 f0）。  

>>> ○ 特征提取和损失函数公式。

<br>

<img width="583" alt="스크린샷 2025-02-24 12 46 10" src="https://github.com/user-attachments/assets/ac4aeea6-b9ee-4e9a-9a9b-d9c5ff831c28" />

<br>

> ④ Swin Transformer：使用基于窗口的局部自注意力的 ViT 变体。

> ⑤ [CTransPath](https://www.sciencedirect.com/science/article/pii/S1361841522002043) **:** Wang 等人，医学图像分析 (2022)

> ⑥ [UNI](https://www.nature.com/articles/s41591-024-02857-3) **:** Chen et al., Nature Medicine (2024)> ⑦ [CONCH](https://www.nature.com/articles/s41591-024-02856-4)（从组织病理学说明中进行对比学习）**：** Lu et al., Nature Medicine (2024)

> ⑧ [Virchow](https://arxiv.org/abs/2309.07778) **:** Vorontsov 等人，arxiv (2023)

> ⑨ [RudolfV](https://arxiv.org/abs/2401.04079) **:** Dippel 等人，arxiv (2024)

> ⑩ [Campanella](https://arxiv.org/abs/2310.07033) **:** Campanella 等人，arxiv (2023)

> ⑪ [Prov-GigaPath](https://www.nature.com/articles/s41586-024-07441-w)：由微软发布，是一个在 170,000 张病理图像（13 亿图块）上训练的视觉基础模型（2024 年）。

> ⑫ 棱镜

<br>

<br>

## **3。图像生成模型** 

⑴ 类型

> ① DALL·E3（OpenAI）

> ② 中途

> ③ 稳定扩散

> ④ 索拉（OpenAI）

> ⑤ 视频法学硕士

<br>

<br>

## **4。视觉语言模型** 

⑴ 类型

> ① [稳定扩散](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb)：从自然语言生成数字图像的人工智能算法

> ② [MedGemma](https://github.com/google-health/medgemma) 

> ③ TITAN：数字病理学 

<br>

<br>

## **5。视频生成模型**

⑴ 类型

> ① XVFI：光流的一种。

> ② FILM(Frame Interpolation for Large Motion)：编码器+类似U-Net的解码器  

<br>

<br>

## **6。可解释性模型**

⑴ 类型

> ① LIME：基于局部近似（代理模型）的解释。可视化每个要素的本地贡献。可以解释任何计算机视觉模型。

> ② SHAP：使用博弈论 Shapley 值分解预测，得出每个特征的公平贡献。

<br>

---

_输入：2024.04.22 14:08_