## **第 21 章.** 自然语言处理 (NLP) 和大型语言模型 (LLM)

推荐阅读：【算法】【算法目录】(https://jb243.github.io/pages/1278)

---

**1.** [自然语言处理](#1-自然语言处理-nlp)

**2.** [大型语言模型](#2-large-language-models-llm)

**3.** [生物信息学和语言模型](#3-生物信息学和语言模型)

---

**a.** [提示工程](https://jb243.github.io/pages/2317)

**b.** 【自然语言处理与LLM实用函数合集】(https://jb243.github.io/pages/2404)

**c.** [LLM相关研究主题](https://jb243.github.io/pages/194)

---

<br>

## **1\.自然语言处理**（NLP）

⑴ 定义：基于文本的AI模型

⑵ 文本预处理：使非结构化文本能够被计算机识别的预处理

> ① 代币化

>> ○ 将句子或语料库划分为最小含义单位、标记，以供计算机识别

>> ○ **英文**：主要以空格分隔

>>> ○ 75 个英语单词 ≃ 100 个标记

>>> ○ 例如：我/吃了/面条/非常/美味

>> ○ 示例：[OpenAI Tokenizer](https://platform.openai.com/tokenizer)

> ② 词性标注（词性标注）

>> ○ 语素词性标注技术

> ③ 词形还原

>> ○ 从单词中查找引理（基本词）的技术

>> ○ 示例：am、are、is → be

> ④ 词干提取

>> ○ 去除单词前缀后缀获取词干的技术

> ⑤ 停用词去除

>> ○ 助词、后缀等对实际意义分析贡献不大的单词的处理技巧

⑶ 文本挖掘 

> ① 主题建模

>> ○ 机器学习和自然语言处理领域的统计模型之一，用于在文档集合中发现抽象主题（称为“主题”）。

>> ○ 用于揭示正文中隐藏的意义结构。

> ② 词云

>> ○ 一种可视化技术，使用自然语言处理来简单地计算和可视化人们的兴趣或频率。

> ③ 社会网络分析（SNA）

>> ○ 一种分析技术，用于分析和可视化群体内人员之间的网络特征和结构。

> ④ TF-IDF（词频-逆文档频率）

>> ○ 一种用于提取某个单词在特定文档（多个文档的集合中）中的重要性的技术。

⑷ 类型

> ① **类型 1.** 潜在语义分析 (LSA)

> ② **类型 2.** 概率潜在语义分析 (PLSA)

> ③ **类型 3.** 潜在狄利克雷分配 (LDA)：生成概率模型。甚至可以在没有参考的情况下用于反卷积（[ref](https://www.nature.com/articles/s41467-022-30033-z#Sec11)）

> ④ **类型4.** Word2Vec：Skip-gram 是Word2Vec 模型的一个组件。

⑸ 语言模型评估

> ①并行文本数据集：加拿大议会（英语←西班牙语）、欧洲议会（支持多种语言）

> ② 社交网络

> ③ Bleu 评分（[参考](https://dl.acm.org/doi/10.3115/1073083.1073135))

> ④ 困惑

<br>

<br>

## **2\.大型语言模型**（法学硕士）

⑴ 定义：数十亿参数的自然语言处理模型

> ① 计数参数方法：【此处】(https://jb243.github.io/pages/2152#:~:text=%E2%97%8B-,计数%20the%20Number%20of%20Parameters,-%3D%20Goal%20of%20the)

> ② 术语1：7B、13B、30B、65B的含义：自然语言模型中的参数数量为70亿、130亿、300亿、650亿

> ③第2项：Token：模型处理的文本单位

>> ○ 词级标记化：[**ChatGPT**、**is**、**an**、**AI**、**语言**、**模型**、**.**]

>> ○ 子词级标记化：[**Chat**, **G**, **PT**, **is**, **an**, **AI**, **language**, **model**, **.**]>> ○ 字符级标记化：[**C**, **h**, **a**, **t**, **G**, **P**, **T**, **i**, **s**, **a**, **n**, **A**, **I**, **l**, **a**, **n**, **g**, **u**, **a**, **g**, **e**, **m**, **o**, **d**, **e**, **l**]

> ④ 第 3 项：0-shot、1-shot 等的含义：每个任务给出的示例输入的数量

>> ○ 0次提示

<br>

__受保护_0__

<br>

>> ○ 少镜头提示

<br>

__受保护_1__

<br>

⑵ **变压器**

> ① **阶段1.** Transformer Encoder：将输入序列转换为更高级别的表示

>> ○ 培训方式：NSP、MLM

>>> ○ 下一个句子预测（NSP）：根据输入数据组成句子 A 和 B 对，并确定 B 是否跟随 A

>>>> ○ 第一句话：“敏捷的棕色狐狸跳过了懒狗。”

>>>> ○ 第二句：“狗不开心。”

>>>> ○ 预测结果：第二句话是否在第一句话之后（例如“否”）

>>> ○ 屏蔽语言模型（MLM）：屏蔽句子中的一些单词并预测屏蔽的单词

>>> ○ 在像 BERT 这样的 Transformer 中，编码器是经过训练的。

>> ○ **1-1.** 自注意力机制

>>> ○ **1-1-1.** 将句子拆分为多个标记

>>> ○ **1-1-2.** 对于输入序列中的每一对 token，生成“query”、“key”和“value”来计算注意力分数，该分数表示每个 token 应该关注其他 token 的程度

<br>

![图片](https://github.com/user-attachments/assets/ee5afe38-a592-49e5-9e7d-1e0fca8a975b)

**图 1.** 一对 token 的注意力得分

<br>

>>> ○ **1-1-3.** 位置编码：变压器无法直接知道输入单词的顺序，因此在每个单词的位置信息中添加位置编码。 

>>>> ○ 这允许模型理解序列的顺序信息。

>>>> ○ 例如，用正弦和余弦函数定义时，值越接近，内积就越大，这样我们就可以理解邻接关系。 

>>>> ○ 如果输入没有固有顺序，我们可以从中省略位置编码。

>>> ○ **1-1-4.** 编码器中的每个标记（单词）都会学习与所有其他标记的关系。

>>> ○ **1-1-5.** Token Embedding：根据注意力分数，计算每个token的加权和，并将每个token转换为新的表示。用位置编码嵌入的结果称为位置嵌入。

>>> ○ **1-1-6.** 当有多个注意力头时，称为多头注意力。

>> ○ **1-2.** FFN（前馈神经网络）：细化通过自注意力生成的表示

>>> ○ 使用非线性激活函数单独调整每个标记嵌入

>> ○ **1-3.** Add & Norm (LayerNorm)：在self-attention和FFN之后进行层归一化

>> ○ **1-4.** “self-attention → Add & Norm → FFN → Add & Norm”的结构在多个层中重复

>>> ○ 这逐渐将每个标记嵌入转变为更具上下文感知的向量。

>>> ○ **类型 1.** 初始层：

>>>> ○ 每层的注意力头在输入特征内的焦点区域上有显着差异

>>>> ○ 在此阶段，广泛的探索有助于区分重要和不太重要的信息

>>> ○ **类型 2.** 中间层：

>>>> ○ 每层的注意力头始终关注排名较低的信息

>>>> ○ 识别数据中固有的噪声并理解细节

>>> ○ **类型 3.** 最终层：

>>>> ○ 每层的注意力头始终关注排名较高的信息

>>>> ○ 提取关键信息并有助于做出最终决策>> ○ **1-5.** 句子嵌入：最终，它综合每个标记嵌入，将其转换为表示句子整体含义的单个向量。 Transformer 解码器不将 Transformer 编码器创建的句子嵌入作为输入。相反，Transformer 解码器从编码器接收上下文化的令牌嵌入作为输入。

> ② **阶段2.** Transformer Decoder：采用编码器生成的表示并最终生成所需的输出

>> ○ 训练方法：NWP

<br>

![图片](https://github.com/user-attachments/assets/c29c0e0d-43a6-4fc5-9037-dcbdf925790a)

**图 2.** 定义 Transformer 解码器问题

<br>

>>> ○ 下一个单词预测 (NWP)：在给定上下文中预测下一个单词的任务

>>> ○ 在像 GPT 这样的 Transformer 中，解码器是经过训练的。

>> ○ **2-1.** 屏蔽自注意力机制

>>> ○ **2-1-1.** 解码器指的是编码器生成的 token 嵌入

>>> ○ **2-1-2.** 解码器中的每个标记仅引用先前的标记来预测下一个标记：应用屏蔽以确保模型看不到未来的信息。下图展示了每个token被嵌入到向量表示中

<br>

![图片](https://github.com/user-attachments/assets/8d0f4dcd-6b22-4fd1-8bd7-3432d7c25cd3)

**图3.** Token嵌入结果和下一个词预测情况

<br>

>> ○ **2-2.** FFN（前馈神经网络）

>>> ○ 用于细化解码器生成的表示以产生最终输出

>>> ○ 与编码器类似，FFN 对每个 token 嵌入进行非线性变换

<br>

![图片](https://github.com/user-attachments/assets/d9782ec5-5b39-4a28-9441-4a89d1ecf43b)

**图 4.** 注意力多层感知器

<br>

>> ○ **2-3.** 多层解码器：堆叠多个解码器层，最终生成输出序列

>>> ○ 通过串联注意力和多层感知器，可以连续生成句子

>>> ○ 解码器将来自编码器的信息与当前生成序列的信息相结合来预测下一个令牌

>>> ○ 迄今为止，有史以来最聪明的思想家是 ___ → 毫无疑问

>>> ○ 迄今为止，有史以来最聪明的思想家无疑是 ___ → 爱因斯坦

>>> ○ 迄今为止，有史以来最聪明的思想家无疑是爱因斯坦，___ →

<br>

![图片](https://github.com/user-attachments/assets/3a93a276-8e3f-45a4-9306-14403d3ebbff)

**图 5.** 多个解码器

<br>

⑶ 重要参数

> ① 温度 = 0：导致非常确定的输出 

> ② max_tokens API 调用设置 = 100：响应上限为 100 个令牌

> ③ top_p = 1.0：使用所有可能的令牌来生成响应

> ④Frequency_penalty = 0.0：不会比自然情况下更多地避免重复标记

> ⑤ Presence_penalty = 0.0：不因重复使用代币而受到惩罚

⑷ 类型

> ① [BERT](https://arxiv.org/abs/1810.04805)、[RoBERTa](https://arxiv.org/abs/1907.11692)、[ALBERT](https://arxiv.org/abs/1909.11942)

>> ○ 使用双向转换器，这有利于理解文本，因为它考虑了输入序列左侧和右侧的上下文。

>> ○ BERT 的输入最多为 512 个 token，但可以通过扩展位置嵌入来增加输入：通常使用 2<sup>n</sup> 作为最大输入。

>> ○ 从 Hugging Face 加载 BERT 或 BioBERT 来为给定句子创建注意力矩阵的函数。

<br>

__受保护_2__

<br>> ② [GPT](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understand_paper.pdf), [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), [GPT-3](https://arxiv.org/abs/2005.14165)、GPT-J、GPT-Neo、GPT-3.5、GPT-4

>> ○ 重点是通过按顺序考虑输入序列中从左到右的上下文来预测下一个单词。

>> ○ 使用自回归模型。

<br>

<img width="396" alt="스크린샷 2024-10-28 오후 10 13 49" src="https://github.com/user-attachments/assets/dcda95fa-51a9-408f-9a49-e5d3eb5a8c31">

<br>

>> ○ **GPT-1：** 2018 年发布。1.5 亿个参数。使用约 4 GB 的网络数据（语料库）进行训练。介绍了迁移学习和微调的概念。

>> ○ **GPT-2：** 2019 年发布。1.17 亿至 15.42 亿个参数。使用 GPT-1 数据和附加数据 (40 GB) 进行训练。引入了元学习的概念。

>> ○ **GPT-3：** 2020 年发布。17.5 亿个参数。使用 570 GB 数据进行训练。提高了对道德问题的认识。在 Microsoft Azure 超级计算机的规模（285,000 个 CPU、10,000 个 GPU、400G 网络）上进行训练。

>> ○ **GPT-3.5：** 于 2022 年发布。在 GPT-3 的基础上进行了微调。使用截至 2021 年 9 月的数据进行培训。强化学习。

>> ○ **GPT-4：** 于 2023 年发布。在 GPT-3 的基础上进行了微调。 1.7 万亿个参数。图像输入（多模式）和扩展上下文长度。

>> ○ **ChatGPT：** 2023 年发布。

> ③ BERT 与 GPT 对比

<br>

|         |伯特 | GPT-4 |
|--------|------------|------------|
|开发商|谷歌人工智能 |开放人工智能 |
|输入数据|考虑输入序列的左右上下文 |从左到右顺序考虑输入序列 |
|参数| 1.5 乙 | 340 M（= 0.34 B）|
|训练方法|变压器编码器（MLM、NSP）|变压器解码器 (NWP) |
|培训数据| 3TB | 45TB |
|主要应用领域 |文本理解 |文本生成|

**桌子。 1.** BERT与GPT的比较

<br>

> ④ 地鼠

> ⑤ 【龙猫】(https://arxiv.org/abs/2203.15556)

> ⑥ 弗兰、PaLM、弗兰-PaLM

> ⑦ OPT-IML

> ⑧ LLaMA、LLaMA2：已安装型号。由元开发。专门从事问答。

> ⑨ 羊驼毛

> ⑩ [XLNet](https://arxiv.org/abs/1906.08237)、[T5](https://arxiv.org/abs/1910.10683)、[CTRL](https://arxiv.org/abs/1909.05858)、[BART](https://arxiv.org/abs/1910.13461):专门从事文本生成。

> ⑪ 杰玛 2/3

>> ○ 多查询关注

>> ○ 绳索埋置

>> ○ GeGLU 激活

>> ○ RMSNorm

> ⑫ 米斯特拉尔

>> ○ 群组查询关注

>> ○ 推拉窗注意事项

>> ○ 滚动缓冲区高速缓存

>> ○ 预填充和分块

> ⑬ [ollama](https://github.com/ollama/ollama)：支持以下法学硕士。

>> ○ 骆驼2 (7B)

>> ○ 米斯特拉尔 (7B)

>> ○ 海豚披 (2.7B)

>> ○ Φ2 (2.7B)

>> ○ 神经聊天 (7B)

>> ○ 椋鸟 (7B)

>> ○ 代码羊驼 (7B)

>> ○ Llama2 未经审查 (7B)

>> ○ 骆驼2 (13B)

>> ○ 骆驼2 (70B)

>> ○ 虎鲸迷你 (3B)

>> ○ 骆驼毛 (7B)

>> ○ 拉瓦 (7B)

> ⑭ [AlphaGeometry](https://jb243.github.io/pages/1213)：实现符号演绎来解决国际数学奥林匹克（IMO）级别的几何问题。

> ⑮ 迷你LM

>> ○ 通过考虑其含义将任意可变长度自然语言句子转换为 384 维向量的函数 ([ref](https://github.com/portrai-io/CELLama/tree/main))

<br>

<br>

> ⑯ 其他有用的生成式 AI 专有工具

>> ○ [GitHub Copilot](https://copilot.microsoft.com)

>> ○ [Perplexity AI](https://www.perplexity.ai)

>> ○ 【共识】(https://consensus.app)

>> ○ [Scite](https://scite.ai/assistant)

>> ○ SciSpace / typeset.io 

>> ○ Elicit.com 

>> ○ [克劳德·艾](https://claude.ai/new)>> ○ 引出人工智能

>> ○ 研究兔

>> ○ [双子座](https://gemini.google.com/) 

>> ○ [Mistral AI](https://chat.mistral.ai/chat)

>> ○ 塔布宁 

>> ○ CodiumAI

>> ○ AWS 代码耳语者

>> ○ Sourcegraph 科迪

>> ○ 笔记本LM

>> ○ 格洛克 

>> ○ 深思 

>> ○ 奎文

>> ○ LSTM：专门用于文本分类。

>> ○ Falcon：专门回答问题。

>> ○ 稳定LM

>> ○ 拉莎

<br>

<img width="1280" height="948" alt="image" src="https://github.com/user-attachments/assets/39d16c9e-1b5c-469c-851b-10d54a7d598f" />

**图 6.** 各种 LLM 模型

<br>

<br>

## **3\.生物信息学和语言模型**

<br>

![图片](https://github.com/JB243/jb243.github.io/assets/55747737/44dcb503-67d5-409d-8382-45a3d1ac0083)

![图片](https://github.com/JB243/jb243.github.io/assets/55747737/31c4f27a-63a8-4399-9ed9-c92a837ed67b)

**图 7.** 生物信息学和语言模型 

<br>

⑴BioBERT

⑵ 比奥纳

⑶ 小队

⑷ 生物ASQ

⑸ [PubMedGPT](https://www.mosaicml.com/blog/introducing-pubmed-gpt) (BioMedLM)

⑹ 生物GPT 

⑺ [scBERT](https://www.nature.com/articles/s42256-022-00534-z)

⑻ GPT-Neo

⑼ PubMedBERT

⑽ BioLinkBERT

⑾ 龙

⑿ 生物医学LM

⒀ [Med-PaLM](https://www.nature.com/articles/s41586-023-06291-2), [Med-PaLM M](https://ai.nejm.org/doi/full/10.1056/AIoa2300138)

⒁ [BioMedGPT](https://arxiv.org/abs/2305.17100)

⒂ [tGPT](https://www.sciencedirect.com/science/article/pii/S2589004223006132) 

⒃ [CellLM](https://arxiv.org/abs/2306.04371) 

⒄ [Geneformer](https://pubmed.ncbi.nlm.nih.gov/37258680/)：基于 BERT。使用基于转换器编码器的架构。采用预训练 → 微调方法。零射击能力实际上毫无用处。

⒅ [scGPT](https://github.com/bowang-lab/scGPT)：基于GPT。使用基于转换器解码器的架构。采用预训练 → 微调方法。预训练模型的零样本性能也相当出色。

⒆ [scFoundation](https://www.biorxiv.org/content/10.1101/2023.05.29.542705v2) 

⒇ [SCimilarity](https://www.biorxiv.org/content/10.1101/2023.07.18.549537v1)

⒇ [CellPLM](https://www.biorxiv.org/content/10.1101/2023.10.03.560734v1.full.pdf) 

⒇ [Nicheformer](https://www.biorxiv.org/content/10.1101/2024.04.15.589472v1) 

⒇ [Evo2](https://arcinstitute.org/manuscripts/Evo2)：通过编译几乎所有生物体的 DNA 构建的免对齐基础模型，总计 8.8 万亿个核苷酸。

⒇ 基因PT

⒇ 协奏曲

⒇ 传输

⒇ [scConcept](https://www.biorxiv.org/content/10.1101/2025.10.14.682419v1.full.pdf)：采用对比学习进行模型训练，而不是掩码语言建模（MLM）

<br>

---

_输入**：** 2021-12-11 17:34_

_修改**：** 2023-05-18 11:36_