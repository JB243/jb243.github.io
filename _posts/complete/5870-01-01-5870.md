## **第 17 章 RNN 算法**

推荐帖子：【算法】【算法索引】(https://jb243.github.io/pages/1278)

---

**1.** [概述](#1-概述)

**2.** [注意事项](#2-注意事项)

**3.** **类型1.** [RNN之前的回归模型](#3-type-1-regression-models-before-rnn)

**4.** **类型 2.** [LSTM](#4-type-2-lstm)

**5.** **类型 3.** [GRU](#5-type-3-gru)

---

<br>

## **1.概述**

⑴ 定义

> ① RNN（循环神经网络）：在由输入层、隐藏层和输出层组成的多层感知器内，隐藏层包含循环神经网络结构的算法。

> ② 可表示为：h<sub>t</sub> = tanh(W<sub>x</sub>x<sub>t</sub> + W<sub>h</sub>h<sub>t-1</sub> + b)

> ③ 优势

>> ○ 模型比较简单，网络结构无论序列长度如何都可以接受输入和输出。

>> ○ 结构可根据需要进行多种灵活设计。

> ④ 缺点

>> ○ 计算速度较慢

>> ○ 长期依赖问题

>> ○ 梯度消失问题

⑵ 结构

<br>

![图片](https://github.com/user-attachments/assets/42d21caf-6606-4840-a9dc-724626d92c63)

**图 1.** RNN 算法的结构

<br>

3 应用领域

> ① 自然语言：使用文本中的前后单词

> ② 语音信号

> ③ 时间序列数据：使用过去和未来值以及当前值

>> 通过观察数据随时间的变化来预测未来虽然具有挑战性，但非常重要。此类问题也可以使用深度学习算法来解决。通过提供多个训练数据集，其中 x(t), x(t - τ), ..., x(t - kτ) 为输入值，x(t + τ) 为目标值，预测值 y 可以表示为 y = f(x(t), x(t - τ), ..., x(t - kτ))。通过深度学习算法，可以创建表示该函数 f 的网络。不仅可以输出单个预测值，还可以生成多个预测值。在这种情况下， **y** = **F**(x(t), x(t - τ), ..., x(t - kτ)) = (x<sub>1</sub>*, ... x<sub>m</sub>*) 是预测值，G(t + τ) = (x<sub>1</sub>, ..., x<sub>m</sub>) 是目标值。

<br>

<br>

## **2.注意事项**

⑴ **长期依赖问题**

> ① 随着数据点从当前时刻回溯到更远的时间，处理上下文变得更加困难。

⑵ **梯度消失（GV）**

> ① 随着层数的增加，反向传播过程中，梯度值越靠近输入层越趋于零，导致参数无法有效更新。

> ② 由于多层网络性能有限、计算机速度缓慢，神经网络研究陷入停滞。

> ③ **解决方案：** 使用ReLU等激活函数代替sigmoid。

>> ○ S 型函数：sig(t) = 1 / (1 + exp(-t))

>>> ○ t=0 时其导数最大，但最大值为 0.25，小于 1。

>>> ○ 因此，通过跨层的重复计算，梯度会趋于缩小并收敛于0。

>> ○ ReLU: ReLU(t) = max(0, t)

>>> ○ 有助于解决梯度消失问题

>>> ○ 计算速度更快

>>> ○ 因为它不输出负值，所以可能导致神经元失活（“dying ReLU”问题）

>>> ○ 深度学习中常用的首选激活函数

⑶ **梯度爆炸（GE）**

> ① 梯度越来越大，导致权重更新到异常大值的问题。

> ② **梯度裁剪：** 一种对梯度值设置上限以防止梯度爆炸的技术。

>> ○ 【为什么梯度裁剪可以加速模型训练】(https://openreview.net/pdf?id=BJgnXpVYwS)

<br>

<br>

## **3。类型 1. RNN 之前的回归模型**⑴ 移动平均模型

<br>

<img width="430" alt="스크린샷 2025-02-12 오후 12 21 30" src="https://github.com/user-attachments/assets/da091936-02de-494f-9db9-697884c1664b" />

<br>

⑵自回归模型

<br>

<img width="405" alt="스크린샷 2025-02-12 오후 12 21 47" src="https://github.com/user-attachments/assets/611442b9-58aa-4c12-a617-15df974d58d4" />

<br>

⑶ ARMA（自回归移动平均线）

<br>

<img width="300" alt="스크린샷 2025-02-12 오후 12 22 01" src="https://github.com/user-attachments/assets/4063e831-e4d8-4009-93ec-56637bcbe2d9" />

<br>

⑷ ARMAX（带有外部输入的自回归移动平均线）：关于外部变量x，

<br>

<img width="300" alt="스크린샷 2025-02-12 오후 12 22 15" src="https://github.com/user-attachments/assets/787b9998-3ab0-4c6b-ad38-7d0bf8922f26" />

<br>

⑸ 问题：线性假设，缺乏应使用多少过去数据的标准。

<br>

<br>

## **4。类型 2.LSTM**

⑴ LSTM（长短期记忆）：一种神经网络算法，旨在解决RNN的长期依赖问题。

⑵结构：LSTM由输入门、遗忘门、输出门组成。

> ① **状态 1.** 单元状态：信息不变地传递。

> ② **状态2.**遗忘门：如果sigmoid输出为1，则信息被保留；如果为0，则被丢弃。

> ③ **状态3.** 输入门：决定哪些新信息将存储在单元状态中。

> ④ **状态 4.** 单元状态更新：更新单元。

> ⑤ **状态 5.** 输出门：决定输出。

<br>

<br>

## **5。类型 3.GRU**

⑴ GRU（Gated Recurrent Unit）：与LSTM类似，但结构更简单。

⑵ 结构

<br>

![图片](https://github.com/user-attachments/assets/edb46b1e-502c-43e8-8ea3-f2348b2415c5)

**图2.** GRU的结构

<img width="401" alt="스크린샷 2025-02-12 오후 12 23 15" src="https://github.com/user-attachments/assets/95ac509b-b599-4699-aa63-a43b1ec9cb36" />

<br>

> ① 当**r**<sub>t</sub>接近0时，中间存储单元忽略**h**<sub>t-1</sub>。

> ② 当 **z**<sub>t</sub> 接近 1 时，**h**<sub>t</sub> 忽略 **x**<sub>t</sub> 并尝试维持 **h**<sub>t-1</sub> 值。

> ③ 输入（文本）- 输出（情绪：正面、负面、中性）之间有隐藏层，模型的性能取决于这些隐藏层的结构。

<br>

---

输入：2023.06.27 00:35