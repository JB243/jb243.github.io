## **第 10 章深度学习概述**

推荐帖子：【算法】【算法索引】(https://jb243.github.io/pages/1278)

---

**1.** [概述](#1-概述)

**2.** [评估](#2-评估)

**3.** [历史](#3-历史)

---

<br>

## **1.概述**

⑴ 人工智能的含义

> ① **数据科学 ⊃ 人工智能 ⊃ 机器学习 ⊃ 人工神经网络 ⊃ 深度学习**

>> ○ 人工智能（AI）：一个广义的概念，指模仿人类思维的计算机智能。

>> ○ 机器学习：一种通过从数据中学习来创建适合解决问题的模型的技术。

>>> ○ 由 Arthur Samuel 于 1959 年首次定义。

>>> ○ 示例：[SVM](https://jb243.github.io/pages/2161)、[adaboost](https://jb243.github.io/pages/2161#:37.-,AdaBoost,-38)、ANN

>> ○ 人工神经网络 (ANN)：模仿人类神经网络的机器学习算法。

>>> ○ **优点 1.** 可以针对难以具体指定的任务进行培训。

>>> ○ **优点2.** 可以压缩大量信息（[Autoencoder](https://jb243.github.io/pages/956)）。

>> ○ 深度学习：具有深层多层结构的人工神经网络。

>>> ○ 与其他机器学习技术的不同之处在于，特征是由计算机而不是人类定义的。

>>> ○ 最近在各个领域表现出了比大多数传统机器学习方法更优越的性能。

> ② 强人工智能 vs. 弱人工智能

> ③ 图灵测试

>> ○ 如果人类与计算机交谈 5 分钟而不意识到它是一台机器，则计算机通过了测试。

>> ○ 由阿兰·图灵于 1950 年发明。

⑵ 深度学习的定义

> ① 数学公式：DNN（深度神经网络）可以用以下公式表示：

>> ○ y<sub>n</sub> = f(x<sub>n-1</sub> · w<sub>i</sub>) + b

>> ○ y<sub>n</sub>：第n个隐藏层神经元的输出向量。

>> ○ x<sub>n-1</sub>：第（n-1）隐藏层神经元的输出向量。

>> ○ w<sub>i</sub>: 神经元 i 的权重向量。

>> ○ b：偏差向量。

>> ○ f：激活函数。

> ② 维数：指向量的大小。例如，**v** = (a<sub>1</sub>, ..., a<sub>n</sub>) 的维度为 n。

> ③ 输入：表示为作为输入提供的数据的输入向量，表示为 **x** = (x<sub>1</sub>, ···, x<sub>ℓ</sub>)。

> ④ 权重：连接节点 i 和节点 j 的权重，表示为 ω<sub>ij</sub>。

>> ○ 代表神经网络中大脑的突触； ω<sub>ij</sub> 构成矩阵 **W**。

> ⑤ 输出：**y** = (y<sub>1</sub>, ···, y<sub>n</sub>) = **y**(**x**, **W**)。

> ⑥ 目标：**t** = (t<sub>1</sub>, ···, t<sub>n</sub>)，对于监督学习算法至关重要，因为它们需要真实值进行训练。

> ⑦ 激活函数g(·)：根据加权输入值确定神经元激活。

>> ○ 阈值函数。

> ⑧ 误差：通过误差计算算法的输出（**y**）与实际目标值（**t**）之间的距离。

> ⑨ 参数和超参数

>> ○ 参数是测量的或从数据中学习的。

>> ○ 参数通常存储为学习模型的一部分。

>> ○ 超参数是为学习而任意设置的值。

>> ○ 超参数的示例包括学习率、决策树中的树深度以及神经网络中的隐藏层数量。

⑶ DNN 的特点

> ① 非线性变换的近似。

> ② 各种隐藏层。

> ③ [深度学习前馈](https://blog.naver.com/beyondlegend/221373971859)：操作从输入层→隐藏层→输出层进行，没有后向反馈循环。

> ④ 反向学习：网络权重按照相反的顺序更新，从输出层→隐藏层。

⑷ 特征选择> ① 概述：更少的特征降低了计算复杂度，不太可能过拟合，但更复杂的任务很难处理

> ② **方法1.** 使用领域知识进行选择。

> ③ **方法2.** 通过特征组合减少：PCA等。

> ④ **方法 3.** 通过标准/排名系统进行选择。

>> ○ 基于过滤器的方法：根据统计标准选择特征。快速且简单，有利于大型数据集。

>> ○ 基于Wrapper的方法：通过多次训练模型找到最优的特征组合。计算成本昂贵且容易过度拟合。

> ⑤ **方法4.** 将分类变量转换为连续变量的方法

>> ○ 分数函数 (SF) 

>> ○ 该死的

>> ○ MuProp

>> ○ 直通式 (ST)

>> ○ 斜坡退火 ST

>> ○ Gumbel-Softmax 

>> ○ ST Gumbel-Softmax 

⑸ 示例：将深度学习模型应用于 MNIST（手写数字）数据。

<br>

__受保护_0__

<br>

⑹ 类型

> ① **类型1.监督学习**

>> ○ 定义：从问题-答案对 {(**x**<sub>i</sub>, y<sub>i</sub>)} 学习映射 x → y。

>> ○ 示例：条件概率、分类、回归函数、监督分类。

>> ○ 优势

>>> ○ 根据之前的经验生成输出数据

>>> ○ 利用经验优化绩效标准

>>> ○ 帮助解决各类问题

>>> ○ 计算复杂度：比较简单

>>> ○ 准确度：非常准确

>> ○ 缺点
 
>>> ○ 必须使用标记数据进行输出

>>> ○ 一般需要很长时间

>>> ○ 对于大数据，可能需要极长的时间

>> ○ **1-1。半监督学习**

>>> ○ 与监督学习类似，但仅标记了一些答案。

>> ○ **1-2。模仿学习**

>>> ○ 监督学习学习输入和输出之间的关系，而模仿学习则使用其他示例数据来学习这种关系。

>> ○ **1-3。元学习**

>>> ○ 学习其他机器学习算法的结果或学习过程。

> ② **类型2.无监督学习**

>> ○ 定义：了解不带标签的特征向量 **x** 的底层结构。

>> ○ 示例：聚类、降维、主题建模、[DIP](https://jb243.github.io/pages/2164)（深度图像先验）、风格迁移。

>> ○ 优势

>>> ○ 不需要单独的标签

>> ○ 缺点

>>> ○ 计算复杂度：相当复杂

>>> ○ 准确度：不太准确

>> ○ 与之前学习的区别

>>> ○ 典型的 CNN 神经网络会使用大型数据集进行先验学习以创建预测模型。

>> ○ 当训练/预测同时发生时，事先学习是必要的。

>>> ○ 另一方面，像 DIP 这样的方法是自学的，不需要事先学习。

> ③ **类型3.强化学习**

>> ○ 定义：根据特定条件下采取的行动所给予的奖励（π（行动|状态））来学习每个状态下的最佳行动的方法。

>> ○ 正确答案的线索以奖励的形式隐式提供。

> ④ **类型 4. 自我监督学习**

>> ○ 定义：机器学习通过独立创建问题和答卷来进行，就像玩具示例一样。

>> ○ 与监督学习的相似之处：区分测试/训练数据集。

>> ○ 与无监督学习的相似之处：模型生成答卷。

>> ○ **4-1。自动联想学习**：也称为[自动编码器](https://jb243.github.io/pages/956)。

>>> ○ 定义：尽可能压缩输入数据，然后将压缩数据恢复为原始形式的神经网络。

>>>> ○ 编码器：从输入层到隐藏层的神经网络。>>>> ○ 解码器：从隐藏层到输出层的神经网络。

>>>> ○ 编码器和解码器一起训练。

>>> ○ **效果1.** 数据压缩：如果隐藏层的节点数少于输入层，则网络可以压缩输入数据。

>>> ○ **效果2.** 数据抽象：将复杂数据转换为多维向量，实现输入数据的分类和重组。

>>>> ○ 这些多维向量也称为隐藏层或特征。

>>>> ○ PCA是一种具有代表性的重组算法。

>> ○ **4-2。基于掩蔽**

>>> ○ 示例：iBOT（掩蔽+蒸馏）。

>> ○ **4-3。基于蒸馏**

>>> ○ 例如：DINO（蒸馏）、iBOT（掩蔽+蒸馏）。

>> ○ **4-4。对比学习**

>>> ○ 定义：通过比较目标数据和控制数据来学习的方法。

>>> ○ 应用信息论的 [surprisal](https://jb243.github.io/pages/2145#:-,uncertainty,-\(surprisalunexpectednessrandomness)) 将损失函数定义为：$-(1/N) × (Σ log Φ_target + Σ log (1-Φ_control))$，其中 Φ 是概率。

>>> ○ 示例：SimCLR、MoCo、BYOL。

>> ○ **4-5。基础模型**

>>> ○ 定义：以归纳方式运行的机器学习模型，必然是自我监督的。

>>> ○ 归纳方式：推理能力（例如知道“苹果”是사과，“香蕉”是바나나，可以猜出“瓜”是멜론）。

>>> ○ 与生成模型的区别：生成模型是指创建新示例数据的模型。

>>> ○ 基础模型可能是生成性的，也可能不是生成性的。

>>> ○ [DIP](https://jb243.github.io/pages/2164)（深度图像先验）是一个生成模型，但不是基础模型，因为它不能以归纳方式工作。

>>> ○ 基础模型分为 LLM、LMM 和扩散模型。

>>> ○ **范式转变：** 机器学习（2000 年代，以特征为中心）→ 深度学习（2010 年代，以模型为中心）→ 基础模型（2020 年代，以数据为中心）。

<br>

![图片](https://github.com/user-attachments/assets/1a3abbdb-63a6-4925-8201-46548e497def)

**图 1.** 机器学习的范式转变

<br>

>> ○ **4-5-1.** [**LLM**](https://jb243.github.io/pages/325)（大型语言模型）和 **LMM**（大型多模式模型）。

>> ○ **4-5-2.** **扩散学习**

>>> ○ 故意向原始图像添加噪声，然后从噪声图像生成原始图像的模型。

<br>

<img width="359" alt="스크린샷 2024-12-12 8 39 40" src="https://github.com/user-attachments/assets/55ff8cbe-4634-4288-86f2-4526dcabc7c6" />

<br>

>>> ○ 生成模型：可以生成各种形式的AI生成图像。

>>> ○ 扩散学习以归纳方式进行，与 [DIP](https://jb243.github.io/pages/2164) 不同。

>>> ○ 扩散模型也用于化学信息学（例如，[DiffPack](https://arxiv.org/abs/2306.01794)、[RFdiffusion](https://www.nature.com/articles/s41586-023-06415-8)）。

>> ○ **4-6。迁移学习**

>>> ○ 通过修改最终输出层来重新训练学习模型的算法。

>>> ○ 允许将预训练模型应用于与最初预期任务不同的新任务（例如，[Geneformer](https://www.nature.com/articles/s41586-023-06139-9)）。

>>> ○ 不一定是自我监督的学习方法；例如，[Step2Heart](https://arxiv.org/abs/2011.12121) 使用监督学习实现。

>>> ○ 需要自我监督学习来归纳操作不同的任务。

> ⑤ 还有很多其他类型。

>> ○ 表征学习

>> ○ 主动学习

>> ○ 在线学习，增量学习，永无止境的学习

>> ○ 课程学习：由易到难的阶段学习方法。

>> ○ 少样本学习、一次性学习>> ○ 多实例学习、多标签学习、分布式学习

>> ○ 度量学习、核学习

>> ○ 联邦学习：在分布式数据集上训练模型。优点包括保护数据隐私和有效利用多个来源的数据。

⑺ 应用领域

> ① 游戏：国际象棋、AlphaGo、星际争霸。

> ② 聊天机器人：Watson、Siri、Alexa、Google Assistant。

>> ○ Watson：第一个在电视节目中击败人类参与者的人工智能系统。

> ③ 图像分析：TikTok、Snapchat。

> ④ 推荐系统：Spotify（音乐）、Netflix（电影）、YouTube（视频）、TikTok（短视频）。

> ⑤ 自动驾驶：Waymo、特斯拉。

> ⑥ 生物学研究：[Alphafold](https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology)、[Alphafold2](https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/AlphaFold2.ipynb)。

⑻ 相关软件 

> ① 张量流

> ② PyTorch

> ③ 抱脸

> ④ 喀拉斯

> ⑤ H2O.ai

⑼ 人工智能开发语言

> ① LISP（列表处理器）

>> ○ 1958 年由麻省理工学院的 John McCarthy 开发。

>> ○ 引入了现代高级语言概念，例如树数据结构、动态类型、递归、条件、自动内存管理（垃圾收集）和高阶函数。

 >> ○ 第一种专为人工智能研究而设计的语言。

> ②Python

>> ○ 1991 年由 Guido van Rossum 在 CWI 正式发布。

>> ○ 实现继承、异常处理、函数和模块，并受到 LISP 的影响。

>> ○ 人工智能研究/开发的现代基本语言。

> ③Java

>> ○ 用于自然语言处理 (CoreNLP)、张量运算 (ND4J) 和 GPU 加速深度学习堆栈 (DL4J) 等工具。

<br>

<br>

## **2.评估**

⑴ 权重空间

<br>

![图片](https://github.com/user-attachments/assets/f0a5d8e6-0b7f-4a01-96d0-c50c7d71ed86)

**图 2.** 权重空间中两个神经元的位置。

<br>

⑵ 维数诅咒

> ① 超球面

>> ○ 定义：任意n维空间中距原点距离为1的点的集合。

>> ○ 当维度增加超过 n > 2π 时，超球面的体积接近 0。

>>> ○ V<sub>n</sub> = (2π/n) V<sub>n-2</sub>。

>> ○ 含义：随着维度的增加，更多的数据进入单位超立方体。

> ② **维度诅咒**

>> ○ 分布分析或模型估计所需的样本数据数量随维数呈指数增长的现象。

>> ○ 原因：同量数据的密度随着维度的增加而迅速降低。

⑶ 过拟合

> ① 过度训练

<br>

![图片](https://github.com/user-attachments/assets/957c28ab-9e15-4395-8d3e-114174127a13)

**图 3.** 函数概括了由于过度拟合而产生的均匀噪声。

<br>

>> ○ 机器学习中的过度训练与训练不足一样危险。

>> ○ 过度训练人工神经网络会导致过度拟合，甚至出现学习错误和数据不准确。

>> ○ 预测准确度下降。

> ② 避免过拟合的策略

>> ○ 在神经网络过度拟合之前停止训练的策略。

>> ○ 在每次训练迭代期间输入误差项的策略。

⑷ 神经网络评估

> ① 定义：评估神经网络使用样本外数据的泛化能力。

> ② 数据集

>> ○ 训练数据：用于给定样本内训练的数据。

>> ○ 测试数据：不用于训练的数据，用于评估训练后的模型。

>> ○ 验证集：来自不同人群的第三个数据集，用于检查过度拟合。

>> ○ 训练和测试数据被归类为样本内数据。

>> ○ 验证集被归类为样本外数据。

> ③ 数据集选择>> ○ 当有足够的数据时：分为训练：测试：验证为 50 : 25 : 25、60 : 20 : 20 或 50 : 30 : 20。

>> ○ 当没有足够的数据时：重复使用某些数据进行训练、测试和验证。通常通过三种方式完成。

> ④ **交叉验证**（轮换估计、样本外测试））

>> ○ 定义：对给定数据进行重新采样以分配训练集、测试集和验证集。

>> ○ **类型 1。详尽的交叉验证**：

>>> ○ **1-1。留一交叉验证 (LOOCV)：** 与 m 倍交叉验证相比，可能导致过度拟合。

<br>

![图片](https://github.com/user-attachments/assets/a77445d8-9ec8-4b0d-a6ad-f9b284bf375a)

**图 4.** 留一法交叉验证。

<br>

>>> ○ **1-2。留出一些交叉验证**。

>> ○ **类型 2.非详尽交叉验证**：

>>> ○ **2-1。多重（m-fold）交叉验证**：

<br>

![图片](https://github.com/user-attachments/assets/b26a2a89-5abf-4b82-a0a2-ff033cfedd14)

**图 5.** m 倍交叉验证。

<br>

![图片](https://github.com/user-attachments/assets/03a78d35-924d-4c4b-84fd-ac7e15b7b04e)

**图 6.** m 倍交叉验证。

<br>

>>>> ○ 1<sup>st</sup>：将给定样本拆分为 m 部分。

>>>> ○ 2<sup>nd</sup>：使用m-1个零件进行参数计算。

>>>> ○ 3<sup>rd</sup>：使用剩余的 1 部分进行性能评估。

>>>> ○ 第 4<sup>th</sup>：对所有不同的组合重复 m 次。

>>>> ○ 第 5<sup>th</sup>：计算平均值以确定最终估算。

>>>> ○ 通常使用 10 倍交叉验证。

>>> ○ **2-2。坚持方法**。

>>> ○ **2-3。重复随机子抽样验证**。

>> ○ **应用 1. Early Stopping：** 防止模型过度拟合给定数据。

<br>

![图片](https://github.com/user-attachments/assets/c3932713-ef0e-48cc-9ab9-f2a3be6cf7c9)

**图 7.** 验证和提前停止。

<br>

>> ○ **应用2.超参数调优：**超参数是用户定义的参数。

>> ○ **应用3.特征子集问题。

⑸ 混淆矩阵

<br>

![图片](https://github.com/user-attachments/assets/0f09b8ee-518b-4d95-a247-1a80de4ea933)

**表 1.** 混淆矩阵。

<br>

> ①“7”代表分类器正确预测类别C1的次数。

> ② 准确度 = (7 + 8 + 9) ÷ (1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9) = 24/45。

⑹ [准确率指标](https://jb243.github.io/pages/1634)

> ① **真阳性**（TP）：当实际值为真且预测值为真时。这意味着一个真正积极的案例。

>> ○ #TP = 7 + 8 + 9 = 24

> ② **假阳性**（FP）：当实际值为假而预测值为真时。这意味着假阳性病例。

>> ○ #FP = 1 + 2 + 3 + 4 + 5 + 6 = 21。

> ③ **True Negative** (TN)：当实际值为假且预测值为假时。这意味着真正的负面案例。

>> ○#TN

> ④ **假阴性**（FN）：当实际值为真而预测值为假时。这意味着假阴性病例。

>> ○ #FN

> ⑤ **准确度** = (#TP + #TN) ÷ (#TP + #FP + #TN + #FN)。

>> ○ 错误率 = 1 - 准确度。

> ⑥ **灵敏度**（真阳性率，TPR）或**召回率** = #TP ÷ (#TP + #FN)。

>> ○ 颠倒正负的含义可切换对特异性的敏感性。

> ⑦ **特异性** = #TN ÷ (#TN + #FP)。

>> ○ 颠倒正负的含义可切换灵敏度的特异性。

> ⑧ **精度**（阳性预测值，PPV）= #TP ÷ (#TP + #FP)。

> ⑨ **阴性预测值 (NPV)：** TN ÷ (TN + FN)。

> ⑩ **假阳性率（FDR，错误发现率）：** FP ÷ (TN + FP) = 1 - 特异性。

> ⑪ **F1 分数** = 2 × 精度 × 召回率 ÷ (精度 + 召回率) = #TP ÷ [#TP + (#FN + #FP)/2]。>> ○ 将精确度和召回率结合到一个单一的性能指标中。

>> ○ 范围在 0 和 1 之间。

>> ○ F1 分数越高表明精度和召回率越高，性能越好。

> ⑫ **Kappa 统计量 (κ):**

>> ○ K = (Pr(a) - Pr(e)) / (1 - Pr(e))。

>> ○ K：Kappa 相关系数。

>> ○ Pr(a)：预测之间一致的概率。

>> ○ Pr(e)：偶然一致的概率。

>> ○ 衡量两个观察者之间对分类数据的一致性。

>> ○ 取值范围为 0 到 1，1 表示完全一致，0 表示不一致。

>> ○ 说明模型评估结果并非随机且超出准确性。

> ⑬ **马修斯相关系数 (MCC)**。

<br>

![图片](https://github.com/user-attachments/assets/eb419abd-dffc-4486-adee-115a6340509d)

<br>

⑺ 【索引】(https://jb243.github.io/pages/1634)

> ① 调整阈值通常会显示灵敏度和特异性之间呈反比关系。

<br>

![图片](https://github.com/user-attachments/assets/b96caea7-8888-4eee-a20e-43567833c28d)

**图 8.** 灵敏度和特异性随阈值的趋势。

<br>

> ② ROC Curve（接收器工作特性曲线）

>> ○ 纵轴为灵敏度、横轴为 1-特异性 (FDR) 的图表。

<br>

![图片](https://github.com/user-attachments/assets/fd1559d7-6151-4d26-a8dd-4126d74d7ecf)

**图 9.** ROC 曲线。

<br>

>> ○ 理想情况是灵敏度 = 1，特异性 = 1。

>> ○ **AUC（曲线下面积）：** 范围从 0 到 1。值越高表示性能越好。

<br>

![图片](https://github.com/user-attachments/assets/b77d9896-e68d-4b6a-96ab-3b4bc96f9865)

**图10.** AUC计算流程

<br>

![图片](https://github.com/user-attachments/assets/523cfc17-abfb-4b3d-8926-526446765884)

**图11.** AUC计算流程

<br>

> ③ 一致性指数：指ROC曲线下的面积。

> ④ 当ROC曲线随机时，一致性指数=0.5。

> ⑤ 一致性不能超过1。

> ⑥ AUPRC（精确率-召回率曲线下面积）

>> ○ 使用精度和召回率计算，而不是 AUROC 中使用的灵敏度和特异性。

>> ○ 当阳性病例（第 1 类）和阴性病例（第 2 类）数量不平衡时，优于 AUC。

⑻ 影响数据输入的因素

> ① **GIGO（Garbage In，Garbage Out）：** 意味着不良的输入数据会导致不良的输出。

> ② **不平衡数据集：**

> ③ 异常值和缺失值

<br>

<br>

## **3。历史**

⑴ **准备期（1943-1956）**

> ①命题逻辑的概念

> ② 图灵测试

> ③ 第一个国际象棋程序（Dietrich Prinz）

⑵ **早期发展时期（1956-1974）**

> ①“人工智能”一词的创造（达特茅斯会议）

> ②感知器神经网络（Rosenblatt）

>> ○ **1958：** Frank Rosenblatt 引入了感知器，这是一种用于二元分类的简单神经网络。

> ③LISP语言的开发（麦卡锡）

> ④ 发表*Perceptrons*（1969，Minsky & Papert）

>> ○ **1969：** Marvin Minsky 和 Seymour Papert 发表了*感知器*，强调了局限性并引发了第一个人工智能“冬天”。

⑶ **第一个人工智能冬天（1974-1980）**

> ① XOR问题，不能用线性分离来解决

⑷成长期（1980-1987）

> ① XCON专家系统开发

> ② 霍普菲尔德网络（John Hopfield）

>> ○ **1982：** John Hopfield 提出了 Hopfield 网络，重新燃起了人们对神经网络的兴趣。

> ③多层感知器（Geoffrey Hinton）

> ④ 反向传播的提议（Seppo Linnainmaa）

> ⑤ 反向传播在神经网络中的应用（Paul Werbos）

> ⑥ 反向传播的普及（Geoffrey Hinton、Rumelhart）>> ○ **1986：** Geoffrey Hinton 及其同事普及了反向传播，使深度神经网络的训练成为可能。

⑸ **第二次人工智能冬季（1987-1993）**

> ①梯度消失、计算成本增加、学习速度慢等。

⑹ **稳定期（1993-2011）**

> ① ReLU、Dropout 等（Geoffrey Hinton）+硬件进步（GPU 等）

⑺ **复兴期（2011年至今）**

> ① 深度学习和ImageNet

>> ○ **2012：** Geoffrey Hinton 在“ImageNet Challenge (ILSVRC)”中以最低错误率获得第一名，凸显了 GPU 的重要性。

> ② Google 项目（吴恩达）

> ③世界围棋冠军（AlphaGo、AlphaGo Zero等）

> ④ 生成式AI（ChatGPT等）

<br>

---

_输入：2018.06.09 10:00._

_修改: 2024.04.02 16:01._