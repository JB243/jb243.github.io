## **Graph Theory**

Recommended Reading: 【Mathematics】 [Mathematics Table of Contents](https://jb243.github.io/pages/764)

---

**1.** [Basics](#1-basics)

**2.** [Causal Inference](#2-causal-inference)

**3.** [Other Applications](#3-other-applications)

---

<br>

## **1. Basics**

⑴ Components of a graph 

> ① V: Node

> ② E: Edge

> ③ G: Graph 

> ④ V(G) = {v<sub>1</sub>, ···, v<sub>n</sub>}

> ⑤ v<sub>i</sub> ~ v<sub>j</sub>: When edge e<sub>ij</sub> connects vertices v<sub>i</sub>, v<sub>j</sub>. 

⑵ Degree of Graph G: Number of nodes, i.e., |V(G)|

⑶ Size of Graph G: Number of edges, i.e., |E(G)|

⑷ Degree of node v: Number of edges connected to node v, i.e., deg(v). 

> ① deg(v<sub>i</sub>) = ∑<sub>v<sub>i</sub>~v<sub>j</sub></sub> 1

> ② δ(G) refers to the minimum degree of nodes in G, and Δ(G) refers to the maximum degree of nodes in G.

> ③ ∑ deg(v<sub>i</sub>) = 2 | E(G) |

> ④ If all the nodes of a graph have the same degree, the graph is regular.

> ⑤ The nodes of an Eulerian graph have even degree.

⑸ Simple graph and complete graph

> ① Simple graph: A graph in which at most one edge exists between any two nodes, and self-loops (edges connecting a node to itself) are not allowed.

<br>

<img width="260" alt="스크린샷 2024-10-07 오후 8 13 51" src="https://github.com/user-attachments/assets/d37838f7-69f0-4a3a-adea-d05033a3b7cd">

**Figure 1.** Simple Graph

<br>

> ② Complete graph: A graph in which all nodes are connected by **unique** edges.

<br>

<img width="171" alt="스크린샷 2024-10-07 오후 8 14 21" src="https://github.com/user-attachments/assets/7c65060b-f12f-4485-80d1-45390a152777">

**Figure 2.** Complete Graph

<br>

>> ○ strongly connected (= irreducible)

>>> ○ A state where any node i can reach any other node j within the graph.

>>> ○ A Markov chain is irreducible if j is reachable from j ∀i, j.

>> ○ The number of edges in a complete graph is <sub>N</sub>C<sub>2</sub> = N(N-1) / 2.

⑹ Subgraph: H is a subgraph of G if V(H) ⊆ V(G) and E(H) ⊆ E(G).

> ① A subgraph H is an induced subgraph of G if two vertices of V(H) are adjacent if and only if they are adjacent in G.

> ② clique: a complete subgraph of a graph.

> ③ Walk: A path like v<sub>0</sub> → v<sub>1</sub> → ... → v<sub>m</sub>, connected by edges v<sub>0</sub>v<sub>1</sub>, v<sub>1</sub>v<sub>2</sub>, ..., v<sub>m-1</sub>v<sub>m</sub>.

>> ○ v<sub>0</sub> is called the initial vertex, and v<sub>m</sub> is called the final vertex.

>> ○ The number of edges in the walk is called the length.

> ④ Path: A walk where all nodes are distinct.

> ⑤ Cycle: A walk where v<sub>0</sub> = v<sub>m</sub>.

> ⑥ Forest: A graph containing no cycles.

> ⑦ Tree: A connected forest

> ⑧ Period: The period of a node i is the greatest common divisor of all paths from node i to itself.

>> ○ Example: If two nodes A and B are connected by two edges such that A = B, the period of each node is 2.

> ⑨ Aperiodic: all nodes have period 1.

>> ○ For an irreducible Markov chain, if one state is aperiodic, then all states are aperiodic.

>> ○ Example: A walk where each node has a path leading back to itself is aperiodic.

> ⑩ Regular

>> ○ regular ⊂ irreducible

>> ○ For some natural number k, all elements of the k-th power of the transition matrix M, M<sup>k</sup>, are positive (i.e., non-zero values).

⑺ Types of graph

> ① Undirected graph: A graph with undirected edges

> ② Directed graph: A graph with directed edges

> ③ k-partite graph: If its set of vertices admits a partition into k classes such that the vertices of the same class are not adjacent.

⑻ Centrality of a network

> ① Degree centrality shows people with many social connections.

> ② Closeness centrality indicates who is at the heart of a social network.

> ③ Betweenness centrality describes people who connect social circles.

> ④ Eigenvector centrality is high among influential people in the network.

⑼ Adjacency matrix: Represented as **A**.

> ① Definition: When the number of nodes is n, an n × n adjacency matrix is defined as, A<sub>i,j</sub> = 1 (if v<sub>i</sub> ~ v<sub>j</sub>), A<sub>i,j</sub> = 0 (otherwise)

<br>

<img width="504" alt="스크린샷 2024-10-12 오후 3 12 00" src="https://github.com/user-attachments/assets/43371cde-f21a-4a11-a122-674f7b063805">

**Figure 3.** Example of adjancy matrix

<br>

> ② Real-valued function on the set of the graph's vertices, f: V → ℝ

<br>

<img width="405" alt="스크린샷 2024-10-12 오후 3 13 40" src="https://github.com/user-attachments/assets/69227caf-dec2-49bc-b67b-2ba98b754bde">

**Figure 4.** Example of the real-valued function f

<br>

> ③ The following fomula holds

<br>

<img width="400" alt="스크린샷 2024-10-12 오후 3 14 34" src="https://github.com/user-attachments/assets/71f82f2b-1ff0-45c6-8a88-4ca0560dd340">

<br>

> ④ Symmetrically normalized adjacency matrix: **Ã** = **D**<sup>-1/2</sup>**AD**<sup>-1/2</sup> (cf. D<sub>ii</sub> = d(v<sub>i</sub>))

⑽ Incidence matrix: Represented as ∇.

> ① Incidence matrix of undirected graph is defined as, B<sub>i,j</sub> = 1 (if vertex v<sub>i</sub> is incident with edge e<sub>j</sub>), B<sub>i,j</sub> = 0 (otherwise)

<br>

<img width="615" alt="스크린샷 2024-10-12 오후 3 37 14" src="https://github.com/user-attachments/assets/fd63090d-3d79-4134-9d1b-109cae028c1a">


**Figure 5.** Example of incidence matrix

<br>

> ② Incidence matrix of directed graph

<br>

<img width="374" alt="스크린샷 2024-10-12 오후 3 16 05" src="https://github.com/user-attachments/assets/62fd83b9-3073-44ab-8120-d84a2c0f30e7">

<br>

> ③ Both cases where nodes are rows and edges are columns (e.g., Fig. 5) and where edges are rows and nodes are columns (e.g., Fig. 6) are being used.

⑾ Laplacian matrix

> ① Overview

>> ○ Definition: **L** = ∇<sup>T</sup>∇ = **D** - **A** (cf. **D**<sub>ii</sub> = d(v<sub>i</sub>)) 

>> ○ Multiplicity: # of connected components of the graph

<br>

<img width="509" alt="스크린샷 2024-10-12 오후 3 46 21" src="https://github.com/user-attachments/assets/bada7405-eea5-4ba6-b929-b166a83869ec">

**Figure 6.** Example of Laplacian matrix

<br>

>> ○ Here, ∇ is as follows: 

<br>

```python
Matrix([
    [1, -1,  0,  0],  # e1: v1 -> v2
    [1,  0, -1,  0],  # e2: v1 -> v3
    [0,  1, -1,  0],  # e3: v2 -> v3
    [0,  1,  0, -1]  # e4: v2 -> v4
])
```

<br>

> ② Modification of Laplacian matrix

>> ○ Normalized graph Laplacian: **L**<sub>n</sub> = **D**<sup>-1/2</sup>**LD**<sup>-1/2</sup> = **I** - **D**<sup>-1/2</sup>**AD**<sup>-1/2</sup>. symmetric, semi-definite positive

>> ○ Transition matrix of Markov chain: **L**<sup>t</sup> = **D**<sup>-1</sup>**A** 

>> ○ Random-walk graph Laplacian: **L**<sub>r</sub> = **D**<sup>-1</sup>**L** = **I** - **L**<sub>t</sub> = **D**<sup>-1/2</sup>**L**<sub>n</sub>**D**<sup>1/2</sup>

> ③ **Feature 1.** **L** has an eigenvalue of 0. **L1**<sub>n</sub> = **0** 

>> ○ If there are k disconnected graphs, there are k eigenvalues of 0.

>> ○ The closer the eigenvalue is to 0, the more connected the graph is.

>> ○ Fiedler vector: The eigenvector corresponding to the second smallest eigenvalue after λ<sub>1</sub> = 0. It represents the overall structure of the graph.

> ④ **Feature 2.** (**Lf**)(v<sub>i</sub>) = ∑<sub>v<sub>j</sub>~v<sub>i</sub></sub> (f(v<sub>i</sub>) - f(v<sub>j</sub>)): Refer to **Fig. 4**.

<br>

<img width="363" alt="스크린샷 2025-01-01 오후 3 53 03" src="https://github.com/user-attachments/assets/cf3a90ad-d5b6-498f-b678-cbc3293a01b2" />

<br>

> ⑤ **Feature 3.** **L** is symmetric and positive semi-definite. 

>> ○ Quadratic form: **f**<sup>T</sup>**Lf** = ∑<sub>e<sub>i,j</sub>, i < j</sub> (f(v<sub>i</sub>) - f(v<sub>j</sub>))<sup>2</sup> = (1/2) ∑<sub>e<sub>i,j</sub></sub> (f(v<sub>i</sub>) - f(v<sub>j</sub>))<sup>2</sup> ≥ 0

<br>

<img width="456" alt="스크린샷 2025-03-31 오후 7 15 58" src="https://github.com/user-attachments/assets/5a3e5cc8-7e22-44ff-a6b2-e7cbf4df998e" />

<br>

>> ○ In the example of **Fig 4**, **f**<sup>T</sup>**Lf** = 9.27

>> ○ That is, **L** has n non-negative, real-valued eigen values.

>> ○ When a matrix **p** is used instead of **f**, it is denoted as tr(**p**ᵀL**p**).

> ⑥ **Feature 4.** Given Y = [**y**<sub>1</sub> **y**<sub>2</sub> ⋯ **y**<sub>m</sub>] ∈ ℝ<sup>k×m</sup>,

>> ○ tr(Y<sup>T</sup>LY) = ∑<sub>e<sub>i,j</sub></sub> || **y**<sub>i</sub> - **y**<sub>j</sub> ||<sup>2</sup>

> ⑦ **Feature 5.** Rayleigh Quotient

<br>

<img width="504" alt="스크린샷 2025-03-31 오후 9 34 30" src="https://github.com/user-attachments/assets/86719a5c-ccf4-4924-844c-26489ecbfd36" />

<br>

>> ○ λ<sub>2</sub>, the Fiedler value (algebraic connectivity), represents the amount of energy required to differentiate or separate values.

>> ○ A large λ<sub>2</sub> indicates that the data is difficult to separate, suggesting a well-connected overall structure.

>> ○ A small λ<sub>2</sub> means the data is easy to separate, indicating a weakly connected structure or one that is clustered into distinct groups.

> ⑧ **Application 1.** Laplacian operator for an undirected weighted graph 

<br>

<img width="304" alt="스크린샷 2025-01-01 오후 3 55 27" src="https://github.com/user-attachments/assets/304be961-9d20-48aa-8685-017201342dd7" />

<br>

> ⑨ **Application 2.** Spectral Clustering ([ref](https://www2.imm.dtu.dk/projects/manifold/Papers/Laplacian.pdf))

>> ○ The original problem

>>> ○ It seeks to find "min-cuts" in a graph: vertex partition with minimum inter-cluster edge weight while normalizing the cluster volume.

>>> ○ The key idea of clustering (graph partitioning) is to make the connections between different clusters (i.e., the sum of edge weights across clusters) small, while preventing an imbalanced split where some clusters become too small.

>>> ○ If we only minimize the inter-cluster connections (min-cut), we often end up with a trivial partition that simply peels off a few nodes. Therefore, we instead minimize the normalized cut, which normalizes the cut by the cluster size (connectivity/association).

>>> ○ Definition of cut

<br>

<img width="223" alt="스크린샷 2025-04-06 오후 11 01 30" src="https://github.com/user-attachments/assets/e0015b34-55e8-42ed-be25-6379609ea39f" />

<br>

>>> ○ Definition of volume

<br>

<img width="452" alt="스크린샷 2025-04-06 오후 11 02 37" src="https://github.com/user-attachments/assets/69a170cc-f029-4883-9384-d735f0067142" />

<br>

>>> ○ 2-cluster optimization problem 

<br>

<img width="331" alt="스크린샷 2025-04-06 오후 11 03 03" src="https://github.com/user-attachments/assets/745f6950-72d0-4828-a334-adf4bc0b6ba0" />

<br>

>>> ○ Multi-cluster optimization problem

<br>

<img width="364" alt="스크린샷 2025-04-06 오후 11 03 29" src="https://github.com/user-attachments/assets/84427611-8d3e-4a9b-ae58-98fff0f7bea4" />

<br>

>>> ○ Equivalent to finding one-hot encoding of cluster labels X ∈ {0,1}<sup>N×K</sup> that satisfies 

<br>

<img width="405" alt="스크린샷 2025-04-06 오후 11 04 43" src="https://github.com/user-attachments/assets/ea83894c-ef15-466a-b1e3-ec845225a176" />

<br>

>> ○ A solution using the Laplacian matrix

>>> ○ Problem definition

<br>

<img width="129" alt="스크린샷 2025-04-06 오후 11 37 06" src="https://github.com/user-attachments/assets/62880845-cec0-4ad8-8dac-2781d1ddc5f7" />

<br>

>>>> ○ Reason why the condition **y**<sup>T</sup>D**y**=1 is necessary: Without this condition, there exists a trivial solution **y**=0, and for any nonzero **y**, there exists a scalar c with 0≤c<1 such that c**y** makes the objective function arbitrarily small.

>>>> ○ Reason why the condition **y**<sup>T</sup>D**1**=0 is necessary: This condition is necessary and sufficient for **y**≠**1** (orthogonality). Without it, the minimum of the objective function is always 0 and always achieved at **y**=**1** (by the method of [Lagrange multipliers](https://jb243.github.io/pages/1813)).

<br>

<img width="328" alt="스크린샷 2025-04-06 오후 11 36 44" src="https://github.com/user-attachments/assets/d6ec7ed5-994a-46c2-af21-4560204e9081" />

<br>

>>>> ○ In the original spectral clustering problem, relaxing the one-hot constraint and adding the condition X<sup>T</sup>DX = I makes it equivalent to this solution.

>>> ○ **Step 1:** Calculate the distance between the given n data points to create an n × n adjacency matrix A. The Gaussian kernel is often used.

<br>

<img width="208" alt="스크린샷 2024-10-12 오후 5 48 15" src="https://github.com/user-attachments/assets/fcc9719d-68a7-43ca-8d7f-31dfc60aa919">

<br>

>>> ○ **Step 2:** Based on the adjacency matrix, calculate the Laplacian matrix: **L**<sub>n</sub> or **L**<sub>r</sub> can also be used.

>>> ○ **Step 3:** Calculate the matrix of eigenvectors **W** = [**ω**<sub>2</sub>, ···, **ω**<sub>k</sub>] ∈ ℝ<sup>n×k</sup> corresponding to the k smallest eigenvalues of the Laplacian matrix. Exclude **ω**<sub>1</sub> = 1, which corresponds to λ<sub>1</sub> = 0, as it is trivial.

>>> ○ **Step 4:** The n data points are naturally embedded into k dimensions: **Y** = **W**<sup>T</sup> = [**y**<sub>1</sub>, ⋯, **y**<sub>n</sub>], with each column vector representing the embedded data points.

>>> ○ **Step 5:** Perform clustering, such as K-means clustering, on the column vectors of **Y**.

> ⑩ **Application 3.** The Laplacian matrix can be used as a Fourier transform operator (e.g., [G-FuNK](https://arxiv.org/abs/2410.04655))

⑿ Degree distribution: The ratio of nodes with degree k among all nodes, i.e., P(k) = N<sub>k</sub> / |V(G)|.

⒀ Distance: The length of the shortest path between two nodes. If no such path exists, the distance is defined as ∞.

⒁ Clustering coefficient: If the degree of a specific node is k<sub>i</sub> and the number of edges between the surrounding nodes connected to that node is e<sub>i</sub>, then:

<br>

<img width="194" alt="스크린샷 2024-10-07 오후 8 16 27" src="https://github.com/user-attachments/assets/cfc24e3e-7f19-4036-addb-c808159bf08f">

<br>

> ① For example, if A is connected to B, C, and D, and there are additional edges B-C and C-D, then for A, e<sub>i</sub> = 2, k<sub>i</sub> = 3, C<sub>i</sub> = 2 / 3.

> ② k<sub>i</sub> (k<sub>i</sub> - 1) / 2: The maximum number of edges that could be formed between the surrounding nodes.

> ③ C<sub>i</sub> has a value between 0 and 1, with a value close to 1 indicating that the surrounding nodes are fully connected.

<br>

<br>

## **2. Causal inference**

⑴ Directed Acyclic Graph (DAG)

> ① Clarifies study question and relevant concepts.

> ② Explicitly identifies assumptions.

> ③ Reduces bias.

> ④ Separates individual effects.

> ⑤ Ascertains appropriate covariates for statistical analysis.

⑵ Markov Equivalence

> ① Markov property of DAG $G$: Only the parent node can decide the distribution of the child node(s).

> ② ℳ($G$): Denotes the set of all probability distributions that are Markov with respect to a DAG $G$.

> ③ $G_1$ and $G_2$ are Markov equivalant (denoted by $G_1$ ~ $G_2$) if they represent the same set of Markovian distributions: ℳ($G_1$) = ℳ($G_2$). 

> ④ If and only if:

>> ○ Skeleton: $G_1$ and $G_2$ have the same skeleton (the same set of edges when directions are ignored.)

>> ○ v-structures: $G_1$ and $G_2$ have the same set of v-structures.

>> ○ Note that v-structures refer to patterns of the form $i$ → $k$ ← $j$ where $i$ and $j$ are not adjacent. They are related to conditional independence (i.e., $i \nperp j \mid k$).

>> ○ Note that d-separations refer to patterns of A → (C, D, E) → B. They are related to information flow.

> ⑤ Markov Equivalence Class (MEC): Class of DAGs that are Markov equivalent to each other.

⑶ Causal Model

> ① Let $G = ([p], E)$ be a Directed Acyclic Graph (DAG).

> ② Nodes refer to the set $[p] := {1, ..., p}$, where each node $i$ corresponds to a random variable $X_i$.

> ③ Edges refer to the edge set $E$ that represents the causal relationships between these variables.

> ④ Let $f$ denote the density function of the data-generating distribution $P$ over the random vector $X = (X_1, ..., X_p)$.

⑷ Causal Markov Property

> ① The density function $f$ is said to be Markov with respect to the DAG $G$ if it factorizes according to the graph structure:

$$
f(x) = \prod_{i\in[p]} f_i(x_i \mid x_{pa_G(i)})
$$

> ② $pa_{G}(i)$ denotes the set of parents of nod $i$ in the DAG $G$.

> ③ $x_{pa_{G}(i)}$ represents the values of the variables corresponding to those parent nodes.

⑸ I-Markov Property

> ① I ⊆ [p]

>> ○ Intervention = Perturbation

>> ○ Perfect intervention 

>> ○ Imperfect intervention 

> ② I-Markov Property (Given (I))

>> ○ $f^{\mathrm{obs}}$: the probability density function of the data without perturbation

>> ○ $f^{I}$: the probability density function of the data with perturbation applied

>> ○ If $\left(f^{\mathrm{obs}}, f^{I}\right)$ is Markov and, for any variable $j \in [p]\setminus I$, the following holds, then it is called (I)-Markov:

<br>

$$
f^{I}!\left(x_j \mid x_{\mathrm{pa}_G(j)}\right) = f^{\mathrm{obs}}!\left(x_j \mid x_{\mathrm{pa}_G(j)}\right)
$$

<br>

>> ○ From the Markov property alone, the following factorization holds:

<br>

$$
f(x) = \prod_{i \notin I} f^{\mathrm{obs}}!\left(x_i \mid x_{\mathrm{pa}*G(i)}\right) \prod*{i \in I} f^{I}!\left(x_i \mid x_{\mathrm{pa}_G(i)}\right)
$$

<br>

<br>

## **2. Applications**

⑴ Pigeonhole principle

⑵ [The number of unique graphs depending on the number of nodes](https://jb243.github.io/pages/2412): Nearly perfectly follows an exponential function.

⑶ [Dijkstra and Floyd algorithms](https://jb243.github.io/pages/61)

⑷ [Algorithm for counting the number of subgraphs](https://jb243.github.io/pages/1892#:countingisland)

⑸ [The power of belief](https://jb243.github.io/pages/869)

⑹ Social network theory or six degrees of separation

> ① The theory that any two people on Earth are connected by no more than six intermediaries.

> ② Proof: If one person knows an average of 100 people, then after six steps, 100<sup>6</sup> = 1 billion, reaching a similar conclusion.

⑺ Perron-Frobenius theorem

> ① **Theorem 1.** If a **finite** Markov chain with transition matrix P is strongly connected, there exists exactly one stationary distribution **q**.

>> ○ The steady-state distribution satisfies P**q** = **q**.

>> ○ Example: if P = I (the identity matrix) ∈ ℝ<sup>2×2</sup>, then it is reducible, so there are infinitely many stationary distributions of the form (x, 1-x) for all x ∈ [0, 1].

<br>

<img width="352" height="83" alt="스크린샷 2026-01-17 오후 2 28 31" src="https://github.com/user-attachments/assets/6798eb4e-9197-4fb4-94d5-8b070d56c8d2" />

<br>

> ② **Theorem 2.** If a **finite** Markov chain with transition matrix P is strongly connected and aperiodic, it is called an **Ergodic Markov chain** and satisfies:

>> ○ P<sub>ij</sub>: The probability of transitioning from node j to node i. ∑<sub>i</sub> P<sub>ij</sub> = 1.

>> ○ **2-1.** the matrix P<sup>k</sup> converges to a matrix with columns all equal to **q** as k → ∞; i.e., P<sub>ij</sub>(k) converges to **q**<sub>i</sub> (for any i and j) as k → ∞

>> ○ **2-2.** for any initial state **x**<sub>0</sub> is, the Markov Chain {**x**<sub>k</sub>} converges to **q** as k → ∞

<br>

<img width="320" height="92" alt="스크린샷 2026-01-17 오후 2 37 55" src="https://github.com/user-attachments/assets/62b7c451-e2b0-45f8-82f7-fa8cb2288363" />

<br>

>> ○ Example: if P=((0,1),(1,0)), then it is periodic (period=2). Hence there exists a unique stationary distribution π=(0.5,0.5), but $\lim_{k\to\infty} p(k)$ does not converge to a single limit and instead splits into two subsequences (for instance, $p^{\text{even }k}=(1,0)$ and $p^{\text{odd }k}=(0,1)$).

<br>

<img width="208" height="122" alt="스크린샷 2026-01-17 오후 2 43 53" src="https://github.com/user-attachments/assets/f9ac805c-d5f7-4ff4-900e-ead8e5db8ec6" />

<br>

⑻ Google's PageRank Algorithm

> ① **Step 1.** Represent web pages as a graph and express it as a Markov process transition matrix P.

> ② **Step 2.** Teleportation: For sink nodes, forcibly assign the transition probability to other pages as 1/n: P → P'.

> ③ **Step 3.** P'' = (1 - α) P' + (α / n) **1**.

>> ○ Here, **1** refers to an n × n matrix filled with ones.

>> ○ While P' is not guaranteed to be regular, P'' is always guaranteed to be regular.

>> ○ P'' is a Markov chain defined by Google's founders, Sergey Brin and Larry Page.

> ④ **Step 4.** Search for the steady-state vector **x** such that P''**x** = **x**.

>> ○ Since P'' is regular, it is strongly connected, which ensures the existence of a unique steady-state.

>> ○ The state-transition probability at this point can be considered as the importance of the recommended page.

> ⑤ Personalized PageRank: Use user input to modify the teleportation distribution (or the graph/candidate set), producing a user-specific stationary distribution.

⑼ Four color theorem

> ① Any map can be colored with at most four colors, ensuring that no two adjacent regions share the same color.

> ② The theorem was proven by verifying that the four-color theorem holds in all cases using a computer.

⑽ Eulerian path problem or Königsberg bridge problem

> ① An Eulerian path, which passes through each node exactly once, exists in a graph if and only if there are 0 or 2 nodes with odd degrees.

> ② If there are 2 nodes with odd degrees, one of these nodes is the starting point and the other is the endpoint.

> ③ There is no path that crosses all seven bridges of Königsberg exactly once.

⑾ Ramsey theory

> ① In any sufficiently large graph, a certain structure (e.g., a complete graph of acquaintances or a subgraph of strangers) must exist.

> ② R(s, t): The minimum number of nodes such that a subgraph of s strangers or a subgraph of t acquaintances is guaranteed to exist.

> ③ **Theorem 1.** R(3, 3) = 6: Among any 6 people, there are either 3 mutual friends or 3 mutual strangers. This can be proven by counting possibilities.

> ④ **Theorem 2.** R(3, 3, 3) = 17.

> ⑤ **Theorem 3.** R(3, 3, 4) = 30.

> ⑥ **Theorem 4.** Recurrence relation of R(s, t): R(s, t) ≤ R(s-1, t) + R(s, t-1).

> ⑦ **Theorem 5.** Existence of an upper bound for R(s, t): Proven through the recurrence relation of R(s, t) and mathematical induction ([ref](https://math.mit.edu/~fox/MAT307-lecture05.pdf)).

<br>

<img width="236" alt="스크린샷 2024-10-07 오후 8 17 54" src="https://github.com/user-attachments/assets/5229df38-0477-42f6-8c4f-93a0a5130ce7">

<br>

> ⑧ **Theorem 6.** R<sub>k</sub>(s<sub>1</sub>, s<sub>2</sub>, ..., s<sub>k</sub>) ≤ R<sub>k/2</sub>(R(s<sub>1</sub>, s<sub>2</sub>), R(s<sub>3</sub>, s<sub>4</sub>), ..., R(s<sub>k-1</sub>, s<sub>k</sub>)).

⑿ Schur's theorem

> ① For any k ≥ 2, there exists an n > 3 such that under k-coloring, there always exists a (x, y, z) satisfying x + y = z, colored the same way in {1, 2, ..., n}. Proven through Ramsey theory.

> ② This is connected to [Fermat's Last Theorem](https://jb243.github.io/pages/2191), revealing the existence of (x, y, z) where x<sup>m</sup> + y<sup>m</sup> = z<sup>m</sup> (mod p).

⒀ Erdős–Rényi Random Graph

> ① A very large graph emerges at a certain threshold, even with a simple rule of connecting nodes randomly.

⒁ Steiner tree algorithm

> ① Finding the spanning tree with the minimum total cost (weight)

> ② NP-hard

⒂ Goemans-Williamson (GW) algorithm

> ① Max-cut problem, SDP(semidefinite programming), Approximation algorithm

<br>

---

_Input: 2024.10.01 19:44_
