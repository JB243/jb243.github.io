## **第 18 讲。回归分析中的正则化**（正则化、惩罚）

推荐帖子：【统计】【统计目录】(https://jb243.github.io/pages/1641)

---

**1.** [概述](#1-概述)

**2.** [MSPE](#2-mspe)

**3.** **技术1.** [岭回归](#3-technique-1-ridge-regression)

**4.技术 2.** [LASSO 回归](#4-technique-2-lasso-regression)

**5.技术3.** [弹性网](#5-technique-3-elastic-net)

**6。技术 4.** [SelectFromModel](#6-technique-4-selecfrommodel)

---

<br>

## **1.概述**

 ⑴ 回归分析中的问题：主要在回归变量较多时突出

> ①【多重共线性】(https://jb243.github.io/pages/1632)

> ② **欠拟合**：模型缺乏灵活性，无法正确学习给定数据

> ③ **过拟合**

>> ○ 在 OLS 估计等标准回归中，模型会学习样本中的噪声，从而降低预测能力

>> ○ 在训练过程中学习偏差实际上可以提高预测能力

 ⑵ 正则化（惩罚）

> ① 解决回归问题，在参数中添加惩罚项

> ② 注意，不应用正则化会导致过拟合，过多会导致欠拟合

> ③ 必须对数据进行标准化处理

>> ○ 值较大的特征系数较大，可能会受到过度惩罚并收缩太多

>> ○ 相反，具有较小值的特征具有较小的系数，并且可能受到较少的惩罚

> ④ 有时包括使用验证集优化参数（例如惩罚项权重）的过程

> ⑤ 正规化预期效果

<br>

![图片](https://github.com/user-attachments/assets/d2dfb20f-a53c-48eb-be5f-4c9863b11647)

**图1.** 正则化的预期结果

<br>

<br>

## **2. MSPE** 

⑴ 概述

> ①误差：假设**e**为平方误差，h为假设，f为真函数

> ② **类型1.** 样本内误差：也称为训练误差。类似于偏见

<br>

![图片](https://github.com/user-attachments/assets/d4154696-e199-4aac-acba-0ef2ca6d7b92)

<br>

> ③ **类型2.** 样本外误差：也称为泛化误差，MSPE。类似于方差

<br>

![图片](https://github.com/user-attachments/assets/fd5278f3-fc41-42c7-8c76-20785f3166d2)

<br>

>> ○ **步骤 1.** 使用给定样本构建预测模型

>> ○ **步骤 2.** 使用样本外部的数据（XOOS、YOOS）比较预测值和实际值

>> ○ 注：ŷ 指使用样本内数据获得的预测

> ④（参考）【偏差-方差权衡】(https://jb243.github.io/pages/1630)

> ⑤ 最佳预测量：称为预言机。 E(YOOS | XOOS)

>> ○ MSPE中的预测误差如下

>> ○ 根本错误：无法改进。 YOOS \- E(YOOS | XOOS)

>> ○ 估计误差：Ŷ(XOOS) - E(YOOS | XOOS)

⑵ MSPE估计器

<br>

<img width="523" alt="스크린샷 2025-06-08 11 24 06" src="https://github.com/user-attachments/assets/773d49ed-b2d8-429a-b5c7-f29551edaf29" />

<br>

> ① 如果 β 已知，则 MSPE = σu2 成立

> ② k/n可能很大

 ⑶ 假设

> ① **假设1.** 无多重共线性

> ② **假设2.**（XOOS、YOOS）是从同一人群中随机抽取的

 ⑷ 变换

> ① 标准化

>> ○ (Xi1*,…,Xki*,Yi*)是从原始样本中提取的值

>> ○ 定义 Xji 为 (Xji* - μXj*) / σXj*

>> ○ 因变量变换为 Yσj ← Yσj \- μY*

> ② 收缩原理

<br>

<img width="214" alt="스크린샷 2025-06-08 11 24 44" src="https://github.com/user-attachments/assets/b3899cc4-59fb-4fdb-9fbb-a427fbd78c47" />

<br>

>> ○ 可以降低MSPE

>> ○ 出现偏差：权衡

>> ○ 最著名的例子是 James-Stein 估计器

 ⑸ 样本内MSPE计算：常用m倍交叉验证> ① 第一<sup>第一</sup>。将给定样本分成 m 部分

> ② 第二<sup>第二</sup>。使用 m-1 部分来估计参数：训练数据

> ③ 第三<sup>第</sup>。使用剩余部分来评估性能：测试数据

> ④ 第 4<sup></sup>。用不同的组合重复 m 次

> ⑤ 第 5<sup></sup>。取平均值来确定最终的估计器

<br>

<img width="311" alt="스크린샷 2025-06-08 11 25 30" src="https://github.com/user-attachments/assets/0d35062e-4ecc-4255-a50c-be1565ba472e" />

<br>

> ⑥ 通常使用10倍交叉验证

 ⑹ 样本外根MSPE计算

> ① 使用样本内数据训练的模型来评估不同样本上的性能

> ② 这个不同的样本称为验证集

<br>

<br>

## **3。技术 1.** 岭回归

 ⑴ 概述

> ① 定义：惩罚平方值以控制模型复杂性。惩罚是权重的函数

> ② 也称为L2正则化

> ③ 1962年由A. E. Hoerl提出，解决回归矩阵的不可逆性

<br>

<img width="135" alt="스크린샷 2025-06-08 11 26 03" src="https://github.com/user-attachments/assets/740a913f-d8ab-49dc-a023-6b9cb908f1c8" />

<br>

> ④【高斯分布的MAP学习】(https://jb243.github.io/pages/1768)

 ⑵ 目标函数

> ① 简单形式：岭估计器最小化

<br>

<img width="500" height="68" alt="스크린샷 2025-12-16 오후 2 01 34" src="https://github.com/user-attachments/assets/b27bb210-4de8-4ec6-b21e-71a77817b4ff" />

<br>

> ② PRSS（惩罚残差平方和）

<br>

<img width="321" alt="스크린샷 2025-06-08 11 26 54" src="https://github.com/user-attachments/assets/607e0942-18a3-41c3-bb16-83f08ab13493" />

<br>

 ⑶ **情况1.** 回归变量不相关

> ① 简单形式：可以相对于 λ = 0 时找到的 β̂j 表示

<br>

<img width="286" alt="스크린샷 2025-06-08 11 27 30" src="https://github.com/user-attachments/assets/048072fe-dad3-447f-a62e-1671a172dc06" />

<br>

> ② 矩阵形式：岭目标函数是凸的，可以通过微分轻松求解

<br>

<img width="294" alt="스크린샷 2025-06-08 11 27 51" src="https://github.com/user-attachments/assets/ee8e4970-6e63-4dae-8de0-11493aaa12ea" />

<br>

 ⑷ **情况 2.** 回归变量是相关的：必须检查 MSPE 与 λ<sub>Ridge</sub> 的关系

> ① 偏差-方差权衡

<br>

![图片](https://github.com/user-attachments/assets/53ed187b-d017-4f20-a458-59adc5d9bc61)

 **图 2.** 一般偏差-方差权衡

<br>

> ② λ<sub>Ridge</sub> 通过交叉验证计算

> ③ λ<sub>Ridge</sub> = 0 最适合样本内但不适用于样本外

<br>

![图片](https://github.com/user-attachments/assets/5af18b40-6f1b-4389-b6bb-374bed7662ef)

**图 3.** 根据 λ<sub>Ridge</sub> 计算 MSPE 的平方根

<br>

 ⑸ 岭回归解的特点

> ① 即使没有可逆的 XtX，λ 也允许计算逆

> ② 每个 λ 给出一个估计量

> ③ λ → 0：**过拟合**。达到线性回归 (OLS) 解

> ④ λ → ∞：**欠拟合**。系数 w 接近 0（**∵** 对大系数的惩罚）

 ⑹ **应用1.** 软序约束：最终变成||w||这样的不等式约束≤ C 进入等式约束

<br>

![图片](https://github.com/user-attachments/assets/0ff9777c-5b4e-49b2-920c-6b298cdbaacf)

<br>

 ⑺ **应用2.** 体重下降：治疗||**w**||像误差项一样并应用标准神经网络更新方法

> ① 标准梯度下降：**w** t \- η∇Ein(**w** t)

<br>

![图片](https://github.com/user-attachments/assets/ebbbe495-b9a3-4835-9573-d7401bd5bcd6)

<br>

 ⑻ **应用3.** MAP（最大后验）

> ① 贝叶斯法则

<br>![图片](https://github.com/user-attachments/assets/b2f1afcf-f18a-4b2d-bfbf-6627e4b0514a)

<br>

> ② 一般 MAP 学习：回想贝叶斯规则中“P(D) = 常数”的情况

<br>

![图片](https://github.com/user-attachments/assets/ec6239db-c8b6-4432-8a43-948fc1badaca)

<br>

>> ○ 假设正态分布：假设 **w** 除 w0 外与先验无关且很小

<br>

![图片](https://github.com/user-attachments/assets/b4b31271-b909-4682-9879-d971047ddd2f)

<br>

> ③岭回归中的MAP学习

<br>

![图片](https://github.com/user-attachments/assets/fdad8c06-e8c5-4117-90f1-297aa9920b9a)

<br>

 ⑼ **应用4.** 与其他方法的比较

<br>

![图片](https://github.com/user-attachments/assets/9ef36e8c-a75d-4a44-bbc7-9f624020d496)

**图 4.** 预测性能比较

<br>

<br>

 ## **4。技术2.** LASSO回归 

 ⑴ 概述

> ① 定义：惩罚绝对值以控制模型复杂性。惩罚是权重的函数

> ② 也称为L1正则化

> ③【拉普拉斯先验的MAP学习】(https://jb243.github.io/pages/1768)

<br>

![图片](https://github.com/user-attachments/assets/8248fe8a-8958-4f9e-b6f6-0b6813f36b91)

**图 5.** 拉普拉斯概率密度函数

<br>

 ⑵ 目标函数

> ① 简单形式：LASSO 估计器最小化 

<br>

<img width="502" height="66" alt="스크린샷 2025-12-16 오후 2 04 17" src="https://github.com/user-attachments/assets/17472bce-0c4b-4ab8-9d0c-7e386b9d757f" />

<br>

> ② 矩阵形式

<br>

![图片](https://github.com/user-attachments/assets/b76b1c97-f9ec-4380-91f0-e087e0ddb534)

<br>

 ⑶ 目标函数的求解：计算关于λ<sub>LASSO</sub>的MSPE

<br>

![图片](https://github.com/user-attachments/assets/91b145b1-8a25-4945-a1e6-eb3637992d13)

**图 6.** 根据 λ<sub>LASSO</sub> 的 MSPE 平方根

<br>

> ① λ<sub>LASSO</sub> 通过交叉验证计算

> ② 与 Ridge 不同，没有通用闭式解

 ⑷ 特点

> ① 当模型具有稀疏性时有用：即许多系数为 0

> ② λ → 0：达到线性回归（OLS）解。样本内最好，但样本外较差

> ③ λ → ∞：系数 w 接近 0（**∵** 对大系数的惩罚）

 ⑸ **应用1.** **稀疏性**原理

> ①拉普拉斯先验将不重要的变量精确设置为0：有效去除不重要的**变量**

<br>

![图片](https://github.com/user-attachments/assets/99b44618-cc1c-4a9a-a9dc-148e0ef8fc00)

 **图 7.** 系数随收缩因子的变化

<br>

> ② **稀疏原理图**

<br>

![图片](https://github.com/user-attachments/assets/555d350d-241a-43e0-b578-b7f71e0338ce)

 **图8.** 对LASSO回归稀疏性的直观理解

<br>

>> ○ 红色椭圆连接相等 MSE（均方误差）的点

>> ○ 蓝色区域连接等罚点

>> ○ 随着 λ 的增大，惩罚增大，LASSO 和 Ridge 都会收缩

>> ○ 在 Ridge 中，最优值出现在红色椭圆接触圆形蓝色区域的点：如果不是，解更接近原点，惩罚更小

>> ○ 在 LASSO 中，如果蓝色区域较小，则最优解出现在某些系数为 0 的点：在这个尖锐点，沿边缘的移动退出红色椭圆（→ 较高的 MSE）

>> ○ 与 Ridge 不同，LASSO 会导致稀疏性

 ⑹ **应用2.** 与其他方法的比较

<br>

![图片](https://github.com/user-attachments/assets/643d52a4-36e7-4ed8-8ba6-92c677fea17b)

**图 9.** 预测性能比较

<br>

<br>

## **5。技术3.** 弹性网

 ⑴ LASSO 和 Ridge 的线性组合。添加权重的绝对值和和平方值作为惩罚项

<br><img width="545" alt="스크린샷 2025-06-08 11 32 22" src="https://github.com/user-attachments/assets/3aee984e-b49e-4c29-bb7e-36ebc8d8919a" />

<br>

 ⑵ **参数1.** Alpha（α）：控制L1和L2惩罚的混合比例。 α = 1 为 LASSO，α = 0 为 Ridge

 ⑶ **参数2.** Lambda (λ)：控制惩罚的强度。乘以整个正则化项

<br>

<br>

## **6。技术 4.** SelectFromModel

 ⑴ 一种基于【决策树】(https://jb243.github.io/pages/2161)算法选择变量的方法

<br>

---

_输入：2019.12.08 12:35_

_编辑：2024.09.27 08:47_