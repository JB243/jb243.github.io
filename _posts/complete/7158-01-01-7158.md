## **第 7 章. 降维算法**

推荐帖子：【算法】【算法索引】(https://jb243.github.io/pages/1278)

---

**1.** [概述](#1-概述)

**2.** **类型 1.** [主成分分析 (PCA)](#2-type-1-principal-component-analysis-pca)

**3\.类型 2.** [SNE、对称-SNE、tSNE](#3-type-2-sne-对称--sne-tsne)

**4\.类型 3.** [UMAP](#4-type-3-umap)

**5\.类型 4.** [SVD](#5-type-4-奇异值分解-svd)

**6\.类型 5.** [ICA](#6-type-5-ica)

**7. 6 型.** [PPCA](#7-type-6-ppca)

**8. 7 型.** [CCA](#8-type-7-cca)

**9\.** [其他](#8-其他)

---

**a.** [SNE、对称-SNE、tSNE](https://jb243.github.io/pages/1730)

**b.** [UMAP](https://jb243.github.io/pages/2203) 

---

<br>

## **1\.概述**

 ⑴ 降维的定义

> ① 当给定高维数据 X<sub>(1)</sub>, ···, X<sub>(n)</sub> ∈ ℝ<sup>p</sup>,

> ② 找到合适的[子空间](https://jb243.github.io/pages/1897) V ⊂ ℝ<sup>p</sup> （其中dim(V) = k），

> ③计算X在V上的投影，记为X̃。

> ④ 是一种无监督算法。

⑵ 特点

> ① 信息保存

> ② 更容易的模型训练：在机器学习中，它需要的参数更少，因此速度更快，计算量更低。

> ③ 更容易的结果解释：高维数据的可视化。

> ④ 降噪

> ⑤ 能够揭示给定数据的真实维数。

 3.方法

> ① 特征选择

>> ○ 从现有变量中选择一些重要变量，并丢弃其余变量。

>> ○ 选择一个具有高相关性或高方差膨胀因子 (VIF) 的变量。

> ② 特征提取

>> ○ 组合所有变量以得出最能代表数据的新变量。

>> ○ 结合现有变量来创建新变量的技术。

<br>

<br>

## **2\.类型 1.** 主成分分析 (PCA)

⑴ 概述

> ①定义：将相关变量转化为正交变量的降维技术。

> ② 也就是说，它是一种只留下少数有意义的坐标系的降维方法。

> ③ 正交变量称为主成分，当其包含与原始数据一样多的信息时即可使用。

> ④ 源于对给定维度和数据真实维度可能不同的认识。

> ⑤ 1901 年由皮尔逊提出。

> ⑥ PCA不仅可以在子空间（经过原点）定义，也可以在仿射空间定义。

> ⑦ **** 按均值归一化，然后对子空间进行 PCA。

> ⑧ PCA 不要求给定数据的多元正态性。

> ⑨ 当数据呈线性分布时可以使用。

> ⑩ 白化：进行标准化时最好沿主成分方向对特征进行标准化。

⑵ 经营场所

> ① 将数据所属空间的基表示为e<sub>1</sub>,…,e<sub>p</sub>（基集不需要是正交的）。

>> ○ e<sub>1</sub>,..., e<sub>p</sub> 可以被认为是表示给定维度的基（不需要正交性）。

> ② 目标：找到这样一个子空间 (k < n) 的正交基 Z<sub>1</sub>,..., Z<sub>k</sub>（典型的无监督问题设置）。

>> ○ Z<sub>1</sub>,..., Z<sub>k</sub> 可以被认为是代表数据真实维度的基数。

<br>

![图片](https://github.com/user-attachments/assets/a040256a-0f86-4033-a62e-5a0b740e92d4)

<br>

> ③ **随机向量**

>> ○ X 代表一个特定的数据点：x<sub>1</sub>、…、x<sub>n</sub> 是每个分量。

<br>

![图片](https://github.com/user-attachments/assets/15c6f2f7-84d2-4c1e-980d-6b64528b5949)

<br>

> ④ **均值向量**：也称为总体均值。

<br>![图片](https://github.com/user-attachments/assets/a2a5e5c0-320a-4f1b-a3c0-c2f2d07055ef)

<br>

> ⑤ **方差-协方差矩阵**

<br>

![图片](https://github.com/user-attachments/assets/0e9b988b-f0ef-4a41-b8df-e10caf819d54)

<br>

> ⑥ **无偏样本方差-协方差矩阵**：用设计矩阵**X**，和中心矩阵**X<sub>c</sub>**，

<br>

![图片](https://github.com/user-attachments/assets/c937618b-0adc-486c-b16d-4eb67d43f50e)

<br>

>> ○ 当总体方差-协方差矩阵未知时使用。

>> ○ 由于 **X** ε ℝ<sup>n×p</sup>，**X**<sup>T</sup>**X** ε ℝ<sup>p×n</sup> × ℝ<sup>n×p</sup> = ℝ<sup>p×p</sup>。

> ⑦ 目标正交基的表达

<br>

![图片](https://github.com/user-attachments/assets/08803bfd-bdfb-43bc-8f23-bed4bbf0e675)

<br>

>> ○ Z<sub>k</sub>: = k-th PC ∈ ℝ<sup>p×1</sup>.

> ⑧ 目标正交基的特点：很容易从线性代数概念中推导出来。

<br>

![图片](https://github.com/user-attachments/assets/37c61a4f-5048-4d0b-a22d-073835dcd87a)

<br>

>> ○ Var、Cov等，是与方差相关的具体值，属于ℝ<sup>1x1</sup>。

>> ○ 不要与方差-协方差矩阵混淆。

⑶ PCA问题的定义

<br>

![图片](https://github.com/user-attachments/assets/724c5796-2854-43e5-833b-158fed21bd29)

<br>

⑷ PCA问题的解决方法

> ① **方法1.** Σ 的特征分解。

>> ○ **第1步：**数据构建和居中

>>> ○ **X** ∈ ℝ<sup>n×d</sup>：具有 _n_ 个样本和 _d_ 个特征的数据矩阵。

>>> ○ 将数据矩阵 **X** 居中： **X**<sub>c</sub> = **X** - **μ** 其中 **μ** ∈ ℝ<sup>d</sup> 是逐列均值的向量）

>> ○ **第2步：**协方差矩阵计算

>>> ○ 根据中心数据计算样本协方差矩阵 **C** ∈ ℝ<sup>d×d</sup>： 

<br>

<img width="99" alt="스크린샷 2024-11-16 10 28 08" src="https://github.com/user-attachments/assets/ef233ea5-b06a-480b-8c9c-7738b79e98b5">

<br>

>>> ○ 由于给定样本代表整个总体，总体均值已知，因此样本协方差是通过除以 n 而不是 n−1 来计算的。

>> ○ **步骤3：**特征分解

>>> ○ 对协方差矩阵_C_进行特征分解，计算特征值和特征向量： **C** · **p**<sub>i</sub> = λ<sub>i</sub> · **p**<sub>i</sub>

>>> ○ 这里，λ<sub>i</sub> 表示特征值，**p**<sub>i</sub> ​ε R<sup>d</sup> 是特征向量。

>>> ○ 注意：对称矩阵 C 始终可以正交对角化（通过[谱定理](https://jb243.github.io/pages/1928)）。

>> ○ **第4步：**数据转换

>>> ○ 定义 **P** = [**p**<sub>1</sub>, **p**<sub>2</sub>, …, **p**<sub>d</sub>] ∈ R<sup>d×d</sup>，一个矩阵，其中特征向量存储为列。

>>> ○ 将数据转换到新的坐标系： **Y** = **X**<sub>c</sub>**P** 其中 **Y** ∈ R<sup>n×d</sup> 是转换后的数据矩阵。

>> ○ **步骤 5：** 协方差矩阵的对角化

>>> ○ 变换后的数据 **Y** 的协方差矩阵变成对角线：

<br>

<img width="445" alt="스크린샷 2024-11-16 10 32 40" src="https://github.com/user-attachments/assets/b6254144-078c-45ac-a26a-2f7999eeb9e8">

<br>

>> ○ **第6步：** 主成分选择和降维

>>> ○ 选择最大特征值 λ<sub>i</sub> 对应的_k_个主成分（特征向量）并降维： **Y**<sub>k</sub> = **X**<sub>c</sub>**P**<sub>k</sub> with **P**<sub>k</sub> ε ℝ<sup>d×k</sup>, **Y**<sub>k</sub> ε ℝ<sup>n×k</sup> 

>>> ○ 或者，不固定主成分的数量，而是可以排除小于特定阈值 η 的特征值。>> ○ **结论1.** 第d个主成分PC<sub>d</sub>是第d个最大特征值对应的特征向量。

>> ○ **结论2.** 特征值相同的特征向量的含义：对应主成分的大小相同。

> ② **方法2.** 中心矩阵的奇异值分解（SVD） **X**<sub>c</sub>：目前PCA算法中使用的标准方法。

>> ○ **步骤1：**将数据居中，得到居中数据**X**<sub>c</sub>。

>> ○ **步骤2：**定义​**C** = (1/n) **X**<sub>c</sub><sup>**T**</sup>**X**<sub>c</sub> = Σ（协方差矩阵）
​
>> ○ **步骤 3：** 由于 **C** 是对称的，因此可以将其正交对角化为 **C** = **VDV**<sup>T</sup> （其中 **V** 与 **X**<sub>c</sub> = **U**Σ**V**<sup>T</sup> 中的 **V** 相同）

>> ○ **步骤 4.** **V** 的第 1<sup>st</sup>、2<sup>nd</sup>、... 列向量是第 1<sup>st</sup>、2<sup>nd</sup>、... 最大特征值对应的特征向量，是主成分。

> ③ **方法3.** 【等式约束下的拉格朗日乘子】(https://jb243.github.io/pages/1813)。

>> ○ **步骤 1.** 通过求解 **w** 来获得第一主成分，该 **w** 在约束ㅣ**w**ㅣ = 1 的条件下最大化 Var(**X**<sub>c</sub>**w**)。应用拉格朗日乘数方法会导致 C**w** = λ**w** 形式的条件，从而简化为特征值问题。

>> ○ **步骤 2.** 然后以相同的方式顺序找到第二个和第三个主成分，并附加约束，即它们与先前找到的成分正交（或不相关）。

 ⑸ 确定轴数

> ① **方法 1.** 总方差分数的卵石图。

>> ○ 总方差分数的定义。

<br>

<img width="204" height="58" alt="스크린샷 2025-12-17 오전 2 06 52" src="https://github.com/user-attachments/assets/5a546c32-4cc3-42e5-875d-da04b1507176" />

<br>

>> ○ 图形表示。

<br>

![图片](https://github.com/user-attachments/assets/da97e824-00d1-4e02-a058-4d54b40e2d7f)

**图 1.** 轴设置示例。

<br>

![图片](https://github.com/user-attachments/assets/888e17f2-c14b-4b9f-964d-97e2001fdb2c)

**图 2.** 碎石图：显示每个主成分解释的方差量。

<br>

> ② **方法2.** 基于特征值的碎石图

>> ○ 绘制有序特征值 λ<sub>1</sub>, …, λ<sub>p</sub> 并识别曲线中的肘部（弯曲）。

>> ○ 选择直到肘点的 PC 数量，此时剩余的特征值变得相对较小且大小相似。

> ③ **方法3.** 通过最小化MSPE来选择p 

>> ○ 改变 PC p 的数量（例如，使用样本内 10 倍交叉验证）并评估预测模型。

>> ○ 选择使 MSPE 最小化的最佳 p（或 RMSE = √MSPE）。

<br>

![图片](https://github.com/user-attachments/assets/d9cf92b1-9211-45bd-a984-c8d3c55a966b)

**图 3.** 轴数量的确定。

<br>

> ④ 由于剩余轴的信息丢失，PCA 起到有损压缩算法的作用。

⑹ Python代码示例

<br>

__受保护_0__

<br>

⑺ PCA的发展

> ① 内核PCA：使用[kernels](https://jb243.github.io/pages/1897)。可以将给定数据扩展到高维度，也可以通过投影将其降低到较低维度。

> ② MDS（多维尺度）

> ③ SNE、t-SNE

> ④ 乌玛普

<br>

<br>

## 3.类型2。[SNE、对称SNE、tSNE](https://jb243.github.io/pages/1730)

⑴ 一种试图保留数据中局部邻域的降维算法。

⑵ 非线性和非确定性。

⑶ 需要了解[深度学习]相关的成本函数、更新算法等知识(https://jb243.github.io/pages/1138)。

<br><br>
 
## 4\.输入 3。 [UMAP](https://jb243.github.io/pages/2203)

<br>

<br>

## **5\.类型 4.** 奇异值分解 (SVD)

⑴从M×N维矩阵中提取奇异值并降低给定数据维数的方法。

⑵ 公式

> ①小结：若 A 是秩为 r 的 m × n 矩阵，则存在满足以下条件的 ℝm×m 矩阵 U 和 ℝn×n 矩阵 V：

<br>

<img width="393" alt="스크린샷 2024-10-22 11 44 56" src="https://github.com/user-attachments/assets/3f9d04a4-797c-49ec-9152-4c62dfa74cae">

<br>

>> ○ U 和 V 是正交矩阵。

>> ○ 由于改变U的每一列和V的对应列的符号仍然满足上式，因此SVD不是唯一确定的。

>> ○ 矩阵 Σ：对角线上有 r 个非零奇异值 (> 0) 的 m × n 矩阵。

<br>

<img width="97" alt="스크린샷 2024-10-22 11 34 47" src="https://github.com/user-attachments/assets/69e9e2b5-a161-46b1-b66d-228a7b78d62c">

<br>

> ② 全程

<br>

![图片](https://github.com/user-attachments/assets/4f94b661-115e-4360-9668-cb95dfd3df98)

<br>

> ③ 奇异值定义为 AA<sup>T</sup> 或 A<sup>T</sup>A 特征值的正平方根。

>> ○ AA<sup>T</sup> 和 A<sup>T</sup>A 零特征值的数量可能不同，但它们产生相同的非零特征值。

>> ○ 非零奇异值的数量等于rank(A)。

>> ○ 零奇异值的数量为 max(dim(A)[0], dim(A)[1])−rank(A)。

<br>

<img width="402" alt="스크린샷 2024-11-29 12 54 54" src="https://github.com/user-attachments/assets/982f6231-7509-4f4f-bd87-7a60fa55bf04">

<br>

> ④Python代码

<br>

__受保护_1__

<br>

⑶ **应用1.** A<sup>T</sup>A = V(Σ<sup>T</sup>Σ)V<sup>T</sup> ：A的奇异值是A<sup>T</sup>A特征值的平方根。然而，根据等级，一些特征值被设置为0。

⑷ **应用2.** 简化SVD：请注意，对于简化版本，U 和V 都有正交列。这意味着 U<sup>T</sup>U = I 且 V<sup>T</sup>V = I。但是，在此版本中 U 和 V 不是方阵，因此它们不是正交矩阵。

<br>

![图片](https://github.com/user-attachments/assets/ac360053-c3ee-48e0-aa9c-8df67c0de893)

<br>

__受保护_2__

<br>

⑸ **应用3.** k 阶近似

> ① 对于 k <rank(A)，可以将 m × n 矩阵 A 拆分为 m × k + k × n 矩阵来实现压缩。

<br>

![图片](https://github.com/user-attachments/assets/1c811701-96a9-4742-8fd9-754983d8fe7d)

<br>

> ② 给定k，如何求A的最优k阶近似：

>> ○ A = UΣV<sup>T</sup> = σ<sub>1</sub>**u**<sub>1</sub>v<sub>1</sub><sup>T</sup> + ··· + σ<sub>r</sub>**u**<sub>r</sub>**v**<sub>r</sub><sup>T</sup>，其中 σ<sub>1</sub> ≥ ⋯ ≥ σ<sub>r</sub>，删除较小的奇异值 σ<sub>r</sub>，σ<sub>r−1</sub>，...。

>> ○ 这样，A<sup>(k)</sup> 和 A 之间的弗罗贝尼乌斯距离定义如下：

<br>

![图片](https://github.com/user-attachments/assets/570427b5-9fee-4986-8b08-a38539a9d781)

<br>

⑹ **应用4.** PCA（主成分分析）

> ① **步骤 1.** 将数据居中

<br>

<img width="393" alt="스크린샷 2024-11-16 8 45 10" src="https://github.com/user-attachments/assets/fc2361b9-742d-4697-a744-aee1af619126">

**图。 4.** PCA的数据中心化

<br>

<img width="409" alt="스크린샷 2024-11-16 8 45 28" src="https://github.com/user-attachments/assets/3e222a33-e7ae-4ccc-8560-3692470c786d">

<br>

> ② **步骤2.** **X**<sub>c</sub> 是居中数据，**C** = (1/n) **X**<sub>c</sub><sup>T</sup>**X**<sub>c</sub>> ③ **步骤3.** **C** 是对称的，因此可正交对角化为 **C** = **VDV**<sup>T</sup>。 **V** 与 SVD 中的 **X**<sub>c</sub> = **U**Σ**V**<sup>T</sup> 相同。 

> ④ **步骤 4.** **V** 的第 1<sup>st</sup>、2<sup>nd</sup>、... 列向量是第 1<sup>st</sup>、2<sup>nd</sup>、... 最大特征值对应的特征向量，是主成分。

> ⑤ **步骤 5.** 如果您想仅根据前 2 个 PC 获取 X 中向量的新坐标，只需将 **x**<sub>i</sub> （长度为 p 的向量）投影到前两个 PC 上即可：

<br>

<img width="401" alt="스크린샷 2024-11-29 12 03 43" src="https://github.com/user-attachments/assets/ad3b43af-cf73-4b8c-913c-3be17ae06063">

<br>

<br>
 
## **6\.类型 5.** 独立成分分析 (ICA)

⑴ 与主成分分析不同，它是一种降维技术，将多元信号统计分离为独立的子成分。

⑵ 例如$N$个麦克风区分$M$个对话的场景。

⑶ 独立成分的分布服从非高斯分布。

⑷ 它利用[矩阵分解](https://jb243.github.io/pages/956)。

<br>

<br>

## **7\.类型 6.PPCA**

⑴ 配方

> ① **步骤1.** 假设观测数据x是由潜在变量z生成的：以下变换本身成为一种降维技术。

<br>

<img width="145" alt="스크린샷 2024-11-07 11 56 26" src="https://github.com/user-attachments/assets/f2c679ac-9f18-42c7-8930-3c83555d29c0">

<br>

>> ○ W：维度 D × d 的变换矩阵（用于从潜在空间映射到观察空间）

>> ○ b: D 维平均向量

>> ○ ϵ：噪声，服从正态分布 N(0,σ<sup>2</sup>I)

> ② **步骤2.** 对潜变量z的假设

<br>

<img width="145" alt="스크린샷 2024-11-07 11 56 18" src="https://github.com/user-attachments/assets/5d674833-40fb-4622-a7e1-aabde181ee86">

<br>

> ③ **步骤 3.** x 给定 z 的条件分布

<br>

<img width="264" alt="스크린샷 2024-11-07 11 56 49" src="https://github.com/user-attachments/assets/cc266b17-e1e1-4ce6-b810-7fd3138368aa">

<br>

> ④ **步骤 4.** 边际分布 p(x)

<br>

<img width="264" alt="스크린샷 2024-11-07 오전 11 57 08" src="https://github.com/user-attachments/assets/f3a1fd86-f285-43d0-ae14-5f4c4c320074">

<br>

> ⑤ **步骤 5.** MLE 估计

>> ○ 最大似然估计 (MLE) 用于根据给定数据估计 b、W 和 σ。

>> ○ MLE 解是以封闭形式获得的。

>> ○ 当 σ → 0 时，该方法收敛于标准 PCA。

⑵ 代码

<br>

__受保护_3__

<br>

<br>

## **8\.类型 7.CCA**

⑴ CCA（典型相关分析）

> ① 问题陈述：给定数据矩阵 X ∈ ℝ<sup>n×p</sup> 和 Y ∈ ℝ<sup>n×q</sup>，目标是最大化变换变量的相关系数。  

<br>

<img width="253" alt="스크린샷 2025-01-29 오후 11 17 08" src="https://github.com/user-attachments/assets/e186b29f-22ea-4805-b3c2-c13737edfa7e" />

<br>

>> ○ Σ<sub>xx</sub> = X<sup>T</sup>X: X ∈ ℝ<sup>p×p</sup> 的协方差矩阵
   
>> ○ Σ<sub>yy</sub> = Y<sup>T</sup>Y：Y ∈ ℝ<sup>q×q</sup> 的协方差矩阵 

>> ○ Σ<sub>xy</sub> = X<sup>T</sup>Y: X 和 Y 之间的互协方差矩阵 ε ℝ<sup>p×q</sup>
   
>> ○ Σ<sub>yx</sub> = (Σ<sub>xy</sub>)<sup>T</sup> 

> ② 解决方法：将问题重新表述为优化问题，约束条件为：w<sub>x</sub><sup>T</sup>Σ<sub>xx</sub>w<sub>x</sub> = 1 和 w<sub>y</sub><sup>T</sup>Σ<sub>yy</sub>w<sub>y</sub> = 1，并应用[拉格朗日乘子]方法](https://jb243.github.io/pages/1813)。

<br><img width="639" alt="스크린샷 2025-01-29 오후 11 21 08" src="https://github.com/user-attachments/assets/300c8ecc-a72f-4782-98e5-dad6481509f7" />

<br>

> ③ 结论：典型相关性由矩阵广义特征值的正平方根给出：Σ<sub>xx</sub><sup>-1</sup>Σ<sub>xy</sub>Σ<sub>yy</sub><sup>-1</sup>Σ<sub>yx</sub>。

⑵ DCCA（对角CCA）

> ① 问题陈述：给定数据矩阵 X ∈ ℝ<sup>n×p</sup> 和 Y ∈ ℝ<sup>n×q</sup>

<br>

<img width="258" alt="스크린샷 2025-01-29 오후 11 23 34" src="https://github.com/user-attachments/assets/f206c8d8-534b-4dc3-a536-e3bd54fbd0e5" />

<br>
  
>> ○ D<sub>x</sub> = diag(Σ<sub>xx</sub>)：仅使用协方差矩阵的对角线元素  

>> ○ D<sub>y</sub> = diag(Σ<sub>yy</sub>)：仅使用协方差矩阵的对角线元素  

> ② 过程：[SVD](#5-type-4-singular-value-decomposition-svd) 的奇异值确定典型相关值。  

<br>

<img width="401" alt="스크린샷 2025-01-29 오후 11 25 55" src="https://github.com/user-attachments/assets/8fe785c0-f693-40f7-81bc-ec00738235b8" />

<br>

⑶ CCA与DCCA的比较

<br>

|方法| CCA | DCCA |
|------------------------|----------------------------------------------------|----------------------------------------|
|协方差矩阵|使用完整矩阵 (\( \Sigma_{xx}, \Sigma_{yy} \)) |仅对角线元素 (\( D_x, D_y \)) |
|解决方案 |广义特征值问题 |奇异值分解|
|计算成本|需要矩阵求逆（成本高）|对角矩阵运算（低成本） |
|结果解读 |特征向量、特征值 |奇异向量、奇异值 |

**表 1.** CCA 和 DCCA 的比较

<br>

<br>
 
## **9\.其他**

⑴因素分析

> ① 定义：一种通过根据相似变量的相关性对相似变量进行分组来解释数据内部结构的方法。

> ② 假设存在数据中无法观察到的潜在变量。

> ③ 常用于社会科学或调查。

⑵ 多维标度（MDS）

> ① 定义：通过识别数据中潜在的模式和结构来可视化低维空间中对象的接近度和分组的统计技术。

> ② 测量对象之间的相似性和相异性，并将它们表示为 2D 或 3D 空间中的点。

⑶ 线性判别分析（LDA）

> ①定义：通过将数据投影到低维空间来降维的方法，类似于PCA。

⑷ RPCA（倒数主成分分析）

⑸ [自动编码器](https://jb243.github.io/pages/956) 

> ① [变分自动编码器（VAE）](https://jb243.github.io/pages/956#2-type-1-variational-autoencoder)

> ② [图自动编码器（GAE）](https://jb243.github.io/pages/956#3-type-2-graph-autoencoder)

> ③【矩阵分解与NMF】(https://jb243.github.io/pages/956#4-type-3-matrix-factorization) 

> ④【潜在主题模型】(https://jb243.github.io/pages/956#5-type-4-latent-topic-model)：潜在语义索引(LSI)、潜在狄利克雷分配(LDA)等。

⑹ 信号模式分析（SPA）（[ref1](https://www.nature.com/articles/s43588-024-00734-0)、[ref2](https://www.perplexity.ai/search/ieee-reviews-on-graph-signal-p-woIqh1tHR7u9oYzt5ODDmw#0))

> ① SPA 没有使用[谱聚类](https://jb243.github.io/pages/616)中使用的传统拉普拉斯矩阵，而是利用从图中导出的马尔可夫转移矩阵 P<sup>t</sup> 的力量来捕获不同扩散时间 t 的信息。> ② 较短的扩散时间 t 对局部模式敏感，而较长的时间反映更多的全局结构，利用这一特性进行分析。

⑺ 磷酸盐

⑻ 官方发展援助

⑼ 光引擎

⑽ LSA

⑾ [PLSA](https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis)（概率潜在语义分析）

> ① 用于从潜在类模型中分解混合组件。

> ② 应用：用于质谱数据降维。

<br>

---

_输入：2021.12.3 16:22_