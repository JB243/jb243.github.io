## **第 11-1 章。多臂强盗**

推荐帖子：【算法】【算法目录】(https://jb243.github.io/pages/1278)

---

**1.** [概述](#1-概述)

**2.** [UCB](#2-ucb)

**3.** [汤普森采样](#3-汤普森采样)

**4.** [提前停止问题](#4-提前停止问题)

---

<br>

## **1\.概述**

⑴ 概述

> ① 问题定义：选择收益最高的最优臂

> ② 以下情况尤其必要

>> ○ 目标函数计算困难时（黑盒算法）

>> ○ 当没有明确的解析形式时

>> ○ 存在奇异值时

>> ○ 当函数不可微时

>> ○ 当数据噪声很大时

⑵ **两种策略**的权衡

> ① **利用：** 使用从当前数据得出的经验平均值最大化奖励

> ② **探索：** 改进经验均值以匹配真实均值

> ③ 如果初始数据偏离真实分布而忽视探索，则错误的判断无法得到纠正

⑶ **类型 1.** **贝叶斯优化**

> ① 【贝叶斯定理】(https://jb243.github.io/pages/1623#:-,,-\(Bayestheroem\))

<br>

<img width="303" alt="스크린샷 2024-11-21 오후 9 54 54" src="https://github.com/user-attachments/assets/609dbe5a-886f-4253-9ae2-47cddc97f47f">

<br>

>> ○ 给出各臂的奖励分布的先验分布（代理模型）

>> ○ 高斯过程常被用作先验模型

>> ○ 每次观察到奖励时都会获得后验分布

> ② 获取功能：进行采样以最大化下一个奖励

<br>

<img width="299" alt="스크린샷 2024-11-21 오후 9 55 12" src="https://github.com/user-attachments/assets/e1c71643-1509-44af-809f-eeeadb5040a9">

<br>

>> ○ 所有采集函数根据后验分布计算均值μ(x)和方差σ2(x)，通过此设计探索策略

>> ○ **类型 1.** EI（预期改进）：EI(x) 的第二个公式仅在使用高斯过程时成立

>>> ○ 注意，Φ和𝜙是标准正态分布的CDF和PDF，Z标准化x

>>> ○ λ 越高，探索越多

<br>

<img width="550" alt="스크린샷 2024-11-21 오후 9 55 34" src="https://github.com/user-attachments/assets/d9bbf263-beb1-41e7-b78a-42e0dff9630b">

<br>

>> ○ **类型 2.** [MPI](https://en.wikipedia.org/wiki/Bayesian_optimization)（最大改进概率）

<br>

<img width="277" alt="스크린샷 2024-11-26 오후 1 41 21" src="https://github.com/user-attachments/assets/bedbeb8c-ac94-423e-bb44-d425bd9a524d">

<br>

>> ○**类型 3.** [UCB](https://jb243.github.io/pages/2160)

<br>

<img width="332" alt="스크린샷 2024-11-26 오후 1 41 44" src="https://github.com/user-attachments/assets/7f430921-3fa4-4277-a0cf-cd2f92c5ec56">

<br>

> ③ 高斯过程：贝叶斯优化时假设正态分布

<br>

<img width="496" alt="스크린샷 2024-11-21 오후 9 55 53" src="https://github.com/user-attachments/assets/99009a47-3644-4d92-ae4a-545977802f9d">

<br>

__受保护_0__

<br>

> ④ 超参数优化

>> ○ 使用最大似然估计 (MLE) 探索最大化边际似然（数据 X 和 y 出现的概率）的超参数

>> ○ 边际似然和高斯过程超参数定义如下

<br>

<img width="602" alt="스크린샷 2024-11-21 오후 9 57 39" src="https://github.com/user-attachments/assets/88190619-192d-4b20-b939-78f055e87461">

<br>

>> ○ 优点：改善欠拟合和过拟合

⑷ **类型 2.** 随机多臂老虎机

> ① **特征1.** 在线决策：每次t = 1,…,T时从N个臂中选择第i个臂> ② **特征2.** 随机反馈：每只手臂提供的奖励遵循固定但未知的分布

> ③ **功能3.** Bandit Feedback：在每个时间步，只有所选手臂的奖励可见

> ④ 目标：在T步内最大化累积预期奖励

<br>

<img width="295" alt="스크린샷 2024-11-21 오후 9 58 06" src="https://github.com/user-attachments/assets/a684d9f4-8b4b-4fa4-b891-15b64dabdf66">

<br>

> ⑤ 最大化奖励相当于最小化后悔 = μ* - μ<sub>t</sub>

<br>

<img width="415" alt="스크린샷 2024-11-21 오후 9 58 40" src="https://github.com/user-attachments/assets/bb655e97-3a65-484e-8ba7-2eb4e1ba84bb">

<br>

> ⑥ 强盗：指失败后夺走的意思，像强盗一样

> ⑦ **示例 1.** N 个臂中选择第 n 个臂时的置信状态更新方程

<br>

<img width="502" height="112" alt="스크린샷 2025-11-09 11 30 04" src="https://github.com/user-attachments/assets/30af6399-7efa-4e98-a210-a8caabb200d7" />

<br>

> ⑧ **示例 2.** [汤普森采样](#3-thompson-sampling)

<br>

<br>

## **2\. UCB** 

⑴ 与 Thompson Sampling 一起，是使用最广泛的老虎机算法之一

⑵配方

> ① 时间 t 期间第 i 组的经验平均值

<br>

<img width="177" alt="스크린샷 2024-11-21 오후 9 58 56" src="https://github.com/user-attachments/assets/d05c8ae2-0732-43a3-b5ae-695d8e1bdb85">

<br>

>> ○ 分子：第 i 臂获得的总奖励

>> ○ I{is = i}：只有当is = i时才输出1，否则输出0

>> ○ ni,t: 直到时间 t 为止 i 被选择的次数

> ② UCB（置信上限）

<br>

<img width="498" height="73" alt="스크린샷 2025-11-09 오후 1 03 18" src="https://github.com/user-attachments/assets/856dedcf-eba4-4b82-bf8f-db3630fa2964" />

<br>

>> ○ **乐观：** 意思是大概率认为高于实际预期值

>> ○ 高概率：99.9%以上

> ③ 选择每一步UCB最大的手臂

<br>

<img width="158" alt="스크린샷 2024-11-21 오후 9 59 44" src="https://github.com/user-attachments/assets/0ed3683f-9bde-48ab-a5c2-a063705199a6">

<br>

> ④ 在每个时间步更新经验均值和 UCB：UCB 表示的不确定性随着每次观察而减小

⑶ 收敛性证明

> ①定理

>> 对于所有 N > 1，如果策略 UCB 在具有任意奖励分布 P<sub>1</sub>, ..., P<sub>N</sub> 且支持 [0, 1] 的 N 个臂上运行，则在任意次数的 T 次游戏后其预期后悔

<br>

<img width="504" height="73" alt="스크린샷 2025-11-09 오후 1 30 41" src="https://github.com/user-attachments/assets/861afb57-5836-45a8-9553-cfa27e1023ac" />

<br>

>> 最多是 

<br>

<img width="336" height="87" alt="스크린샷 2025-11-09 오후 1 31 17" src="https://github.com/user-attachments/assets/995420ba-3815-4cbc-a489-349ad2dc8130" />

<br>

>> 其中 μ<sub>1</sub>, ..., μ<sub>N</sub> 是分布 P<sub>1</sub>, ..., P<sub>N</sub> 的均值。

> ② 证明 

>> 令 c<sub>t,s</sub> := √(2log(t) / s)。还定义如下 X̄<sub>n</sub><sup>i</sup> = (1/n) Σ<sub>t=1 to n</sub> X<sub>t</sub> 其中 {X<sub>t</sub><sup>i</sup>}<sub>t∈ℕ</sub> 是连续播放时第 i 组奖励的随机过程。对于任何手臂 i，我们在任何比赛序列上设置 UCB<sub>i</sub>(T) 的上限。令 I<sub>t</sub> 表示在时间 t 玩的手臂，那么我们可以得到 ℓ 是任意正整数 

<br>

<img width="550" height="286" alt="스크린샷 2025-11-09 오후 1 35 15" src="https://github.com/user-attachments/assets/0854c9af-03f1-44cd-be1d-7edaf0ea379f" />

<br>>> 现在我们观察到 X̄<sub>s</sub>* + c<sub>t,s</sub> ≤ X̄<sub>s<sub>i</sub></sub><sup>i</sup> + c<sub>t,s<sub>i</sub></sub> 意味着至少必须满足以下条件之一： X̄<sub>s</sub>* ≤ μ* - c<sub>t,s</sub>， X̄<sub>s<sub>i</sub></sub><sup>i</sup> ≤ μ<sub>i</sub> + c<sub>t,s<sub>i</sub></sub>, μ* < μ<sub>i</sub> + 2c<sub>t,s<sub>i</sub></sub>。否则，存在矛盾：X̄<sub>s</sub>* + c<sub>t,s</sub> > μ* ≥ μ<sub>i</sub> + 2c<sub>t,s<sub>i</sub></sub> > X̄<sub>s<sub>i</sub></sub><sup>i</sup> + c<sub>t,s<sub>i</sub></sub>。我们可以通过使用 Azuma-Hoeffding 不等式的版本来限制前两个事件的概率，因为 sX̄<sub>s</sub>* - sμ* 和 s<sub>i</sub>X̄<sub>s<sub>i</sub></sub><sup>i</sup> - s<sub>i</sub>μ<sub>i</sub> 在 0 处的边际为零，增量为 [-μ*, 1分别为 - μ*] 和 [-μ<sub>i</sub>, 1 - μ<sub>i</sub>]。然后产生以下结果

<br>

<img width="399" height="97" alt="스크린샷 2025-11-09 오후 1 40 35" src="https://github.com/user-attachments/assets/84284d0e-f00e-46ef-bb38-a8f4776699e2" />

<br>

>> 另请注意，我们有 

<br>

<img width="652" height="80" alt="스크린샷 2025-11-09 오후 1 41 10" src="https://github.com/user-attachments/assets/d3c83c88-f572-4b2b-a74a-7c9a9bc38ee8" />

<br>

>> s<sub>i</sub> ≥ 8 log(T) / Δ<sub>i</sub><sup>2</sup> （因为 T ≥ t）。因此，如果我们取 ℓ = ⌈8 log(T) / Δ<sub>i</sub><sup>2</sup>⌉，那么我们就不能得到 μ* < μ<sub>i</sub> + 2c<sub>t,s<sub>i</sub></sub> 并且只有前两个不等式成立。这样，我们得到

<br>

<img width="609" height="245" alt="스크린샷 2025-11-09 오후 1 43 11" src="https://github.com/user-attachments/assets/5655ef5f-2beb-4eca-a235-418a85032e1d" />

<br>

>> 因此，根据 𝔼[n<sub>i,T</sub>] ≤ 𝔼[UCB<sub>i</sub>(T)] ≤ 8log(T) / Δ<sub>i</sub><sup>2</sup> + C，我们得到 Regret(T) ≤ 8 log(T) / Δ<sub>i</sub> + CΣΔ<sub>i</sub>。

> ③结论

>> ○ 每次后悔收敛到0：意味着每一个选择都成为最好的选择

>> ○ 仅使用经验平均值并不能保证最佳臂选择

>> ○ 可能导致永远无法尝试最佳手臂

⑷ UCB 与 Thompson 抽样比较

> ① **共性 1. **总遗憾为 O(log T)：每次遗憾收敛到 0，为 O(log T / T)

> ② **差异 1.** Thompson 采样通常优于 UCB：UCB 是一种稍微保守的算法，收敛速度较慢

> ③ **差异 2.** UCB 是确定性的，而 Thompson 采样是概率性的（随机的）

<br>

<br>

## **3\.汤普森采样**

⑴ 概述

> ①最古老的老虎机算法：Thompson于1933年提出

> ② 自然高效的启发式算法

⑵ 工艺流程

> ① 保持对每个手臂参数的信念（例如，平均奖励）

>> ○ 使用先验分布，例如 [Beta 分布](https://jb243.github.io/pages/1627) 或 [高斯分布](https://jb243.github.io/pages/1627)

> ② 从每个先验分布中提取估计奖励

>> ○ 简单抽样，不提取期望值

>> ○ 观测值越少，方差越大，提取越容易

> ③ 选择估计奖励最大的arm，观察其后验奖励

>> ○ 后验分布必须始终由研究人员维护

> ④ 使用贝叶斯方法用后验更新先验信念

> ⑤ 两种模式下也能正常工作

⑶ **示例：** 使用 Beta 分布进行 Thompson 抽样

> ① 以 Beta(1, 1) 作为所有臂的先验信念

> ② 对于每个时间 t，

>> ○ 先验：Beta(α, β)

>> ○ 每臂独立采样 θi,t

>> ○ 选择它 = argmaxi θi,t

>> ○ Beta(α+1, β)：观察到 1 时

>> ○ Beta(α, β+1): 观测到 0 时

⑷ **示例：** 使用高斯分布的 Thompson 采样

> ① 以 N(0, ν2) 作为所有臂的先验信念> ② 对于每个时间 t，

>> ○ 先验：N(0, ν2)

>> ○ 每臂独立采样 θi,t

>> ○ 选择它 = argmaxi θi,t

>> ○ 更新所选臂的经验平均值 µˆ（后验）：其中 n 是独立观测值的数量

<br>

<img width="114" alt="스크린샷 2024-11-21 오후 10 00 35" src="https://github.com/user-attachments/assets/6ae24436-a6c0-4a1e-8795-0d40078cf299">

<br>

⑸ 收敛性

> ① 经过一定数量的步骤后，两个手臂之间的良好分离最终会产生最佳手臂

<br>

<img width="114" alt="스크린샷 2024-11-21 오후 10 00 47" src="https://github.com/user-attachments/assets/f865cb8d-68ee-4b10-a20f-a61438b80f8b">

<br>

> ② 贝塔分布

<br>

<img width="312" alt="스크린샷 2024-11-21 오후 10 01 07" src="https://github.com/user-attachments/assets/cee6cd7c-4bf3-48e1-a461-be7fec84baa5">

<br>

> ③ 高斯分布：基于Azuma-Hoeffding不等式

<br>

<img width="177" alt="스크린샷 2024-11-21 오후 10 01 21" src="https://github.com/user-attachments/assets/f269b2a6-213d-4642-adec-e49b9d8bd14c">

<br>

⑹ UCB 与 Thompson 抽样比较

> ① **共性 1.** 总后悔是 O(log T)：每次后悔收敛到 0，为 O(log T / T)

> ② **差异 1.** Thompson 采样通常优于 UCB：UCB 是一种稍微保守的算法，收敛速度较慢

> ③ **差异 2.** UCB 是确定性的，而 Thompson 采样是概率性的（随机的）

<br>

<br>

## **4\.提前停止问题**

⑴ 秘书问题

> ① 问题

>> __受保护_1__

> ② 证明

<br>

<img width="1280" height="1928" alt="image" src="https://github.com/user-attachments/assets/100d43c7-e248-4fb3-8114-1c455c58926f" />

<br>

⑵ [未应用贝叶斯优化的提前停止（最优停止）问题](https://jb243.github.io/pages/384)

<br>

---

_输入：2021.12.10 00:52_

_编辑时间：2024.11.21 15:30_