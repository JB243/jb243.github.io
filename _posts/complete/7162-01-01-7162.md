## **第 11 章强化学习**（强化学习；RL）

推荐阅读：【算法】【算法索引】(https://jb243.github.io/pages/1278)

---

**1.** [概述](#1-概述)

**2.** [马尔可夫链](#2-马尔可夫链)

**3.** [马尔可夫决策过程](#3-马尔可夫决策过程)

**4.** [一般决策过程](#4-一般决策过程)

---

**a.** [随机控制理论](https://jb243.github.io/pages/895) 

---

<br>

## **1\.概述**

 ⑴ 定义

> ① 监督学习

>> ○ 数据：(x, y)（其中 x 是特征，y 是标签）

>> ○ 目标：计算映射函数 x → y

> ② 无监督学习

>> ○ 数据：x（其中x是一个特征，没有标签）

>> ○ 目标：了解x的底层结构

> ③ 强化学习

>> ○ 数据：(s, a, r, s')（其中 s 是状态，a 是动作，r 是奖励，s' 是下一个状态）

>> ○ 目标：在多个时间步长内最大化总奖励

⑵【随机控制理论】(https://jb243.github.io/pages/895)

> ① **要素 1.** 状态

> ② **要素2.** 奖励

>> ○ 定义：状态的变化。

>> ○ **价值函数：** 未来奖励的预期价值，表示为终生价值（VLT）。

>>> ○ 公式：对于状态 s，政策 π 和将未来值调整为现值的贴现因子 γ。

<br>

<img width="295" height="66" alt="스크린샷 2025-09-26 오후 12 53 43" src="https://github.com/user-attachments/assets/ddb55021-a199-4bcc-ac27-f5864c114304" />

<br>

>>> ○ 通过评估状态的好坏，为选择行动提供依据。

> ③ **要素 3.** 动作

> ④ **要素 4.** 政策

>> ○ 定义：代理人的行为；将状态作为输入并输出操作的映射。

>> ○ 决策过程：决策问题的一般框架，其中状态、行动和奖励在一个过程中展开。

>> ○ **类型 1：** 确定性策略

>> ○ **类型 2：** 随机策略

>>> ○ **原因一：** 在学习中，最优行为是未知的，因此需要探索。

>>> ○ **原因 2：** 最佳情况本身可能是随机的（例如，石头剪刀布，或者对手利用决定论）。

> ⑤ **要素 5.** 型号

>> ○ 定义：环境的行为/动态。

>> ○ 给定一个状态和一个动作，模型决定下一个状态和奖励。

>> ○ 请注意无模型方法和基于模型方法之间的区别。

⑶ **特点**：区别于监督学习和无监督学习

> ① 隐式获得正确答案：以奖励的形式提供

> ② 需要考虑与环境的交互：延迟反馈可能是一个问题

> ③ 之前的决定影响未来的互动

> ④主动收集信息：强化学习包括获取数据的过程

<br>

<br>

## **2\.马尔可夫链** 

⑴ 概述

> ①定义：未来状态仅取决于当前状态而不取决于过去状态的系统

<br>

<img width="577" alt="스크린샷 2025-02-25 오후 10 30 26" src="https://github.com/user-attachments/assets/08615c69-63f0-4680-806d-ddd8a159569c" />

<br>

> ② 马尔可夫链是指状态空间有限或可数无限的马尔可夫过程。

> ③ **引理1.** [Chapman-Kolmogorov 分解](https://jb243.github.io/pages/895) 

> ④ **引理2.** [线性状态空间模型](https://jb243.github.io/pages/895) 

⑵【图论】(https://jb243.github.io/pages/616) 

> ① 强连通（= 不可约）：图中任意节点 i 都可以到达任意其他节点 j 的状态

> ② 周期：返回节点 i 的所有路径的最大公约数

>> ○ 示例：如果两个节点 A 和 B 用两条边连接为 A=B，则每个节点的周期为 2

> ③ 非周期：当所有节点的周期为1时>> ○ 非周期 ⊂ 不可约

>> ○ 示例：如果每个节点都有返回自身的步行，则它是非周期的

> ④ 平稳状态：如果 Pr(xn | xn-1) 与 n 无关，则马尔可夫过程是平稳的（时不变）

> ⑤ 常规

>> ○ 正则 ⊂ 不可约

>> ○ 如果存在一个自然数 k，使得转移矩阵 M^k 的 k 次方的所有元素均为正（即非零）

> ⑥ **引理1.** [Perron-Frobenius 定理](https://jb243.github.io/pages/895) 

> ⑦ **引理2.** 【李雅普诺夫方程】(https://jb243.github.io/pages/895) 

> ⑧ **引理3.** [贝尔曼方程](https://jb243.github.io/pages/895) 

 ⑵ **类型1.** 二态马尔可夫链

<br>

![图片](https://github.com/user-attachments/assets/9674a0ab-b585-4627-ad22-b8973f1ca627)

**图 1.** 二尺度马尔可夫链

<br>

> ① M：一步引起状态转换的变换

> ② M<sup>n</sup>：n步引起状态转换的变换

> ③稳态向量：满足Mq = q的向量q，即特征值为1的特征向量

⑶ **类型2.** HMM（隐马尔可夫模型）

> ① χ = {X<sub>i</sub>} 是马尔可夫过程，Y<sub>i</sub> = phi(X<sub>i</sub>) （其中 phi 是确定性函数），则 y = {Y<sub>i</sub>} 是隐马尔可夫模型。

> ② **鲍姆-韦尔奇算法**

>> ○ 目的：学习HMM参数

>> ○ 输入：观测数据

>> ○ 输出：HMM的状态转移概率和发射概率

>> ○ 原理：EM（Expectation Maximization）算法的一种

>> ○ 公式

>>> ○ A<sub>kl</sub>: 从状态 k 到 l 的转换次数

<br>

<img width="376" alt="스크린샷 2025-03-14 8 40 21" src="https://github.com/user-attachments/assets/4881807e-d62c-4f7d-a97a-28e0ce8f8175" />

<br>

>>> ○ E<sub>k</sub>(b): 状态 k 的观测 b 的发射次数

<br>

<img width="338" alt="스크린샷 2025-03-14 8 40 38" src="https://github.com/user-attachments/assets/6fd17c4c-3fca-444d-b634-c72f04dbd0b9" />

<br>

>>> ○ B<sub>k</sub>：状态 k 的初始概率

<br>

<img width="256" alt="스크린샷 2025-03-14 8 40 55" src="https://github.com/user-attachments/assets/f4e86c0e-25c1-4f99-bc7a-6dd1e08e00c6" />

<br>

> ③ **【维特比算法】(http://www.mcb111.org/w06/durbin_book.pdf)** 

>> ○ 目的：给定 HMM 找到最可能的隐藏状态序列

>> ○ 输入：HMM 参数和观测数据

>>> ○ N：可能的隐藏状态数

>>> ○ T：观测数据的长度

>>> ○ A：状态转移概率，a<sub>kl</sub> = 从状态 k 转移到状态 l 的概率

>>> ○ E：发射概率，e<sub>k</sub>(x) = 在状态 k 下观察到 x 的概率

>>> ○ B: 初始状态概率

>> ○ 输出：最可能的状态序列

>> ○ 原理：利用动态规划计算最优路径

<br>

![图片](https://github.com/user-attachments/assets/e1fa8f67-d229-41db-bfa1-27320721e6b6)

<br>

>> ○ **步骤1.** 初始化

<br>

<img width="229" alt="스크린샷 2025-02-25 오후 10 32 04" src="https://github.com/user-attachments/assets/7052ecde-8df0-400a-a125-7f4b29469018" />

<br>

>>> ○ b<sub>k</sub>: 状态 k 的初始概率，P(s0 = k)

>>> ○ e<sub>k</sub>(σ)：在状态 k 下观测到第一个观测值 σ 的概率，P(x<sub>0</sub> | s<sub>0</sub> = k)

>> ○ **步骤 2.** 递归

>>> ○ 计算每个时间步 i = 1, ..., T 的先前状态的最大概率

<br>

<img width="291" alt="스크린샷 2025-02-25 오후 10 32 46" src="https://github.com/user-attachments/assets/9ef4d062-dddc-4496-ad1d-8dae4c2f05de" />

<br>

>>> ○ 计算反向指针 (ptr)，存储最可能的先前状态

<br>

<img width="291" alt="스크린샷 2025-02-25 오후 10 33 01" src="https://github.com/user-attachments/assets/351dfb0c-7f3d-4949-883f-d16a7f34af21" />

<br>>>> ○ ptr<sub>i</sub>(l) 用于存储最有可能转变到当前状态 l 的先前状态 k。

>> ○ **步骤 3.** 终止

>>> ○ 选择最后一个时间步的最高概率

<br>

<img width="243" alt="스크린샷 2025-02-25 오후 10 34 15" src="https://github.com/user-attachments/assets/d7211923-2c9d-499a-9917-7b9fee1b34b9" />

<br>

>>> ○ 确定最优序列的最后状态

<br>

<img width="223" alt="스크린샷 2025-02-25 오후 10 34 32" src="https://github.com/user-attachments/assets/82218707-8776-4d83-b228-b55b1419a3b6" />

<br>

>>> ○ v<sub>k</sub>(i - 1): 状态 k 中前一时间步 i - 1 的最优概率

>>> ○ a<sub>kl</sub>：从状态 k 转换到 l 的概率

>> ○ **步骤 4.** 回溯

>>> ○ 从 i = T, ..., 1 开始通过 ptr 数组回溯，恢复最优路径

<br>

<img width="154" alt="스크린샷 2025-02-25 오후 10 34 55" src="https://github.com/user-attachments/assets/516197ed-77f4-461e-98ce-f13fbbaf587f" />

<br>

>> ○ 示例

<br>

![图片](https://blog.kakaocdn.net/dn/nKnMO/btsMvUfkPcP/7YrFhrnIeBARENfxO3x1t1/img.gif)

 **图 2.** 维特比算法示例

<br>

>> ○ Python 代码

<br>

__受保护_0__

<br>

> ④ **类型1.** PSSM：更简单的HMM结构

> ⑤ **类型 2.** Profile HMM：在以下方面优于 PSSM： 

>> ○ 轮廓HMM图

<br>

![图片](https://github.com/user-attachments/assets/7068b7b0-a363-4a77-941b-6b6ed71322bf)

**图3.** 轮廓HMM图 

<br>

>>> ○ M、I、D 分别代表匹配、插入、删除。

>>> ○ M<sub>i</sub> 可以转换为 M<sub>i+1</sub>、I<sub>i</sub> 和 D<sub>i+1</sub>。

>>> ○ I<sub>i</sub> 可以转换为 M<sub>i+1</sub>、I<sub>i</sub> 和 D<sub>i+1</sub>。

>>> ○ D<sub>i</sub> 可以转换为 M<sub>i+1</sub>、I<sub>i</sub> 和 D<sub>i+1</sub>。

>> ○ **优点1.** 能够对插入和删除进行建模

>> ○ **优点 2.** 转换仅限于有效状态遍历之间。

>> ○ **优势 3.** 州之间的界限得到更好的界定。

⑸ **类型 3.** 马尔可夫链蒙特卡罗 (MCMC)

> ① 定义：一种从遵循复杂概率分布的马尔可夫链生成样本的方法

> ② **方法 1.** Metropolis-Hastings

>> ○ 从当前状态生成新的候选样本 → 接受或拒绝候选样本 → 如果接受，则转换到新状态

> ③ **方法2.** 吉布斯抽样

<br>

<img width="616" height="236" alt="스크린샷 2025-12-13 오전 1 36 59" src="https://github.com/user-attachments/assets/c61ca8fe-374f-4831-9de4-76547d9fa460" />

<br>

> ④ **方法3.** 重要性/拒绝抽样

> ⑤ **方法4.** 可逆跳转MCMC

>> ○ 一般 MCMC 方法，如 **方法 1** 和 **方法 2** 从固定维参数空间中的概率分布中采样

>> ○ 可逆跳转 MCMC 在变维参数空间中运行：采样过程中参数数量动态变化

<br>

<br>

## **3\.马尔可夫决策过程** 

 ⑴ 概述

> ① 定义：未来仅取决于当前状态的决策过程。

>> ○ 在实践中，绝大多数问题设置都可以视为马尔可夫决策过程（MDP）。

> ② 示意图：对于转换，t+1 处的状态仅由 t 处状态的函数确定。

<br>

![图片](https://github.com/user-attachments/assets/ba1eb84c-c867-4871-b402-37be8c6ba06b)

 **图 4.** MDP 中的代理与环境交互

<br>

>> ○ 状态：s<sub>t</sub> ∈ S

>> ○ 动作：a<sub>t</sub> ∈ A

>> ○ 奖励：r<sub>t</sub> ∈ R(s<sub>t</sub>, a<sub>t</sub>)

>> ○ 策略：a<sub>t</sub> ~ π(**·** | s<sub>t</sub>)>> ○ 转换：(s<sub>t+1</sub>, r<sub>t+1</sub>) ~ P(**·** | s<sub>t</sub>, a<sub>t</sub>)

> ③最优解V(s)的存在性

<br>

<img width="159" alt="스크린샷 2025-02-25 오후 10 39 24" src="https://github.com/user-attachments/assets/f48b935c-a53b-45a5-8d8b-7b5a71134b81" />

<br>

>> ○ **前提1.** 马尔可夫性质

>> ○ **前提2.** 平稳假设

>> ○ **前提 3.** 无分配变化

 ⑵ **类型1.** Q-learning

> ①定义：学习所有状态的价值函数V<sup>π</sup>(s)

>> ○ 转移概率 P(s' | s, a) 未知

>> ○ 奖励函数 R(s, a) 未知

> ② **方法1.**

>> ○ 初始化所有状态/动作对的估计：Q̂(s, a) = 0

>> ○ 初始迭代方案

>>> ○ 采取随机行动 a

>>> ○ 观察下一个状态 s' 并从环境中接收 R(s, a)

>>> ○ 更新 Q̂(s, a)

>> ○ 后期迭代方案

>>> ○ 取 a = arg max<sub>a</sub> Q̂(s, a)

>>> ○ 观察下一个状态 s' 并接收 R(s, a)

>>> ○ 更新 Q̂(s, a)

<br>

<img width="270" alt="스크린샷 2025-02-25 오후 10 40 02" src="https://github.com/user-attachments/assets/f1840829-0a69-4428-8b56-8d40425a76f5" />

<br>

>>> ○ 随机重启

> ③ **方法2.** ε-贪婪（epsilon-greedy）

>> ○ 初始化所有状态/动作对的估计：Q̂(s, a) = 0

>> ○ 迭代方案

>>> ○ 以 1 - ε 的概率，采取 a = arg maxa Q̂(s, a)，以 ε 的概率，采取随机行动

>>> ○ 观察下一个状态 s' 并接收 R(s, a)

>>> ○ 更新 Q̂(s, a)

<br>

<img width="270" alt="스크린샷 2025-02-25 오후 10 40 18" src="https://github.com/user-attachments/assets/7992b3f8-a6a1-4ccb-8bb4-3254d1347a9f" />

<br>

>>> ○ 在后续迭代中执行随机重启

> ④ **方法3.** DQN（深度Q网络）

>> ○ 定义：利用深度学习实现Q-learning

>> ○ 应用于多种游戏：Atari 2600 (2013) 上的 DQN、AlphaGo (2016) 等。

 ⑶ **类型2.** 政策梯度

> ①定义：直接学习策略π(s)

> ② 特点

>> ○ 在 Q-learning 中，状态可以是连续的，但动作必须是离散的

>> ○ 在策略梯度中，状态和动作都可以是连续的

> ③ 算法

>> ○ **步骤1.** 接收帧

>> ○ **步骤2.** 前向传播以获得P(action)

>> ○ **步骤 3.** 从 P(action) 中采样 a

>> ○ **步骤 4.** 玩游戏的其余部分

>> ○ **步骤5.** 如果比赛获胜，则在∇θ方向更新

>> ○ **步骤6.** 如果比赛输了，则在-∇θ方向更新

 ⑷ 其他类型

> ① 深度O-网络（DON）

> ② A3C（异步优势Actor-Critic）

> ③遗传算法

> ④ SARSA（国家-行动-奖励-国家-行动）

<br>

<br>

## **4\.一般决策过程** 

⑴ [MAB](https://jb243.github.io/pages/2160#1-overview)(多臂老虎机)

⑵ [UCB](https://jb243.github.io/pages/2160#2-ucb) 

⑶ [汤普森采样](https://jb243.github.io/pages/2160#3-thompson-sampling) 

⑷ 【提前停止问题】(https://jb243.github.io/pages/2160#4-early-stopping-problem) 
 
<br>

---

_输入：2021.12.13 15:20_

_更新时间：2024.10.08 22:43_