## Dynamic Programming Example Problems [01-10]

Higher category: ã€Algorithmã€‘ [Algorithm and Machine Learning Index](https://jb243.github.io/pages/1278)

---

<br>

## Overview

> â—‹ **Example 1.** Unix diff for comparing two files

> â—‹ **Example 2.** [Viterbi](https://jb243.github.io/pages/2050) for hidden Markov model

> â—‹ **Example 3.** [Smith-Waterman](https://jb243.github.io/pages/2050) for genetic sequence alignment

> â—‹ **Example 4.** -Ford for shortest path routing in networks

> â—‹ **Example 5.** Cocke-Kasami-Younger for parsing context free grammars

<br>

<br>

## Q1. 

Consider a rooted binary tree network. Each internal node x has two children, denoted by x(lhs) and x(rhs). The cost of moving from x to its child x(a) is l<sub>a</sub>, where a âˆˆ {lhs,rhs}. For each node x, let L<sub>x</sub> be the minimum cost of any path from x to a leaf node. For each leaf node y, define L<sub>y</sub> = 0. Show that for any internal node x the following holds: 

<br>

<center>L<sub>x</sub> â€‹= min<sub>aâˆˆ{lhs,rhs}</sub> â€‹ {l<sub>a</sub> + L<sub>x</sub>(a)}.</center>

## S1.

See [this link](https://www.youtube.com/watch?v=yzZVRXJ0d80&list=PLGboZ4litMr_TOwUANH-s-uFnczzy2uuW). Applying this, the shortest-path algorithms (e.g., Dijkstra algorithm, Floyd algorithm) were developed. ([ref](https://jb243.github.io/pages/61))

<br>

<br>

## Q2. 

An investor has a fund. It is worth x pounds at time zero, and the principal cannot be withdrawn. The fund pays interest at rate r Ã— 100% per year for T years. At time t, the investor consumes a proportion a<sub>t</sub> of the interest and reinvests the rest in the fund. What choice of {a<sub>t</sub>} should the investor make in order to maximize total consumption?

## S2. 

See [this link](https://www.youtube.com/watch?v=VMjrzLld9W0&list=PLGboZ4litMr_TOwUANH-s-uFnczzy2uuW&index=3). The optimal policy is to save all income at first and then, once a certain threshold is reached, switch to consuming all of it.

<br>

<br>

## Q3. 

You want to sell a car. At each time (t = 0, ..., T-1), you choose a price p<sub>t</sub>, and then a customer comes and looks at the car. The probability that the customer buys the car at price p is D(p). If the car has not been sold by time T, it is sold for a fixed price W<sub>T</sub>, where W<sub>T</sub> < 1. Maximize the expected reward from selling the car, and find the recursion (dynamic programming / Bellman equation) for the optimal reward when D(p) = max(1 - p, 0).

## S3. 

See [this link](https://www.youtube.com/watch?v=d1VkSkt0EbY&list=PLGboZ4litMr_TOwUANH-s-uFnczzy2uuW&index=7). The expected reward becomes larger as t gets closer to 0, and the price p<sub>t</sub> at time t should be set as: 

<br>

<center>p<sub>t</sub> = ((expected gain at t+1) + 1â€‹) / 2.</center>

<br>

<br>

## Q4. 

You own an expensive fish. Each day you are offered a price X for the fish, where X has probability density function f(x). You may either accept or reject the offer. With probability 1âˆ’p the fish dies on that day. Find the policy that maximizes the expected profit from selling the fish.

## S4. 

See this [link](https://www.youtube.com/watch?v=oGodjYGcy5A&list=PLGboZ4litMr_TOwUANH-s-uFnczzy2uuW&index=8&pp=iAQB). This system shows a time-invariant characteristics. Thus, the Bellman equation is as follows:

<br>

<center>V<sub>t</sub>(x) = max{x, pğ”¼[V<sub>t+1</sub>(X<sub>t+1</sub> ã…£ x)]} = max{x, pğ”¼[V<sub>t+1</sub>(X<sub>t+1</sub>)]} = max{x, a},</center>

<br>

where a is a constant. Therefore, the optimal policy is as follows: if x â‰¥ a, buy it; otherwise, reject it. Here, we can obtain the following fixed-point equation

<br>

<center>a = âˆ«<sub>-âˆ to a</sub> af(y)dy + âˆ«<sub>a to âˆ</sub> yf(y) dy â‡” 0 = âˆ«<sub>a to âˆ</sub> (y - a)f(y) dy.</center>

<br>

The LHS is fixed to zero, but the RHS is nonincreasing on a, so the fixed-point is unique.

<br>

<br>

## Q5. 

A gambler has i pounds and wants to reach N pounds. At each play, the gambler may stake any integer amount j with 1 â‰¤ j â‰¤ i. The gambler wins with probability p and loses with probability q = 1âˆ’p; if they win, they gain j pounds, and if they lose, they lose j pounds. The game ends when the gamblerâ€™s fortune reaches either 0 or N. Assuming that p > 1/2, argue that it is always optimal for the gambler to gamble 1 pound at a time. 

## S5. 

See this [link](https://www.youtube.com/watch?v=MHPRlItuTyA&list=PLGboZ4litMr_TOwUANH-s-uFnczzy2uuW&index=11). 

<br>

<center>V<sub>t</sub>(x) = max<sub>j</sub> {pğ”¼[V<sub>t+1</sub>(x+j)] + qğ”¼[V<sub>t+1</sub>(x-j)]} â†’ V(x) = max<sub>j</sub> {pğ”¼[V(x+j)] + qğ”¼[V(x-j)]} (âˆµ time-invariant)</center>

Note that R(i) = (1 - (q/p)<sup>i</sup>) / (1 - (q/p)<sup>N</sup>) is the solution of R(i) = pR(i-1) + qR(i+1), R(0) = 0, R(N) = 1. The given problem is more complex, but in the end, we obtain V(x) = R(x). Intuitively, if infinite time is allowed, the optimal policy is to invest the minimum unit to decrease the risk.

The function form can be obtained by assuming the linear sum of exponential functions. Let's suppose R<sub>1</sub>, R<sub>2</sub> be the solutions of R(i) = pR(i-1) + qR(i+1). Then, the difference of two functions, R<sub>1</sub> - R<sub>2</sub>, also satisfy the equation. This equation is similar to the internally dividing point in geometry, so we know R<sub>1</sub> âˆ’ R<sub>2</sub> is always zero in the integer grid points considering the number of variables and equations. If we increase the number of integer grid points, we know R<sub>1</sub> - R<sub>2</sub> is identical to 0 for all the points in the given interval.

<br>

<br>

## Q6. 

Let X<sub>k</sub> denote the price of a given stock on day k, and suppose X<sub>k+1</sub> = X<sub>k</sub> + W<sub>k</sub>, k = 0, 1, 2, ..., where W<sub>0</sub>, W<sub>1</sub>, W<sub>2</sub>, ... are independently identically distributed (i.i.d.) random variables with a given distribution P, having finite mean, and are also independent of X<sub>0</sub>, the initial price. Suppose you have an option to buy one share of stock at a fixed price K. You have N days to exercise this option. You do not have to exercise the option, but if you do on a given day k when the price is x, your profit will be x - K. You can exercise the option at most once. Then, determine a strategy that maximizes your expected profit.

## A6. 

See this [link](https://www.youtube.com/watch?v=oGodjYGcy5A&list=PLGboZ4litMr_TOwUANH-s-uFnczzy2uuW&index=8). At any point of time, if you exercise the option at price x, the return is x - K. Hence, the stochastic dynamic program is:

<br>

<center>V<sub>N</sub>(x) = max(x - K, 0)</center>

<center>V<sub>t</sub>(x) = max(x - K, ğ”¼<sub>W<sub>t</sub></sub>[V<sub>t+1</sub>(x + V<sub>t</sub>)]), t = 0 ~ N-1</center>

<br>

We prove the following properties of the value function: (i) V<sub>t</sub>(x) â‰¥ V<sub>t+1</sub>(x), âˆ€x, (ii) V<sub>t</sub>(x) - x is decreasing in x. Note that for t = N, V<sub>N</sub>(x) â‰¥ 0. Therefore, V<sub>N-1</sub>(x) = max(x - K, ğ”¼<sub>W<sub>N-1</sub></sub>[V<sub>N</sub>(x + W<sub>N-1</sub>)]) â‰¥ max(x-K, 0) = V<sub>N</sub>(x). Let's assume the assertion of part (i) is true for t+1, that is V<sub>t+1</sub>(x) â‰¥ V<sub>t+2</sub>(x), âˆ€x. To complete the proof of (i) we must show that V<sub>t</sub>(x) â‰¥ V<sub>t+1</sub>(x), âˆ€x. Because of the inductive assumption, we get 

<br>

<center>V<sub>t</sub>(x) = max(x - K, ğ”¼<sub>W<sub>t</sub></sub>[V<sub>t+1</sub>(x + W<sub>t</sub>)] â‰¥ max(x - K, ğ”¼<sub>W<sub>t</sub></sub>[V<sub>t+2</sub>(x + W<sub>t</sub>)]) = V<sub>t+1</sub>(x)</center>

<br>

Regarding the proof of (ii), the claim is true for t = N since V<sub>N</sub>(x) - x = max(x - K, 0) - x = max(-K, -x) which is decreasing in x. Suppose the assertion of (ii) is true for t+1, that is, V<sub>t+1</sub>(x) - x is decreasing in x. To complete the proof of (ii) we must show that V<sub>t</sub>(x) - x is decreasing in x. 

<br>

<center>V<sub>t</sub>(x) - x = max(-K, ğ”¼<sub>W<sub>t</sub></sub>[V<sub>t+1</sub>(x + W<sub>t</sub>)] - x)</center>

<center>ğ”¼<sub>W<sub>t</sub></sub>[V<sub>t+1</sub>(x + W<sub>t</sub>)] - x = âˆ«[V<sub>t+1</sub>(x+w) - (x+w)]P<sub>W</sub>(dw) + ğ”¼(W<sub>t</sub>)</center>

<br>

By the induction hypothesis, V<sub>t+1</sub>(x + w) - (x + w) is decreasing in x for each fixed w. Since the right-hand-side of the above equation is a nonnegative linear combination of such functions, it follows that âˆ«[V<sub>t+1</sub>(x + w) - (x + w)]P(dw) is decreasing in x. Consequently, the right-hand-side of the above equation is decreasing in x, thus V<sub>t</sub>(x) - x is decreasing in x. The proof of (ii) is complete.

Consider the equation for the indifferent point: -K = ğ”¼<sub>W</sub>[V<sub>t+1</sub>(x + W)] - x. Property (ii) of the value function implies that the indifference equation has at most one solution. Define p<sub>t</sub> := unique solution of the indifference equation if it exists; +âˆ otherwise. Property (i) of the value function implies V<sub>t</sub>(x) â‰¥ V<sub>t+1</sub>(x), and thus ğ”¼[V<sub>t</sub>] â‰¥ ğ”¼[V<sub>t+1</sub>], which implies p<sub>t</sub> â‰¤ p<sub>t-1</sub> (the sequence is decreasing). Finally, it is obvious that p<sub>N</sub> = K. Finally, from (1) and (2), it follows that the optimal policy is to exercise the option at time t if and only if the current price x â‰¥ p<sub>t</sub>.

<br>

<br>

## Q7. 

You own a call option with strike price p. This means you may buy one share at price p; if the share price at time t is X<sub>t</sub>, your profit from exercising then is X<sub>t</sub> âˆ’ p. The option must be exercised no later than time T. The stock price X<sub>t</sub> follows X<sub>t+1</sub> = X<sub>t</sub> + Îµ<sub>t</sub>, where the Îµt are independent random shocks with the same distribution and with finite mean. Show that there exists a decreasing sequence {a<sub>t</sub>}<sub>0â‰¤tâ‰¤T</sub> such that it is optimal to exercise the option exactly when X<sub>s</sub> â‰¥ a<sub>s</sub> occurs at time s.

## A7. 

See this [link](https://www.youtube.com/watch?v=oGodjYGcy5A&list=PLGboZ4litMr_TOwUANH-s-uFnczzy2uuW&index=8). W<sub>t</sub>(x) can be shown to be a decreasing function of t. It can also be shown that W<sub>t</sub>(x) - x is a decreasing function of x. Intuitively, the larger x is, the higher the expected profit, so one tends to want to exercise the option. Hence we can show that on day m, it is optimal to exercise the option when x â‰¥ a<sub>s</sub>. Furthermore, from the dynamic program and the monotone behavior of W<sub>t</sub>(x) in m, we can easily show that a<sub>0</sub> â‰¥ a<sub>1</sub> â‰¥ Â·Â·Â· â‰¥ a<sub>N</sub>. Intuitively, the smaller t is (i.e., the earlier it is, with more time remaining), the higher the basic expected profit, so instead of exercising the option hastily, one prefers to wait strategically.

<br>

<br>

## Q8. 

The detection period for a certain type of interstellar particle is N days (N fixed). A scientist may look for that type of particle on any given day or may stay home. When the particle count is x, the probability of detecting it is p(x), and the probability of detecting more than one is zero. The probability p(Â·) is a known non-decreasing function of x. A particle detected is worth one unit, and a day of searching costs c units (0 < c < 1). If the scientist stays home he/she does not incur any cost and receives no reward. It is assumed that:

(A1) The number x of particles on any given day is known to the scientist.

(A2) The number of particles decreases by one when the scientist is successful; otherwise it is unchanged from one day to the next. 

The objective is to determine a detection policy to maximize the scientistâ€™s expected reward during the detection period. 

## A8. 

See this [link](https://jb243.github.io/pages/1130). Let V<sub>t</sub>(x) be the value function when the time is t and the number of particles is x. Then we obtain

<br>

<center>Q<sub>t</sub>(x, search) = -c + p(x)(1 + V<sub>t+1</sub>(x-1)) + (1 - p(x)) V<sub>t+1</sub>(x)</center>

<center>Q<sub>t</sub>(x, stay) = V<sub>t+1</sub>(x)</center>

<center>V<sub>N+1</sub>(x) = 0</center>

<center>V<sub>t</sub>(x) = max(Q<sub>t</sub>(x, search), Q<sub>t</sub>(x, stay)) = max( -c + p(x)(1 + V<sub>t+1</sub>(x-1)) + (1 - p(x)) V<sub>t+1</sub>(x), V<sub>t+1</sub>(x) ), t = 1, Â·Â·Â·, N</center>

<br>

V<sub>t</sub>(x) is decreasing in t and increasing in x. Thus, at any given time t, the incentive to search becomes stronger as x grows, and one can show that on day t it is optimal to search only when x â‰¥ x<sub>t</sub>\*. If we define â€œstayâ€ iff Q<sub>k</sub>(x, stay) < Q<sub>k</sub>(x, search), then we obtain x<sub>1</sub>\* = Â·Â·Â· = x<sub>n</sub>\*). Since this can be shown at any given time point, and the same argument holds by symmetry. That is, one can show that x<sub>N-1</sub>\* = x<sub>N</sub>\* and V<sub>N-1</sub>(x) = V<sub>N</sub>(x) for all x â‰¤ x<sub>N</sub>\*, and, by symmetry, it is intuitively clear that this inductive hypothesis is preserved. If instead we define â€œstayâ€ iff Q<sub>k</sub>(x, stay) â‰¤ Q<sub>k</sub>(x, search), then we obtain x<sub>1</sub>\* â‰¥ Â·Â·Â· â‰¥ x<sub>n</sub>\* since â€œstayâ€ is then weakly preferred.

<br>

<br>

## Q9. 

An optimal stopping problem is a Markov decision process with two actions: a = 0 meaning â€œstop,â€ and a = 1 meaning â€œcontinue.â€ There are two types of costs: c(x, a = 0) = k(x) for the stopping cost and c(x, a = 1) = c(x) for the continuation cost. This defines a stopping problem. Assuming that the time horizon is finite, the Bellman equation is 

<br>

<center>C<sub>t</sub>(x) = min{k(x), c(x) + ğ”¼[C<sub>t-1</sub>(X)]}</center>

<br>

with boundary conditions C(0) = 0, C<sub>0</sub>(x) = k(x) .

## A9. 

See this [link](https://www.youtube.com/watch?v=5F37qbwvJps&list=PLGboZ4litMr_TOwUANH-s-uFnczzy2uuW&index=15). Assume that the cost functions are increasing in x. Then the value function C<sub>t</sub>(x) is increasing in x and decreasing in the remaining time t. Hence the optimal policy is of threshold type: with t periods remaining, there exists a number x<sub>t</sub>* â€‹ such that it is optimal to stop if and only if x â‰¥ x<sub>t</sub>*. Moreover, the thresholds are monotone in t, in the sense that x<sub>t</sub>* â‰¤ x<sub>t+1</sub>*, so that having more time left makes continuation more attractive and one stops only for larger values of x.

<br>

<br>

## Q10. 

(Secretary Problem) Suppose you are presented with n offers in sequential order. After looking at an offer you must decide either to accept it and terminate the process or to reject it. Once rejected, the offer is lost. Suppose at any time you know the relative rank of the present offer compared to the previous ones. Determine the strategy that maximizes the probability of selecting the best offer when all n! orderings of offers are assumed to be equally likely.

## A10.

Let V<sub>k</sub> be the optimal success probability before observing the k<sup>th</sup> offer, X<sub>k</sub> be a relative rank out of the given k offers (the higher the better), and Q(x, a), a âˆˆ ğ’œ = {accept, reject} be state-action value function. Then, we have 

<br>

<img width="510" height="135" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-22 á„‹á…©á„Œá…¥á†« 1 07 11" src="https://github.com/user-attachments/assets/3a927d20-9920-4b68-afa7-87efb3d89575" />

<br>

Note that at the n<sup>th</sup> offer, we already know the exact rank of the offer, so the terminal condition is as follows: V<sub>n</sub>(x) = ğŸ™{x = n}. From this Bellman equation, we can have the following deterministic optimal policy: 

<br>

<img width="617" height="199" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-22 á„‹á…©á„Œá…¥á†« 1 08 48" src="https://github.com/user-attachments/assets/c5092a97-2c41-4189-aba0-00372e2451d2" />

<br>

Note that the success probability converges to 1/e.

<br>

<img width="410" height="343" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-22 á„‹á…©á„Œá…¥á†« 1 09 08" src="https://github.com/user-attachments/assets/a813e79a-32a1-483e-9134-1027b9cee32e" />

<br>

## A10-2.

Let the probability that the m-th offer is the best be p(m) = m/n, and let the probability that, after rejecting the m-th offer, the best offer still lies ahead be L(m) = 1 - m/n. Define V(m) = max{p(m), L(m)}. As m increases, p(m) increases, while L(m) decreases. Therefore, the optimal strategy is to keep rejecting offers at first, and then, after a certain point, accept the first offer that is the best among all offers observed up to that time.

<br>

<br>

## Q11. 

You look for a parking space on a street; each space is free with probability p = 1âˆ’q. You can't tell whether a space is free until you reach it. Once at a space you must decide to stop or continue. From position s (that is, s spaces from your destination), the cost of stopping is s. The cost of passing your destination without parking is D.

## A11. 

See this [link](https://www.youtube.com/watch?v=EEuZgZ1f-V4&list=PLGboZ4litMr_TOwUANH-s-uFnczzy2uuW&index=17). Let V<sub>s</sub> a value function for the position from the destination, then we have the following Bellman equation:

<br>

<center>V<sub>s</sub> = (1-q) max{s, V<sub>s-1</sub>} + qV<sub>s-1</sub>, V<sub>0</sub> = qD</center>

<br>

Whether, as in the given problem, the cost decreases as you get closer to the destination (left) or, conversely, increases (right), the optimal policy is given as follows: stop if and only if s â‰¤ s*

<br>

<img width="593" height="166" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-22 á„‹á…©á„’á…® 8 21 35" src="https://github.com/user-attachments/assets/ff017bae-5e24-4ac5-9a14-c11e35018a53" />

<br>
 
Since we always stop when we are close to the destination, we end up solving V<sub>s</sub>\* = (1âˆ’q)s + qV<sub>sâˆ’1</sub>\* and in this case V<sub>s</sub>\*â€‹ can be written in a neat closed form (cf. homogeneous and particular solutions): stop if and only if s â‰¤ V<sub>s</sub>\* = s - (q/p) + (qD + q/p) q<sup>s</sup> 

<br>

<br>

## Q12. 

Consider a Markov chain (M.C.) that evolves in discrete time t = 0, 1, 2, â€¦ with state space X = S = {1, 2, â€¦}. The transition probabilities are given by the matrix {P(j ã…£ i)}, for i, j = 1, 2, â€¦. At each time t = 0, 1, 2,â€¦ , the decision-maker can perfectly observe the current state X<sub>t</sub> of the Markov chain. Each observation incurs a cost c. At some time t, the decision-maker may choose to stop observing the evolution of the Markov chain. If they stop at time t, they receive a reward r(X<sub>t</sub>). In particular, if X<sub>t</sub>â€‹ = i, then r(X<sub>t</sub>) = r(i). The reward r(X<sub>t</sub>) is terminal in the sense that, once the decision-maker stops at time t, the decision is final and no further costs or rewards are incurred. The decision-maker must make this final stopping decision no later than time T. The objective is to choose a strategy g := (g<sub>0</sub>, g<sub>1</sub>, â€¦, g<sub>Tâˆ’1</sub>) that maximizes J<sup>g</sup> = ğ”¼<sup>g</sup>[âˆ’c(Ï„(g)+1) + r(X<sub>Ï„(g)</sub>)], where Ï„(g) denotes the (random) time at which the decision-maker stops under strategy g. For every strategy g, we have 0 â‰¤ Ï„(g) â‰¤ T. Determine the optimal policy.

## A12.

The dynamic program can be formulated as follows:

<br>

> V<sub>T</sub>(i) = r(i)

> V<sub>t</sub>(i) = max{r(i), -c + âˆ‘<sub>j</sub> P(i, j)V<sub>t+1</sub>(j)}, t = 0 ~ T-1

<br>

From this we obtain V<sub>t</sub>(i) â‰¥ V<sub>t+1</sub>(i). Intuitively, if the reward at time t is large enough, one can stop and make the decision immediately; otherwise, one should wait and aim for the next transition. Since the value function increases the earlier the time t is, the incentive to wait for a transition becomes stronger in order to maximize the reward. Hence, the stopping set at time t, S<sub>t</sub> := {i âˆˆ X : V<sub>t</sub>(x) = r(i)} is a subset of the stopping set at time (t+1), S<sub>t+1</sub>; that is, S<sub>t</sub> âŠ‚ S<sub>t+1</sub>. Assume that the stopping set at time (T-1), S<sub>T-1</sub>, has the closure property, i.e., for all (i, j) such that i âˆˆ S<sub>T-1</sub> and j âˆ‰ S<sub>T-1</sub>, we have P(j ã…£ i) = 0. In this case the size of S<sub>T-1</sub> is drastically reduced, and inductively we obtain S<sub>t</sub> âŠƒ S<sub>t+1</sub> âŠƒ Â·Â·Â· âŠƒ S<sub>T-1</sub>. Therefore, the optimal policy at time t is â€œstop iff i âˆˆ S<sub>t</sub>â€, and under the closure property it becomes â€œstop iff i âˆˆ S<sub>t</sub> = S<sub>T-1</sub>â€.

<br>

<br>

## Q13. 

An individual wants to sell her house. An offer comes at the beginning of each day. It is assumed that successive offers are independent and an offer is x<sub>j</sub> with probability p<sub>j</sub> , j = 1, 2, . . . , n, where x<sub>j</sub> , j = 1, 2, . . . , n, are non-negative scalars. Any offer that is not immediately accepted is not lost but may be accepted at any later date. A maintenance cost c is incurred for each day that the house remains unsold. There is a deadline to sell the house within N days. The objective is to maximize the price at which the house is sold minus the maintenance cost.

## A13. 

We can find an appropriate information state z<sub>t</sub> = max{x<sub>1</sub>, Â·Â·Â·, x<sub>t</sub>}. Then, we have the following dynamic program:

<br>

<center>Q<sub>k</sub>(z, sell) = z</center>

<center>Q<sub>k</sub>(z, remain) = -c + ğ”¼<sup>g</sup>[V<sub>k+1</sub>(z<sub>k+1</sub>) ã…£ z<sub>k</sub> = z] = -c + âˆ‘<sub>j</sub> p<sub>j</sub> V<sub>k+1</sub>(max{z, x<sub>j</sub>})</center>

<center>V<sub>n</sub>(z) = z</center>

<center>V<sub>k</sub>(z) = max{Q<sub>k</sub>(z, sell), Q<sub>k</sub>(z, remain)}</center>

<br>

We can find V<sub>t</sub>(z) â‰¥ V<sub>t+1</sub>(z), V<sub>t</sub>(z) â‰¥ V<sub>t</sub>(z-1), the convexivity of V<sub>t</sub>(z) in the dynamic program. The optimal policy is sell iff z â‰¥ r<sub>t</sub>. If "remain iff Q<sub>k</sub>(z, remain) < Q<sub>k</sub>(z, sell)", we obtain r<sub>1</sub> = Â·Â·Â· = r<sub>n</sub>, because we can show it in a step and it holds for other time steps by symmetry. If "remain iff Q<sub>k</sub>(z, remain) â‰¤ Q<sub>k</sub>(z, sell)", we obtain r<sub>1</sub> â‰¥ Â·Â·Â· â‰¥ r<sub>n</sub>, because "remain" is preferred.

<br>

<br>

## Q14. 

An object is located in one of n possible locations. The probability that the object is in location i is p<sub>i</sub> (p<sub>i</sub> is the prior probability, p<sub>i</sub> > 0, âˆ‘p<sub>i</sub> = 1). A search in location i costs c<sub>i</sub>, c<sub>i</sub> > 0, and if the object is present in that location the probability that it will be discovered is Î±<sub>i</sub>, i = 1, 2, . . . , n. Determine a policy that discovers the object at minimal cost. 

**Hint:** Show that if (Ï€<sub>1</sub>(t), Ï€<sub>2</sub>(t), ..., Ï€<sub>n</sub>(t)) is the information state at time t, t = 1,2,..., then an optimal policy is to search the location that has the maximal value of Î±<sub>i</sub>Ï€<sub>i</sub>(t) / c<sub>i</sub>, i = 1, 2, . . . , n.

## A14. 

Information state can be defined as the posterior probability â„™(outcome ã…£ prior).

<br>

<img width="551" height="373" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-23 á„‹á…©á„’á…® 5 35 07" src="https://github.com/user-attachments/assets/abd0ebcc-502c-415d-9442-b57f5ec5c081" />

<br>

<br>

## Q15. 

An individual is offered 3 to 1 odds in a coin tossing game where she wins whenever a tail occurs. However, she suspects that the coin is biased and has an a priori probability distribution with CDF F(p) and pdf f(p), for the probability p that a head occurs at each toss. A maximum of T coin tosses is allowed. The individualâ€™s objective is to determine a policy of deciding whether to continue or stop participating in the game, given the outcomes of the game so far, so as to maximize her earnings.

## A15. 

Information state can be defined as the posterior probability â„™(outcome ã…£ prior).

For general information state,

<br>

<img width="654" height="256" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-23 á„‹á…©á„’á…® 4 21 17" src="https://github.com/user-attachments/assets/7a328c97-6d20-4fdd-81a6-80de40173cb9" />

<br>

For specific information state,

<br>

<img width="700" height="353" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-23 á„‹á…©á„’á…® 4 47 29" src="https://github.com/user-attachments/assets/d53784ad-1853-4e64-bf76-67ecdfd5e089" />

<br>

<br>

## Q16.

(Sequential binary hypothesis testing) Consider a binary state H âˆˆ {0, 1} with prior probability â„™(H = 0) = p. At each time t = 0, 1, Â·Â·Â·, T, the decision maker chooses one of the following actions: (10 Stop and declare a decision u âˆˆ {0, 1}, or (2) Continue (incurring a cost (C)) and obtain an observation Y<sub>t</sub>. If the declaration is incorrect, a penalty K is incurred.

> **a.** Write down the terminal value function V<sub>T</sub>(Ï€) at time (t = T).

> **b.** Write the Bellman equation for V<sub>t</sub>(Ï€) for 0 â‰¤ t â‰¤ T-1.

> **c.** Let f<sub>0</sub>(y) and f<sub>1</sub>(y) denote the probability mass functions of Y under H = 0 and H = 1, respectively. Derive the belief update rule Ï€<sub>t+1</sub> = ğ’¯(Ï€<sub>t</sub>, y) after observing y.

> **d.** Express the conditional expected cost of choosing â€œcontinue,â€ ğ”¼[V<sub>t+1</sub>(Ï€<sub>t+1</sub>) ã…£ Ï€<sub>t</sub> = Ï€], in integral form.

> **e.** Show by mathematical induction that for all t, the function V<sub>t</sub>(Ï€) is concave in Ï€.

> **f.** Assuming V<sub>t</sub>(Ï€) is concave, show that the optimal policy is characterized by two threshold values Ï<sub>t</sub><sup>1,\*</sup> â‰¤ Ï<sub>t</sub><sup>0,\*</sup>. Write down the equations that determine these threshold values.

> **g.** (Infinite-horizon fixed point) Let T â†’ âˆ. Write the fixed-point equation satisfied by the time-invariant value function V(Ï€).

> **h.** Suppose T = 1 (i.e., the current time is t = 0 and there is only one more opportunity to observe), K = 1, C = 0.1, and the observation takes discrete values y âˆˆ {a, b} with likelihoods f<sub>0</sub>(a) = 0.7, f<sub>1</sub>(a) = 0.3, f<sub>0</sub>(b) = 0.3, and f<sub>1</sub>(b) = 0.7. Given the initial belief Ï€ = â„™(H = 0) = 0.45, determine whether it is optimal to continue or to stop.

## A16.

**a.** V<sub>T</sub>(Ï€) = min((1 - Ï€)K, Ï€K)

**b.** V<sub>t</sub>(Ï€) = min((1 - Ï€)K, Ï€K, C + ğ”¼[V<sub>t+1</sub>(Ï€<sub>t+1</sub>) ã…£ Ï€<sub>t</sub> = Ï€])

**c.** Ï€<sub>t+1</sub> = f<sub>0</sub>(y)Ï€<sub>t</sub> / (f<sub>0</sub>(y)Ï€<sub>t</sub> + f<sub>1</sub>(y)(1 - Ï€<sub>t</sub>)) = ğ’¯(Ï€<sub>t</sub>, y)

**d.** ğ”¼[V<sub>t+1</sub>(Ï€<sub>t+1</sub>) ã…£ Ï€] = âˆ« V<sub>t+1</sub>(ğ’¯(Ï€, y)) p(y ã…£ Ï€) dy = âˆ« V<sub>t+1</sub>(ğ’¯(Ï€, y)) (Ï€f<sub>0</sub>(y) + (1-Ï€)f<sub>1</sub>(y)) dy

**e.** If V<sub>t+1</sub>(Ï€) is concave by induction, we can put it as inf<sub>iâˆˆâ„</sub> {Î±<sub>i</sub>Ï€ + Î²<sub>i</sub>}, resulting in ğ”¼[V<sub>t+1</sub>(Ï€<sub>t+1</sub>) ã…£ Ï€] = âˆ« inf<sub>i</sub> {Î±<sub>i</sub> ğ’¯(Ï€,y) + Î²<sub>i</sub>} p(y ã…£ Ï€) dy = âˆ« inf<sub>i</sub> { Î±<sub>i</sub>f<sub>0</sub>(y)Ï€ + Î²<sub>i</sub> (f<sub>0</sub>(y)Ï€ + f<sub>1</sub>(y)(1 - Ï€)) } dy â†’ concave

**f.** 

<br>

<img width="247" height="294" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-22 á„‹á…©á„Œá…¥á†« 2 05 38" src="https://github.com/user-attachments/assets/1c414190-9c3f-4f2a-b33e-9d85da914481" />

<img width="228" height="82" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-22 á„‹á…©á„Œá…¥á†« 2 06 03" src="https://github.com/user-attachments/assets/e8b456d5-ab65-4c63-a1b4-49272b825bc1" />

<br>
 
**g.** Then, threshold values also converge to time-invariant constants.

**h.** If calculated, it is optimal to continue (â‰ˆ 0.371) < stop (0.45).

<br>

---

_Input: 2025.11.21 00:36_
