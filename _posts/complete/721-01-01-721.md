## Dynamic Programming Example Problems [01-10]

Higher category: ã€Algorithmã€‘ [Algorithm and Machine Learning Index](https://jb243.github.io/pages/1278)

---

<br>

## Overview

> â—‹ **Example 1.** Unix diff for comparing two files

> â—‹ **Example 2.** [Viterbi](https://jb243.github.io/pages/2050) for hidden Markov model

> â—‹ **Example 3.** [Smith-Waterman](https://jb243.github.io/pages/2050) for genetic sequence alignment

> â—‹ **Example 4.** Bellman-Ford for shortest path routing in networks

> â—‹ **Example 5.** Cocke-Kasami-Younger for parsing context free grammars

<br>

<br>

## Q1. 

Consider a rooted binary tree network. Each internal node x has two children, denoted by x(lhs) and x(rhs). The cost of moving from x to its child x(a) is l<sub>a</sub>, where a âˆˆ {lhs,rhs}. For each node x, let L<sub>x</sub> be the minimum cost of any path from x to a leaf node. For each leaf node y, define L<sub>y</sub> = 0. Show that for any internal node x the following holds: 

<br>

<center>L<sub>x</sub> â€‹= min<sub>aâˆˆ{lhs,rhs}</sub> â€‹ {l<sub>a</sub> + L<sub>x</sub>(a)}.</center>

## S1.

See [this link](https://www.youtube.com/watch?v=yzZVRXJ0d80&list=PLGboZ4litMr_TOwUANH-s-uFnczzy2uuW). Applying this, the shortest-path algorithms (e.g., Dijkstra algorithm, Floyd algorithm) were developed. ([ref](https://jb243.github.io/pages/61))

<br>

<br>

## Q2. 

An investor has a fund. It is worth x pounds at time zero, and the principal cannot be withdrawn. The fund pays interest at rate r Ã— 100% per year for T years. At time t, the investor consumes a proportion a<sub>t</sub> of the interest and reinvests the rest in the fund. What choice of {a<sub>t</sub>} should the investor make in order to maximize total consumption?

## S2. 

See [this link](https://www.youtube.com/watch?v=VMjrzLld9W0&list=PLGboZ4litMr_TOwUANH-s-uFnczzy2uuW&index=3). The optimal policy is to save all income at first and then, once a certain threshold is reached, switch to consuming all of it.

<br>

<br>

## Q3. 

You want to sell a car. At each time (t = 0, ..., T-1), you choose a price p<sub>t</sub>, and then a customer comes and looks at the car. The probability that the customer buys the car at price p is D(p). If the car has not been sold by time T, it is sold for a fixed price W<sub>T</sub>, where W<sub>T</sub> < 1. Maximize the expected reward from selling the car, and find the recursion (dynamic programming / Bellman equation) for the optimal reward when D(p) = max(1 - p, 0).

## S3. 

See [this link](https://www.youtube.com/watch?v=d1VkSkt0EbY&list=PLGboZ4litMr_TOwUANH-s-uFnczzy2uuW&index=7). The expected reward becomes larger as t gets closer to 0, and the price p<sub>t</sub> at time t should be set as: 

<br>

<center>p<sub>t</sub> = ((expected gain at t+1) + 1â€‹) / 2.</center>

<br>

<br>

## Q4. 

You own an expensive fish. Each day you are offered a price X for the fish, where X has probability density function f(x). You may either accept or reject the offer. With probability 1âˆ’p the fish dies on that day. Find the policy that maximizes the expected profit from selling the fish.

## S4. 

See this [link](https://www.youtube.com/watch?v=oGodjYGcy5A&list=PLGboZ4litMr_TOwUANH-s-uFnczzy2uuW&index=8&pp=iAQB). This system shows a time-invariant characteristics. Thus, the Bellman equation is as follows:

<br>

<center>V<sub>t</sub>(x) = max{x, pğ”¼[V<sub>t+1</sub>(X<sub>t+1</sub> ã…£ x)]} = max{x, pğ”¼[V<sub>t+1</sub>(X<sub>t+1</sub>)]} = max{x, a},</center>

<br>

where a is a constant. Therefore, the optimal policy is as follows: if x â‰¥ a, buy it; otherwise, reject it. Here, we can obtain the following fixed-point equation

<br>

<center>a = âˆ«<sub>-âˆ to a</sub> af(y)dy + âˆ«<sub>a to âˆ</sub> yf(y) dy â‡” 0 = âˆ«<sub>a to âˆ</sub> (y - a)f(y) dy.</center>

<br>

The LHS is fixed to zero, but the RHS is nonincreasing on a, so the fixed-point is unique.

<br>

<br>

## Q5. 

A gambler has ğ‘– i pounds and wants to reach N pounds. At each play, the gambler may stake any integer amount j with 1 â‰¤ j â‰¤ i. The gambler wins with probability p and loses with probability q = 1âˆ’p; if they win, they gain j pounds, and if they lose, they lose j pounds. The game ends when the gamblerâ€™s fortune reaches either 0 or N. Assuming that p > 1/2, argue that it is always optimal for the gambler to gamble 1 pound at a time. 

## S5. 

See this [link](https://www.youtube.com/watch?v=MHPRlItuTyA&list=PLGboZ4litMr_TOwUANH-s-uFnczzy2uuW&index=11). 

<br>

<center>V<sub>t</sub>(x) = max<sub>j</sub> {pğ”¼[V<sub>t+1</sub>(x+j)] + qğ”¼[V<sub>t+1</sub>(x-j)]} â†’ V(x) = max<sub>j</sub> {pğ”¼[V(x+j)] + qğ”¼[V(x-j)]} (âˆµ time-invariant)</center>

Note that R(i) = (1 - (q/p)<sup>i</sup>) / (1 - (q/p)<sup>N</sup>) is the solution of R(i) = pR(i-1) + qR(i+1), R(0) = 0, R(N) = 1. The given problem is more complex, but in the end, we obtain V(x) = R(x). Intuitively, if infinite time is allowed, the optimal policy is to invest the minimum unit to decrease the risk.

The function form can be obtained by assuming the linear sum of exponential functions. Let's suppose R<sub>1</sub>, R<sub>2</sub> be the solutions of R(i) = pR(i-1) + qR(i+1). Then, the difference of two functions, R<sub>1</sub> - R<sub>2</sub>, also satisfy the equation. This equation is similar to the internally dividing point in geometry, so we know R<sub>1</sub> âˆ’ R<sub>2</sub> is always zero in the integer grid points considering the number of variables and equations. If we increase the number of integer grid points, we know R<sub>1</sub> - R<sub>2</sub> is identical to 0 for all the points in the given interval.

<br>

<br>

## Q6. 

Let X<sub>k</sub> denote the price of a given stock on day k, and suppose X<sub>k+1</sub> = X<sub>k</sub> + W<sub>k</sub>, k = 0, 1, 2, ..., where W<sub>0</sub>, W<sub>1</sub>, W<sub>2</sub>, ... are independently identically distributed (i.i.d.) random variables with a given distribution P, having finite mean, and are also independent of X<sub>0</sub>, the initial price. Suppose you have an option to buy one share of stock at a fixed price K. You have N days to exercise this option. You do not have to exercise the option, but if you do on a given day k when the price is x, your profit will be x - K. You can exercise the option at most once. Then, determine a strategy that maximizes your expected profit.

## A6. 

See this [link](https://www.youtube.com/watch?v=oGodjYGcy5A&list=PLGboZ4litMr_TOwUANH-s-uFnczzy2uuW&index=8). Let V<sub>m</sub>(x) denote the maximal expected profit when the current stock price is x and today is day m (that is, the option expires in N-m days). 

<br>

<center>V<sub>N</sub>(x) = max(x-K, 0)</center>

<center>V<sub>t</sub>(x) = max(x-K, ğ”¼<sub>W<sub>t</sub></sub>[V<sub>t+1</sub>(x + W<sub>t</sub>)]</center>

<br>

We can show that V<sub>m</sub>(x) - x is decreasing in x, and for every x, V<sub>m</sub>(x) is decreasing in m. We can also show that on day m an optimal policy is to exercise the option if and only if x â‰¥ p<sub>m</sub>. Intuitively, the larger x is, the higher the expected profit, so you will want to exercise the option. Moreover, from the Bellman equation and the tendency of V<sub>m</sub>(x) on m, one can easily show that p<sub>0</sub> â‰¥ p<sub>1</sub> â‰¥ Â·Â·Â· â‰¥ p<sub>N</sub>. Intuitively, the smaller t is (i.e., the more time remains), the higher the basic expected profit, so instead of exercising the option prematurely it may be better to wait strategically.

<br>

<br>

## Q7. 

You own a call option with strike price p. This means you may buy one share at price p; if the share price at time t is Xt, your profit from exercising then is Xt âˆ’ p. The option must be exercised no later than time T. The stock price Xt follows Xt+1 = Xt + Îµt, where the Îµt are independent random shocks with the same distribution and with finite mean. Show that there exists a decreasing sequence {at}0â‰¤tâ‰¤T such that it is optimal to exercise the option exactly when Xs â‰¥ as occurs at time ğ‘  s.

## A7. 

See this [link](https://www.youtube.com/watch?v=oGodjYGcy5A&list=PLGboZ4litMr_TOwUANH-s-uFnczzy2uuW&index=8). W<sub>t</sub>(x) can be shown to be a decreasing function of t. It can also be shown that W<sub>t</sub>(x) - x is a decreasing function of x. Intuitively, the larger x is, the higher the expected profit, so one tends to want to exercise the option. Hence we can show that on day m, it is optimal to exercise the option when x â‰¥ a<sub>s</sub>. Furthermore, from the Bellman equation and the monotone behavior of W<sub>t</sub>(x) in m, we can easily show that a<sub>0</sub> â‰¥ a<sub>1</sub> â‰¥ Â·Â·Â· â‰¥ a<sub>N</sub>. Intuitively, the smaller t is (i.e., the earlier it is, with more time remaining), the higher the basic expected profit, so instead of exercising the option hastily, one prefers to wait strategically.

<br>

<br>

## Q8. 

The detection period for a certain type of interstellar particle is N days (N fixed). A scientist may look for that type of particle on any given day or may stay home. When the particle count is x, the probability of detecting it is p(x), and the probability of detecting more than one is zero. The probability p(Â·) is a known non-decreasing function of x. A particle detected is worth one unit, and a day of searching costs c units (0 < c < 1). If the scientist stays home he/she does not incur any cost and receives no reward. It is assumed that:

(A1) The number x of particles on any given day is known to the scientist.
(A2) The number of particles decreases by one when the scientist is successful; otherwise it is unchanged from one day to the next. 

The objective is to determine a detection policy to maximize the scientistâ€™s expected reward during the detection period. 

## A8. 

See this [link](https://jb243.github.io/pages/1130). Let V<sub>t</sub>(x) be the value function at time t when there are x particles. Then we have

<br>

Q<sub>t</sub>(x, search) = -c + p(x)(1 + V<sub>t+1</sub>(x-1)) + (1 - p(x)) V<sub>t+1</sub>(x)

Q<sub>t</sub>(x, stay) = V<sub>t+1</sub>(x)

V<sub>N+1</sub>(x) = 0

V<sub>t</sub>(x) = max(Q<sub>t</sub>(x, search), Q<sub>t</sub>(x, stay)) = max( -c + p(x)(1 + V<sub>t+1</sub>(x-1)) + (1 - p(x)) V<sub>t+1</sub>(x), V<sub>t+1</sub>(x) ), t = 1, Â·Â·Â·, N

<br>

Intuitively, V<sub>t</sub>(x) decreases as t increases and increases as x increases. Therefore, for a given time t, the larger x is, the stronger the incentive to search, and we can show that on day k, it is optimal to search only when x â‰¥ x<sub>k</sub>*. When little time remains, the expected payoff is not very high, so one prefers to secure short-term gains, which increases the incentive to search.; that is, x<sub>t</sub>* â‰¥ x<sub>t+1</sub>*.

<br>

<br>

## Q9. 

An optimal stopping problem is a Markov decision process with two actions: a = 0 meaning â€œstop,â€ and a = 1 meaning â€œcontinue.â€ There are two types of costs: c(x, a = 0) = k(x) for the stopping cost and c(x, a = 1) = c(x) for the continuation cost. This defines a stopping problem. Assuming that the time horizon is finite, the Bellman equation is 

<br>

<center>C<sub>t</sub>(x) = min{k(x), c(x) + ğ”¼[C<sub>t-1</sub>(X)]}</center>

<br>

with boundary conditions C(0) = 0, C<sub>0</sub>(x) = k(x) .

## A9. 

See this [link](https://www.youtube.com/watch?v=5F37qbwvJps&list=PLGboZ4litMr_TOwUANH-s-uFnczzy2uuW&index=15). Assume that the cost functions are increasing in x. Then the value function C<sub>t</sub>(x) is increasing in x and decreasing in the remaining time t. Hence the optimal policy is of threshold type: with t periods remaining, there exists a number x<sub>t</sub>* â€‹ such that it is optimal to stop if and only if x â‰¥ x<sub>t</sub>*. Moreover, the thresholds are monotone in t, in the sense that x<sub>t</sub>* â‰¤ x<sub>t+1</sub>*, so that having more time left makes continuation more attractive and one stops only for larger values of x.

<br>

<br>

## Q10. 

(Secretary Problem) Suppose you are presented with n offers in sequential order. After looking at an offer you must decide either to accept it and terminate the process or to reject it. Once rejected, the offer is lost. Suppose at any time you know the relative rank of the present offer compared to the previous ones. Determine the strategy that maximizes the probability of selecting the best offer when all n! orderings of offers are assumed to be equally likely.

## A10.

Let V<sub>k</sub> be the optimal success probability before observing the k<sup>th</sup> offer, X<sub>k</sub> be a relative rank out of the given k offers (the higher the better), and Q(x, a), a âˆˆ ğ’œ = {accept, reject} be state-action value function. Then, we have 

<br>

<img width="510" height="135" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-22 á„‹á…©á„Œá…¥á†« 1 07 11" src="https://github.com/user-attachments/assets/3a927d20-9920-4b68-afa7-87efb3d89575" />

<br>

Note that at the n<sup>th</sup> offer, we already know the exact rank of the offer, so the terminal condition is as follows: V<sub>n</sub>(x) = ğŸ™{x = n}. From this Bellman equation, we can have the following deterministic optimal policy: 

<br>

<img width="617" height="199" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-22 á„‹á…©á„Œá…¥á†« 1 08 48" src="https://github.com/user-attachments/assets/c5092a97-2c41-4189-aba0-00372e2451d2" />

<br>

Note that the success probability converges to 1/e.

<br>

<img width="410" height="343" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-22 á„‹á…©á„Œá…¥á†« 1 09 08" src="https://github.com/user-attachments/assets/a813e79a-32a1-483e-9134-1027b9cee32e" />

<br>

<br>

## Q11. 

An individual wants to sell her house. An offer comes at the beginning of each day. It is assumed that successive offers are independent and an offer is x<sub>j</sub> with probability p<sub>j</sub> , j = 1, 2, . . . , n, where x<sub>j</sub> , j = 1, 2, . . . , n, are non-negative scalars. Any offer that is not immediately accepted is not lost but may be accepted at any later date. A maintenance cost c is incurred for each day that the house remains unsold. There is a deadline to sell the house within N days. The objective is to maximize the price at which the house is sold minus the maintenance cost.

## A11. 

To be updated.

<br>

<br>

## Q12. 

An object is located in one of n possible locations. The probability that the object is in location i is p<sub>i</sub> (p<sub>i</sub> is the prior probability, p<sub>i</sub> > 0, âˆ‘p<sub>i</sub> = 1). A search in location i costs c<sub>i</sub>, c<sub>i</sub> > 0, and if the object is present in that location the probability that it will be discovered is Î±<sub>i</sub>, i = 1, 2, . . . , n. Determine a policy that discovers the object at minimal cost. Show that if (Ï€<sub>1</sub>(t), Ï€<sub>2</sub>(t), ..., Ï€<sub>n</sub>(t)) is the information state at time t, t = 1,2,..., then an optimal policy is to search the location that has the maximal value of Î±<sub>i</sub>Ï€<sub>i</sub>(t) / c<sub>i</sub>, i = 1, 2, . . . , n.

## A12. 

To be updated.

<br>

<br>

## Q13. 

An individual is offered 3 to 1 odds in a coin tossing game where she wins whenever a tail occurs. However, she suspects that the coin is biased and has an a priori probability distribution with CDF F(p) and pdf f(p), for the probability p that a head occurs at each toss. A maximum of T coin tosses is allowed. The individualâ€™s objective is to determine a policy of deciding whether to continue or stop participating in the game, given the outcomes of the game so far, so as to maximize her earnings.

## A13. 

To be updated.

<br>

<br>

## Q14.

(Sequential binary hypothesis testing) Consider a binary state H âˆˆ {0, 1} with prior probability â„™(H = 0) = p. At each time t = 0, 1, Â·Â·Â·, T, the decision maker chooses one of the following actions: (10 Stop and declare a decision u âˆˆ {0, 1}, or (2) Continue (incurring a cost (C)) and obtain an observation Y<sub>t</sub>. If the declaration is incorrect, a penalty K is incurred.

> **a.** Write down the terminal value function V<sub>T</sub>(Ï€) at time (t = T).

> **b.** Write the Bellman equation for V<sub>t</sub>(Ï€) for 0 â‰¤ t â‰¤ T-1.

> **c.** Let f<sub>0</sub>(y) and f<sub>1</sub>(y) denote the probability mass functions of Y under H = 0 and H = 1, respectively. Derive the belief update rule Ï€<sub>t+1</sub> = ğ’¯(Ï€<sub>t</sub>, y) after observing y.

> **d.** Express the conditional expected cost of choosing â€œcontinue,â€ ğ”¼[V<sub>t+1</sub>(Ï€<sub>t+1</sub>) ã…£ Ï€<sub>t</sub> = Ï€], in integral form.

> **e.** Show by mathematical induction that for all t, the function V<sub>t</sub>(Ï€) is concave in Ï€.

> **f.** Assuming V<sub>t</sub>(Ï€) is concave, show that the optimal policy is characterized by two threshold values Ï<sub>t</sub><sup>1,\*</sup> â‰¤ Ï<sub>t</sub><sup>0,\*</sup>. Write down the equations that determine these threshold values.

> **g.** (Infinite-horizon fixed point) Let T â†’ âˆ. Write the fixed-point equation satisfied by the time-invariant value function V(Ï€).

> **h.** Suppose T = 1 (i.e., the current time is t = 0 and there is only one more opportunity to observe), K = 1, C = 0.1, and the observation takes discrete values y âˆˆ {a, b} with likelihoods f<sub>0</sub>(a) = 0.7, f<sub>1</sub>(a) = 0.3, f<sub>0</sub>(b) = 0.3, and f<sub>1</sub>(b) = 0.7. Given the initial belief Ï€ = â„™(H = 0) = 0.45, determine whether it is optimal to continue or to stop.

## A14.

**a.** V<sub>T</sub>(Ï€) = min((1 - Ï€)K, Ï€K)

**b.** V<sub>t</sub>(Ï€) = min((1 - Ï€)K, Ï€K, C + ğ”¼[V<sub>t+1</sub>(Ï€<sub>t+1</sub>) ã…£ Ï€<sub>t</sub> = Ï€])

**c.** Ï€<sub>t+1</sub> = f<sub>0</sub>(y)Ï€<sub>t</sub> / (f<sub>0</sub>(y)Ï€<sub>t</sub> + f<sub>1</sub>(y)(1 - Ï€<sub>t</sub>)) = ğ’¯(Ï€<sub>t</sub>, y)

**d.** ğ”¼[V<sub>t+1</sub>(Ï€<sub>t+1</sub>) ã…£ Ï€] = âˆ« V<sub>t+1</sub>(ğ’¯(Ï€, y)) p(y ã…£ Ï€) dy = âˆ« V<sub>t+1</sub>(ğ’¯(Ï€, y)) (Ï€f<sub>0</sub>(y) + (1-Ï€)f<sub>1</sub>(y)) dy

**e.** If V<sub>t+1</sub>(Ï€) is concave by induction, we can put it as inf<sub>iâˆˆâ„</sub> {Î±<sub>i</sub>Ï€ + Î²<sub>i</sub>}, resulting in ğ”¼[V<sub>t+1</sub>(Ï€<sub>t+1</sub>) ã…£ Ï€] = âˆ« inf<sub>i</sub> {Î±<sub>i</sub> ğ’¯(Ï€,y) + Î²<sub>i</sub>} p(y ã…£ Ï€) dy = âˆ« inf<sub>i</sub> { Î±<sub>i</sub>f<sub>0</sub>(y)Ï€ + Î²<sub>i</sub> (f<sub>0</sub>(y)Ï€ + f<sub>1</sub>(y)(1 - Ï€)) } dy â†’ concave

**f.** 

<br>

<img width="247" height="294" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-22 á„‹á…©á„Œá…¥á†« 2 05 38" src="https://github.com/user-attachments/assets/1c414190-9c3f-4f2a-b33e-9d85da914481" />

<img width="228" height="82" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-22 á„‹á…©á„Œá…¥á†« 2 06 03" src="https://github.com/user-attachments/assets/e8b456d5-ab65-4c63-a1b4-49272b825bc1" />

<br>
 
**g.** Then, threshold values also converge to time-invariant constants.

**h.** If calculated, it is optimal to continue (â‰ˆ 0.371) < stop (0.45).

<br>

---

_Input: 2025.11.21 00:36_
