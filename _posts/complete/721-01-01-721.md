## Dynamic Programming Example Problems [01-10]

Higher category: „ÄêAlgorithm„Äë [Algorithm and Machine Learning Index](https://jb243.github.io/pages/1278)

---

<br>

## Q1. 

Consider a rooted binary tree network. Each internal node x has two children, denoted by x(lhs) and x(rhs). The cost of moving from x to its child x(a) is l<sub>a</sub>, where a ‚àà {lhs,rhs}. For each node x, let L<sub>x</sub> be the minimum cost of any path from x to a leaf node. For each leaf node y, define L<sub>y</sub> = 0. Show that for any internal node x the following holds: 

<br>

<center>L<sub>x</sub> ‚Äã= min<sub>a‚àà{lhs,rhs}</sub> ‚Äã {l<sub>a</sub> + L<sub>x</sub>(a)}.</center>

## S1.

See [this link](https://www.youtube.com/watch?v=yzZVRXJ0d80&list=PLGboZ4litMr_TOwUANH-s-uFnczzy2uuW).

<br>

<br>

## Q2. 

An investor has a fund. It is worth x pounds at time zero, and the principal cannot be withdrawn. The fund pays interest at rate r √ó 100% per year for T years. At time t, the investor consumes a proportion a<sub>t</sub> of the interest and reinvests the rest in the fund. What choice of {a<sub>t</sub>} should the investor make in order to maximize total consumption?

## S2. 

See [this link](https://www.youtube.com/watch?v=VMjrzLld9W0&list=PLGboZ4litMr_TOwUANH-s-uFnczzy2uuW&index=3). The optimal policy is to save all income at first and then, once a certain threshold is reached, switch to consuming all of it.

<br>

<br>

## Q3. 

You want to sell a car. At each time (t = 0, ..., T-1), you choose a price p<sub>t</sub>, and then a customer comes and looks at the car. The probability that the customer buys the car at price p is D(p). If the car has not been sold by time T, it is sold for a fixed price W<sub>T</sub>, where W<sub>T</sub> < 1. Maximize the expected reward from selling the car, and find the recursion (dynamic programming / Bellman equation) for the optimal reward when D(p) = max(1 - p, 0).

## S3. 

See [this link](https://www.youtube.com/watch?v=d1VkSkt0EbY&list=PLGboZ4litMr_TOwUANH-s-uFnczzy2uuW&index=7). The expected reward becomes larger as t gets closer to 0, and the price p<sub>t</sub> at time t should be set as: 

<br>

<center>p<sub>t</sub> = ((expected gain at t+1) + 1‚Äã) / 2.</center>

<br>

<br>

## Q4. 

You own an expensive fish. Each day you are offered a price X for the fish, where X has probability density function f(x). You may either accept or reject the offer. With probability 1‚àíp the fish dies on that day. Find the policy that maximizes the expected profit from selling the fish.

## S4. 

See this [link](https://www.youtube.com/watch?v=oGodjYGcy5A&list=PLGboZ4litMr_TOwUANH-s-uFnczzy2uuW&index=8&pp=iAQB). This system shows a time-invariant characteristics. Thus, the Bellman equation is as follows:

<br>

<center>V<sub>t</sub>(x) = max{x, pùîº[V<sub>t+1</sub>(X<sub>t+1</sub> „Ö£ x)]} = max{x, pùîº[V<sub>t+1</sub>(X<sub>t+1</sub>)]} = max{x, a},</center>

<br>

where a is a constant. Therefore, the optimal policy is as follows: if x ‚â• a, buy it; otherwise, reject it. Here, we can obtain the following fixed-point equation

<br>

<center>a = ‚à´<sub>-‚àû to a</sub> af(y)dy + ‚à´<sub>a to ‚àû</sub> yf(y) dy ‚áî 0 = ‚à´<sub>a to ‚àû</sub> (y - a)f(y) dy.</center>

<br>

The LHS is fixed to zero, but the RHS is nonincreasing on a, so the fixed-point is unique.

<br>

---

_Input: 2025.11.21 00:36_
