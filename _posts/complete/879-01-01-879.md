## **Chapter 5-2. Types of Distances and Similarities**

Recommended Article: 【Statistics】 Chapter 5. [Statistics](https://jb243.github.io/pages/1625)

---

**1.** [Overview](#1-overview)

**2.** [Types of Norm Concepts](#4-types-of-norm-concepts)

**3.** [Types of Distance Concepts](#3-types-of-distance-concepts)

**4.** [Types of Similarity Concepts](#4-types-of-similarity-concepts)

---

<br>

## **1\. Overview**

 ⑴ The concepts of distance and similarity are often used interchangeably.

> ① Commonality: Saying two data points are close (short distance) is equivalent to saying they are similar. In other words, distance ∝ 1 / similarity.

> ② Commonality: In machine learning, terms like loss function, error function, and cost function also refer to the difference between the true value and the predicted value (∝ 1 / similarity).

> ③ Difference: While a distance function is rigorously defined in [linear algebra](https://jb243.github.io/pages/1897), it's not necessary for loss functions or similarity measures to satisfy that definition.

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/14750d75-3811-4b13-9a84-108367ccb9d8)

<br>

⑵ The distinction between the following distance concepts and similarity concepts is not rigorous.

⑶ Various types of distances and concepts of similarity.

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/de664a3e-1db6-4428-bdd6-3c94deadc082)

**Figure 1.** Various types of distances and concepts of similarity

<br>

<br>

## **2. Types of Norm Concepts**

⑴ **Type 1.** L1-norm

⑵ **Type 2.** L2-norm

⑶ **Type 3.** p-norm

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/a0154c4a-7c97-4bb0-85c1-bb8f58b54ae9)

<br>

⑷ **Type 4.** Frobenius norm

<br>

<br>

## **3\. Types of Distance Concepts**

⑴ **Type 1.** [L1 Loss Function](https://jb243.github.io/pages/1140) (L1-distance, MAE)

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/704693e0-2df3-4238-8aec-0af269a5466e)

<br>

> ① **1-1.** Manhattan Distance: A method of calculating distance by setting paths in shapes like 'ㄱ' and 'ㄴ'.

 ⑵ **Type 2.** [L2 Loss Function](https://jb243.github.io/pages/1140) (L2-distance, MSE): Euclidean distance using the Pythagorean theorem (**standard**)

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/091c3d05-2507-4c39-a81a-261180769e05)

<br>

 ⑶ **Type 3.** [Cross Entropy](https://jb243.github.io/pages/1140): Typically has a binary cross-entropy (BCE).

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/ad4a4661-9016-432b-bedd-6c4b2c2dbaf2)

<br>

⑷ **Type 4.** Distance in [Information Theory](https://jb243.github.io/pages/2145)

<br>

![image](https://github.com/user-attachments/assets/35701e55-b2cf-4a8b-b16c-e48a919fc34d)

<br>

⑸ **Type 5.** [Delaunay Triangulation](https://jb243.github.io/pages/1140)(Delaunay triangulation)

⑹ **Type 6.** Dot Product: Vector inner product

⑺ **Type 7.** [Linkage Metric](https://jb243.github.io/pages/2150): Defines distance between clusters

⑻ **Type 8. Hamming Distance**: Assigns a binary value to each data point and measures the distance of the data point as the difference in values.

⑼ **Type 9. Standardized Distance**:

> ① Distance standardized by the measurement unit of the variable.

> ② Formula:

>> d(i, j)<sup>2</sup> = (X<sub>i</sub> - X<sub>j</sub>)<sup>T</sup> D<sup>-1</sup> (X<sub>i</sub> - X<sub>j</sub>) 

>> ○ X<sub>i</sub>: Starting point matrix

>> ○ X<sub>j</sub>: Endpoint matrix

>> ○ D: Sample variance (diagonal) matrix

⑽ **Type 10.** Mahalanobis Distance**

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/d096966d-1476-463f-b019-0f85e59a97a0)

**Figure 2.** Mahalanobis Distance

<br>

> ① A statistical distance that considers both the standardization of variables and the correlation between variables (shape of the data distribution).

> ② Formula: When trying to determine the distance d between two data points X<sub>i</sub> and X<sub>j</sub>, the following formula is used:

>> d(i, j)<sup>2</sup> = (X<sub>i</sub> - X<sub>j</sub>)<sup>T</sup> S<sup>-1</sup> (X<sub>i</sub> - X<sub>j</sub>) 

>> ○ X<sub>i</sub> : Starting point matrix

>> ○ X<sub>j</sub> : Endpoint matrix

>> ○ S : Sample covariance matrix

> ③ Advantages: Unlike Euclidean distance, it is scale-free, considers data correlation, and has benefits such as outlier detection.

> ④ Limitations: Assumes the normality of the data. The process of calculating the sample covariance matrix is computationally intensive.

> ⑤ Python code

<br>

```python
import numpy as np
from scipy.linalg import inv

def mahalanobis_distance(x, y, covariance_matrix, regularization=1e-6):
    # Add regularization to the covariance matrix's diagonal
    regularized_cov = covariance_matrix + np.eye(covariance_matrix.shape[0]) * regularization
    
    x_minus_y = np.array(x) - np.array(y)
    covariance_matrix_inv = inv(regularized_cov)
    distance = np.dot(np.dot(x_minus_y, covariance_matrix_inv), x_minus_y.T)
    return np.sqrt(distance)

# Example usage:
x = [1, 2, 3]
y = [4, 5, 6]
data = np.array([x, y])
cov_matrix = np.cov(data, rowvar=False)  # Here, we're just using the covariance of x and y for simplicity

print(mahalanobis_distance(x, y, cov_matrix))
```

<br>

⑾ **Type 11.** Levenshtein Distance: An algorithm that determines how similar two strings, A and B, are to each other.

⑿ **Type 12.** Minkowski Distance

> ① Distance in m-dimensional Minkowski space.

> ② When m = 1, it is equivalent to Manhattan distance.

> ③ When m = 2, it is equivalent to Euclidean distance.

⒀ **Type 13.** Hausdorff Distance

> ① Formalization: For two sets A = {a<sub>1</sub>, ..., a<sub>p</sub>} and B = {b<sub>1</sub>, ..., b<sub>q</sub>},

>> H(A, B) = max(h(A, B), h(B, A)) 

> ② directed Hausdorff distance: The distance between the two points in A and B that are furthest apart,

>> h(A, B) = max<sub>a ∈ A</sub> min<sub>b ∈ B</sub> <span>|</span><span>|</span> a - b <span>|</span><span>|</span>

⒁ **Type 14.** Focal Loss

> ① Formalization

>> FL = -(1 - P<sub>t</sub>)<sup>γ</sup> log (P<sub>t</sub>)

⒂ **Type 15.** Sørensen–Dice Coefficient (Dice Distance)

> ① Formalization

>> 2 <span>|</span> A ∩ B<span>|</span> / <span>|</span> A ∪ B <span>|</span>

⒃ **Type 16.** [Gromov-Wasserstein distance](https://jb243.github.io/pages/2386) (Kantorovich–Rubinstein metric, Earth Mover's Distance, EMD)

⒄ **Type 17.** Sinkhorn divergence

⒅ **Type 18.** Cressie-Read power divergence

⒆ **Type 19.** Jensen-Shannon distance

⒇ **Type 20.** total variation (TV) distance

⒇ **Type 21.** Kolmogorov-Sminrov distance

⒇ **Type 22.** Hellinger distance: Requires kernel density estimation for probability density function (pdf).

⒇ **Type 24.** Huber loss function

⒇ **Type 25.** Bhattacharyya loss 

⒇ **Type 26.** evidence lower bound (ELBO)

⒇ **Type 27.** Aitchison distance: Concept of distance in a [simplex](https://nate9389.tistory.com/2203#footnote_link_67_50).

⒇ **Type 28.** Bray Curtis distance

> BC<sub>ij</sub> = 1 - 2C<sub>ij</sub> / (S<sub>i</sub> + S<sub>j</sub>)

> ① i = one site, j = another site

> ② S<sub>i</sub> = numbers of species in i, S<sub>j</sub> = numbers of species in j

> ③ C<sub>ij</sub> = the less number of the overlapping sites in species

⒇ **Type 29.** Fourier loss 

<br>

<br>

## **4\. Types of Similarity Concepts**

⑴ **Type 1.** [Pearson Correlation Coefficient](https://jb243.github.io/pages/1625)(Pearson correlation coefficient)

> ① Given the standard deviations σx, σy of X and Y,

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/9a95df8f-293f-4cf2-beb3-1eeca529cfa7)

<br>

⑵ **Type 2.** [Spearman Correlation Coefficient](https://jb243.github.io/pages/1625)(Spearman correlation coefficient)

> ① Define x' = rank(x) and y' = rank(x),

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/2fb73f40-0d74-41cb-bf01-31d48e691188)

<br>

⑶ **Type 3.** [Kendall Correlation Coefficient](https://jb243.github.io/pages/1625#)(Kendall correlation coefficient)

> ① Define correlation for concordant and discordant pairs,

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/04049609-9668-40cc-a4e2-bbe749c986cf)

<br>

⑷ **Type 4.** Matthew correlation coefficient (MCC)

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/5d476c4f-780b-45c4-b54a-7e937d602000)

<br>

⑸ **Type 5.** [χ<sup>2</sup>](https://jb243.github.io/pages/1625)

> ① For measurement data xm, ym, and the approximating function f(x),

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/9ae571c8-3deb-4a81-8bf1-7d16835cf743)

<br>

⑹ **Type 6.**[SSIM](https://jb243.github.io/pages/2067)

> ① Image similarity comparison algorithm

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/b52c2038-74c0-4d36-94f7-ed244afaf72c)

<br>

> ② [Python Code](https://jb243.github.io/pages/1892)

<br>

```python
def SSIM(x, y):
    # assumption : x and y are grayscale images with the same dimension

    import numpy as np
    
    def mean(img):
        return np.mean(img)
        
    def sigma(img):
        return np.std(img)
    
    def cov(img1, img2):
        img1_ = np.array(img1[:,:], dtype=np.float64)
        img2_ = np.array(img2[:,:], dtype=np.float64)
                        
        return np.mean(img1_ * img2_) - mean(img1) * mean(img2)
    
    K1 = 0.01
    K2 = 0.03
    L = 256 # when each pixel spans 0 to 255
   
    C1 = K1 * K1 * L * L
    C2 = K2 * K2 * L * L
    C3 = C2 / 2
        
    l = (2 * mean(x) * mean(y) + C1) / (mean(x)**2 + mean(y)**2 + C1)
    c = (2 * sigma(x) * sigma(y) + C2) / (sigma(x)**2 + sigma(y)**2 + C2)
    s = (cov(x, y) + C3) / (sigma(x) * sigma(y) + C3)
        
    return l * c * s
```

<br>

⑺ **Type 7.** Mutual Information

> ① Principle: Can the second image be predicted given the first image?

> ② Useful for analyzing the relationship between two images obtained from different modalities

>> ○ Example: In MRI, T1-weighted and T2-weighted images have many inverted points; mutual information considers this.

> ③ Code

<br>

```python
def mutual_information(img1, img2):
    import numpy as np
    import cv2
    import matplotlib.pyplot as plt
    
    # img1 and img2 are 3-channel color images
    
    a = img1[:,:,0:1].reshape(img1.shape[0], img1.shape[1])
    b = img2[:,:,0:1].reshape(img2.shape[0], img2.shape[1])
    
    hgram, x_edges, y_edges = np.histogram2d(
     a.ravel(),
     b.ravel(),
     bins=20
    )

    pxy = hgram / float(np.sum(hgram))
    px = np.sum(pxy, axis=1) # marginal for x over y
    py = np.sum(pxy, axis=0) # marginal for y over x
    px_py = px[:, None] * py[None, :] # Broadcast to multiply marginals

    nzs = pxy > 0 # Only non-zero pxy values contribute to the sum
    
    return np.sum(pxy[nzs] * np.log(pxy[nzs] / px_py[nzs]))
```

<br>

> ④ [Reference](https://matthew-brett.github.io/teaching/mutual_information.html)

⑻ **Type 8.** [Relative entropy](https://jb243.github.io/pages/1140)(Kullback-Leibler divergence, KL divergence, KLD)

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/aa6bf930-721a-4456-8e57-49b722663d0e)

<br>

⑼ **Type 9.** Mr (Thresholded Mander's Colocalization Coefficient)

> ① Ratio of overlapping pixels between two different monochrome images

> ② tMr (Thresholded Mr): Mr calculated considering values below a specific threshold as background with zero values

> ③ Background: Pearson correlation coefficient is not suitable for comparing monochrome images due to its negative values

> ④ **Feature 1.** Ranges from 0 to 1

> ⑤ **Feature 2.** Sensitive to background pixel values, but not heavily influenced by values of overlapping pixels

> ⑥ **Feature 3.** Dependent on Pearson correlation

> ⑦ **Step 1.** First, use Pearson correlation to obtain p-value and test for colocalization

> ⑧ **Step 2.** If colocalization is present, calculate tM1 and tM2 values

> ⑨ Usage: [ImageJ](https://www.med.unc.edu/microscopy/wp-content/uploads/sites/742/2018/06/Dr-Bob-on-Colocalization.pdf)

⑽ **Type 10.** Jaccard Similarity (IoU, intersection over union)

> ① Jaccard score: For two sets A and B,

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/dac90ae0-752e-4aef-a071-90095950acb7)

<br>

⑾ **Type 11.** Cosine Similarity

> ① Cosine value: For two vectors A and B,

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/0102ad89-ca48-40ec-8ace-3aa2acbd1402)

<br>

⑿ **Type 12.** Euclidean Similarity

> ① Euclidean distance: For two vectors A and B,

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/f07d878f-66a4-4894-a5ca-a24230700145)

<br>

⒀ **Type 13.** Coverage Score

> ① For two sets A and B,

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/bdbdec86-ae8b-4c58-80ee-79e7fc2cefed)

<br>

⒁ **Type 14.** Fisher Exact Test

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/74c0b388-16b7-40c1-ad41-d7edef619a18)

![image](https://github.com/JB243/jb243.github.io/assets/55747737/66cea093-5b54-4b7d-be12-8c2b46741511)

<br>

> ① For two sets A and B,

⒂ **Type 15.** Faiss: Faiss is a library for efficient similarity search and clustering of dense vectors. Developed by Meta.

⒃ **Type 16.** Smith–Waterman similarity: Used for evaluating the similarity between nucleic acid or amino acid sequences.

⒄ **Type 17.** Maximal information coefficient (MIC)

<br>

---

_Input: 2022.08.02 16:03_

_Modified: 2023.08.23 14:28_
