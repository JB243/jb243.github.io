## Stochastic Control Theory

Recommended post: „ÄêControl Theory„Äë [Table of Contents for Control Theory](https://jb243.github.io/pages/1909)

---

**1.** [Sigma-algebra](#1-sigma-algebra)

**2.** [Terminology of Stochastic Control Theory](#2-terminology-of-stochastic-control-theory)

**3.** [Laws of Stochastic Control Theory](#3-laws-of-stochastic-control-theory)

---

**a.** [Reinforcement Learning](https://jb243.github.io/pages/2162)

**b.** [The Power of Belief](https://jb243.github.io/pages/869)

---

<br>

## **1. Sigma-algebra**

‚ë¥ [Probability Space](https://jb243.github.io/pages/1623)

‚ëµ [Sigma-algebra](https://jb243.github.io/pages/910)(œÉ-algebra)

<br>

<br>

## **2. Terminology of Stochastic Control Theory**

‚ë¥ Variable definitions

> ‚ë† **state** (system state) x<sub>t</sub>**:** denotes a specific value or a random variable; same below

> ‚ë° **observation** y<sub>t</sub>**:** in the perfect observation case, y<sub>t</sub> = x<sub>t</sub>

> ‚ë¢ **noise** (system noise), **disturbance**, **primitive random variable** w<sub>t</sub>, v<sub>t</sub>

> ‚ë£ **system state noise** w<sub>t</sub>

> ‚ë§ **observation noise** v<sub>t</sub>

> ‚ë• primitive random seed creating stochastic uncertainty x<sub>0</sub>

> ‚ë¶ **control** u<sub>t</sub>

> ‚ëß **control strategy / law / policy** g<sub>t</sub>

> ‚ë® **system state sequence** `x<sub>t+1</sub> := f<sub>t</sub>(x<sub>t</sub>, u<sub>t</sub>, w<sub>t</sub>)`

> ‚ë© **observation sequence** `y<sub>t</sub> := h<sub>t</sub>(x<sub>t</sub>, v<sub>t</sub>)`

> ‚ë™ **control input**, **action** `u<sub>t</sub> := g<sub>t</sub>(y<sub>0:t</sub>, u<sub>0:t-1</sub>)`. Using all past information is called **perfect recall**.

‚ëµ Classification by system sequence

> ‚ë† **DDS** (deterministic system)**:** `x<sub>t+1</sub> := f<sub>t</sub>(x<sub>t</sub>, u<sub>t</sub>, w<sub>t</sub>) = f<sub>t</sub>(x<sub>t</sub>, u<sub>t</sub>). y<sub>t</sub> := h<sub>t</sub>(x<sub>t</sub>, v<sub>t</sub>) = h<sub>t</sub>(x<sub>t</sub>)`. Case where at any time t both the state variable xt and the output variable y<sub>t</sub> are known

> ‚ë° **SDS** (stochastic dynamical model): `x<sub>t+1</sub> := f<sub>t</sub>(x<sub>t</sub>, u<sub>t</sub>, w<sub>t</sub>), y<sub>t+1</sub> := h<sub>t</sub>(x<sub>t</sub>, v<sub>t</sub>), w<sub>t</sub>, v<sub>t</sub> ‚â¢ 0`.

‚ë∂ Classification by control input

> ‚ë† **open loop control**: `u<sub>t</sub> := g<sub>t</sub>(y<sub>0:t</sub>, u<sub>0:t-1</sub>) = g<sub>t</sub>(u<sub>0:t-1</sub>)`.

> ‚ë° **feedback control**: cases where past outputs y<sub>0:t</sub> influence the control action u

> ‚ë¢ **centralized stochastic control :** **(1)** stochastic dynamical system + **(2)** one controller + **(3)** controller with perfect recall

> ‚ë£ **multi-controller problem**: team problem, competitive game, etc.

‚ë∑ Classification by policy

> ‚ë† **decision process**: a general framework that deals with decision-making problems where state, action, and reward follow through a process

> ‚ë° **Markov process**: (regardless of whether it is a decision process) the future depends only on the current state

<br>

<img width="577" height="29" alt="image" src="https://github.com/user-attachments/assets/5ce57777-d8e9-4464-ad03-14c48e2c9e82" />

<br>

>> ‚óã **Markov chain**: among Markov processes, refers to those with a finite or countably infinite state space

>> ‚óã **controlled Markov chain**: Markov chain + decision process

<br>

<img width="330" height="26" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2025-10-05 ·Ñã·Ö©·Ñå·Ö•·Ü´ 2 11 59" src="https://github.com/user-attachments/assets/4433ae56-3122-497e-b289-8897944f23b9" />

<br>

> ‚ë¢ **MDP** (Markov decision process): among decision processes, cases where the future depends only on the current state

>> ‚óã **dynamic programming**: recurrence relation (break the time dependence). If MDP refers to the system framework, dynamic programming refers to the methodology.

>> ‚óã **POMDP** (partially observed Markov decision process): an MDP system where only partial information rather than full state information can be used

>> ‚óã **constrained MDP**, **constrained POMDP** also exist

>> ‚óã [Related algorithms](https://jb243.github.io/pages/2162)

> ‚ë£ **Gaussian process**: the state process {X<sub>t</sub>} is such that any finite subset follows a joint Gaussian distribution

> ‚ë§ **Gaussian-Markov process**

>> ‚óã **Condition 1.** {X<sub>t</sub>} is a Gaussian process

>> ‚óã **Condition 2.** Markov property: P(X<sub>n+1</sub> ‚àà A | X<sub>0</sub>, ¬∑¬∑¬∑, X<sub>n</sub>) = P(X<sub>n+1</sub> ‚àà A | X<sub>n</sub>)

<br>

<br>

## **3. Laws of Stochastic Control Theory**

‚ë¥ **Lemma 1.** In open-loop control, x<sub>t</sub> is a function of x<sub>0</sub>, u<sub>0:t-1</sub>, w<sub>0:t-1</sub>, and y<sub>t</sub> is a function of x<sub>0</sub>, u<sub>0:t-1</sub>, w<sub>0:t-1</sub>, v<sub>t</sub>

<br>

<img width="599" height="207" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2025-10-05 ·Ñã·Ö©·Ñå·Ö•·Ü´ 2 13 47" src="https://github.com/user-attachments/assets/ac28b4fd-761c-4b26-948d-c19f489626d5" />

<br>

‚ëµ **Lemma 2.** open-loop system vs. feedback system

> ‚ë† Under DDS, open-loop and feedback systems are equivalent: because uniqueness of u<sub>t</sub> in DDS naturally holds,

<br>

<img width="549" height="126" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2025-10-05 ·Ñã·Ö©·Ñå·Ö•·Ü´ 2 14 14" src="https://github.com/user-attachments/assets/26b25c6a-7f14-447a-830c-c2f394adc13f" />

<br>

> ‚ë° Under SDS, open-loop and feedback systems are not equivalent

>> ‚óã **Counterexample 1.**

<br>

<img width="298" height="368" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2025-10-05 ·Ñã·Ö©·Ñå·Ö•·Ü´ 2 14 49" src="https://github.com/user-attachments/assets/fcac0b86-524a-4099-bcad-0cfe27647da7" />

<br>

‚ë∂ **Lemma 3.** **policy independence**: If W<sub>t</sub> is independent of X<sub>0:t-1</sub>, U<sub>0:t-1</sub>, then ‚Ñô(x<sub>t+1</sub><sup>g</sup> ‚àà A | x<sub>0:t</sub>, u<sub>0:t</sub>) = ‚Ñô(x<sub>t+1</sub><sup>g</sup> ‚àà A | x<sub>t</sub>, u<sub>t</sub>) = ‚Ñô(f<sub>t</sub>(x<sub>t</sub>, u<sub>t</sub>, w<sub>t</sub>) ‚àà A | x<sub>t</sub>, u<sub>t</sub>) (**Markov property**), so dependence on policy g disappears

<br>

<img width="300" height="285" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2025-10-05 ·Ñã·Ö©·Ñå·Ö•·Ü´ 2 16 25" src="https://github.com/user-attachments/assets/a841d6d1-4d9a-46f9-9f78-b5753b49c126" />

<br>

> ‚ë† In DDS, if you know the current state, you can know the next state immediately, but in SDS, past states matter, so conditional probabilities on the history are important.

> ‚ë° That is, when w<sub>t</sub> is independent, system evolution follows natural laws + pure noise, so the policy is irrelevant; but if w<sub>t</sub> depends on the policy, the policy changes the noise distribution, so the future state distribution depends on the policy.

> ‚ë¢ Sometimes written as W<sub>t</sub> ‚´´ x<sub>0</sub>, U<sub>0:t-1</sub>, W<sub>0:t-2</sub> instead of W<sub>t</sub> ‚´´ x<sub>0</sub>, U<sub>0:t-1</sub>, but the former is a stronger claim.

‚ë∑ **Lemma 4.** **Gaussian process** (GP)

> ‚ë† Definition: the state process {X<sub>t</sub>} is such that any finite subset follows a joint Gaussian distribution

> ‚ë° **4-1.** Even if each X<sub>i</sub> is Gaussian, it does not imply {X<sub>i</sub>}<sub>i‚àà‚Ñï</sub> is a GP.

>> ‚óã Example: X<sub>2</sub> = X<sub>1</sub> *I*{|X<sub>1</sub>| ‚â§ k} + (-X<sub>1</sub>) *I*{|X<sub>1</sub>| > k}, Y = (X<sub>1</sub> + X<sub>2</sub>) / 2 is not a GP

> ‚ë¢ **4-2.** For X<sub>t+1</sub> = AX<sub>t</sub> + BU<sub>t</sub> + GW<sub>t</sub>, X<sub>0</sub> ~ ùí©(0, ‚àë<sub>0</sub>), W<sub>t</sub> ~ ùí©(0, Q), {X<sub>t</sub>} is a GP

> ‚ë£ **4-3.** Under a feedback policy, {Xt} is generally not a GP

> > ‚óã Example: If Ut := gt(Yt) = gt(Xt) = Xt2, then X1 = AX0 + BX02 + GW0, which is not Gaussian

> ‚ë§ (Note) **MMSE** (minimum mean-square estimator)

> ‚ë• (Note) **orthogonality principle**

> ‚ë¶ (Note) **LMMSE** (linear minimum mean-square estimator): If X and Y are jointly Gaussian, then LMMSE = MMSE holds

‚ë∏ **Lemma 5.** **multi-step prediction**

> ‚ë† In general, ‚Ñô(xt+2g ‚àà A | xt, ut, ut+1) ‚â† ‚Ñô(xt+2g ‚àà A | x0:t, u0:t+1)

> ‚ë° Since ut+1 = gt(y0:t+1, u0:t) that is not independent of u0:t-1 implies information about wt via observation, conditioning on ut+1 breaks the past-independence of wt: here ‚Äúpast‚Äù means x0:t-1, u0:t-1

> > ‚óã **Counterexample 1.** In open-loop control, ut+1 = gt(u0:t) holds, so it cannot imply information about wt, hence equality holds.

> > ‚óã **Counterexample 2.** When wt is a constant

> > ‚óã **Counterexample 3.** When ut is defined to have the Markov property and memoryless feedback, e.g., ut = Œºt(xt): the following is the case yt = xt = ut

> ‚ë¢ multi-step prediction with open-loop control

> ‚ë£ **Chapman-Kolmogorov decomposition**

‚ëπ **Lemma 6.** **linear Gaussian state-space model**

> ‚ë† (Note) **Gaussian-Markov process**

> > ‚óã **Condition 1.** {Xt} is a Gaussian process

> > ‚óã **Condition 2.** Markov property: P(Xn+1 ‚àà A | X0, ¬∑¬∑¬∑, Xn) = P(Xn+1 ‚àà A | Xn)

> ‚ë° System definition

> > ‚óã Markov property: applies even with feedback policy

> > ‚óã multi-step Markov property

> > ‚óã mean propagation

> > ‚óã cross-covariance Cov(**X** t+m, **X** t)

> > ‚óã covariance propagation

> ‚ë¢ **DALE** (discrete-time algebraic Lyapunov equation)

> > ‚óã If the absolute values of all eigenvalues (including complex ones) of a square matrix A are less than 1, the matrix is defined as stable: because A‚àû = 0

> > ‚óã If A is stable, then ‚àë‚àû = limt‚Üí‚àû ‚àët = limt‚Üí‚àû ùîº[(Xt - ùîº[Xt])(Xt - ùîº[Xt])·µÄ] exists uniquely

> > ‚óã Proof of uniqueness of ‚àë‚àû

> > ‚óã stability of A is a **sufficient**, but **not necessary** condition: ‚àë‚àû may still exist uniquely even if A is not stable

> ‚ë£ **reachability**

> > ‚óã Definition

> > ‚óã **Theorem 1.** The following are all equivalent: assume w ‚àà ‚Ñùs

> > > ‚óã In condition 3, the noise sequence w should be interpreted as the control input applied to the system; due to them, the system can be steered from 0 to a given state x over n time steps.

> > ‚óã **Theorem 2.** **Lyapunov stability test**

> > > ‚óã Note that in condition 2 it is PD (positive definite), not PSD (positive semidefinite)

‚ë∫ **Lemma 7.** [Graph Theory](https://jb243.github.io/pages/616)

> ‚ë† strongly connected (= irreducible, communicable): a condition where from any node i in the graph one can reach any other node j

**Figure. 1.** Example of irreducible

**Figure. 2.** Example of reducible (state 3 is a sink)

> ‚ë° period**:** the period of a specific node i is the greatest common divisor of the lengths of all paths from i back to i

> > ‚óã Example: with two nodes A, B connected by two edges A=B, the period of each node is 2

**Figure. 3.** Example of a matrix with period m

S1 ‚Üí S2 ‚Üí ¬∑¬∑¬∑ ‚Üí Sm ‚Üí S1 ‚Üí ¬∑¬∑¬∑ has such a cycle

> ‚ë¢ aperiodic: all nodes have period 1

> > ‚óã aperiodic ‚äÇ irreduicible

> > ‚óã Example: if each node has a walk to itself, it is aperiodic

> ‚ë£ stationary state: If Pr(xn | xn-1) is independent of n, the Markov process is stationary (time-invariant)

> ‚ë§ regular

> > ‚óã regular ‚äÇ irreduicible

> > ‚óã For some natural number k, every entry of the power Mk of the transition matrix M is positive (i.e., nonzero)

> ‚ë• transition matrix

> ‚ë¶ Markov policy: ut = gt(xt)

> ‚ëß One can prove the second law of thermodynamics (law of increasing entropy) using a Markov process

> > ‚óã Because one can simulate the law of diffusion: provided a uniform stationary distribution is assumed

> > ‚óã Related concept: random walk

> ‚ë® **Perron-Frobenius theorem**

> > ‚óã **Theorem 1.** If a Markov chain with transition matrix P is strongly connected, there exists exactly one stationary distribution **q**

> > > ‚óã The stationary distribution satisfies P**q** = **q**

> > ‚óã **Theorem 2.** If a Markov chain with transition matrix P is strongly connected and aperiodic, it is called an **Ergodic Markov chain** and satisfies:

> > > ‚óã Pij: probability of transition from node j to node i. ‚àëi Pij = 1

> > > ‚óã **2-1.** The (i, j) entry Pij(k) of Pk converges to **q** i as k ‚Üí ‚àû: note it converges to the same value for fixed i regardless of j

> > > ‚óã **2-2.** Regardless of the initial state **x** 0, the k-th state **x** k converges to **q** as k ‚Üí ‚àû

‚ëª **Lemma 8.** value function

> ‚ë† **recursive and backward iteration**: for example, with present value (discounting future value to present),

> > ‚óã JTg ‚àà ‚Ñù1√ó1

> > ‚óã œÄ0 ‚àà ‚Ñù1√ón: initial distribution of the Markov chain

> > ‚óã V0g ‚àà ‚Ñùn√ó1: vector of state-wise value functions collecting expected cumulative cost at each state under policy g

> ‚ë° **Bellman equation****:** related to the discounted cost problem

> > ‚óã (Note) time-homogeneous: {xtg}t‚â•0 and {xtg}t‚â•œÑ,‚àÄœÑ‚àà‚Ñ§+ follow the same distribution

> > ‚óã **Condition 1.** time-homogeneous transition: Pt(j | i, u) = P(j | i, u) ‚àÄt

> > ‚óã **Condition 2.** time-homogeneous cost: Ct(x, y) = C(x, y) ‚àÄt

> > ‚óã **Condition 3.** stationary policy: gt = g ‚àÄt

> > ‚óã If all the above hold, one can obtain the following fixed-point equation

> > ‚óã Since Pg is stable, all eigenvalues have absolute value less than 1, so det(I - Œ≤Pg) = Œ≤ det( (1/Œ≤)I - Pg ) ‚â† 0

> > > ‚óã Vg ‚àà ‚Ñùn√ó1: vector of state-wise value functions collecting expected discounted cumulative cost at each state under policy g

> > > ‚óã Pg ‚àà ‚Ñùn√ón: transition matrix; the (i, j) entry is the probability of transition from i to j

> ‚ë¢ **Ces√†ro limit**: related to the long-term average cost problem

> ‚ë£ **Poisson equation**: related to average cost

> > ‚óã Jg is unique

> > ‚óã Lg: relative value function. Lg is not unique (**‚àµ** Lg + Œ±**1** ‚àÄŒ± ‚àà ‚Ñù is also a solution to the Poisson equation)

> > ‚óã Existence of solutions

‚ëº **Lemma 9.** When not irreducible

> ‚ë† If Pg is not irreducible, the state space S splits into the transient states T and one or more recurrent communicating classes C1, ¬∑¬∑¬∑

> ‚ë° transient state: visited only finitely many times. Eventually the process leaves the transient states and enters a recurrent state.

> > ‚óã The stationary distribution œÄg assigns probability 0 to all transient states.

> ‚ë¢ recurrent state: since a recurrent communicating class is a closed set, no communication occurs with outside nodes

> > ‚óã i ‚Üí j: means there exists a path with positive probability from i to j

> > ‚óã i ‚ÜîÔ∏é j: means i ‚Üí j and j ‚Üí i; i and j communicate

> > ‚óã positive recurrent: the mean return time to that state is finite. A chain starting in a positive recurrent state has a unique stationary distribution.

> > ‚óã null recurrent: the mean return time to that state is infinite. No stationary distribution exists.

> > ‚óã absorbing state: a state that, once entered, you remain in forever

> ‚ë£ **Example 1.** Finite state space

> > Let S = {0, 1, ¬∑¬∑¬∑, I}. Since Vg(0) = 0 and C(0, g(0)) = 0, we can focus only on ≈ö = S \ {0} = {1, ¬∑¬∑¬∑, I}, the non-absorbing states. Let ·πºg be the value vector for states in ≈ö, and let Rg be the submatrix of Pg for transitions among states inside ≈ö. Then the system of equations for these states is ·πºg = cÃÉ + Rg·πºg. To show uniqueness of ·πºg, suppose there are two solutions ·πº1g, ·πº2g. Let their difference be Ug = ·πº1g - ·πº2g; subtracting the two equations yields Ug = RgUg = ‚ãØ = (Rg)nUg = ‚ãØ = **0** (‚àµ limn‚Üí‚àû (Rg)n = 0, method of infinite descent) ‚áî ·πº1g = ·πº2g. Thus, in a finite state space where state 0 is absorbing and all other states can reach 0, the first-passage-time cost equation has a unique nonnegative solution.

> ‚ë§ **Example 2.** Countably infinite state space

> > Uniqueness is not trivial. Assuming the solution is bounded often allows one to show uniqueness. Consider the equation for the difference of two solutions Ug = RgUg. Connecting this to the Bellman equation yields Ug(‚Ñì+1) - Ug(‚Ñì) = (Œª - 1)(Ug(‚Ñì) - Ug(‚Ñì-1)). The consecutive differences Œî(‚Ñì) = Ug(‚Ñì+1) - Ug(‚Ñì) form a geometric sequence with ratio (Œª - 1). If |Œª - 1| < 1, these differences converge to 0, suggesting a bounded solution. If |Œª - 1| ‚â• 1, the differences may diverge, implying that uniqueness may fail.

‚ëΩ **Lemma 10.**
[Martingale](https://jb243.github.io/pages/910)

> ‚ë† **Doob's theorem**

> > ‚óã œÉ(X1, X2, ¬∑¬∑¬∑, Xn): the smallest œÉ-algebra that makes X1, X2, ¬∑¬∑¬∑, Xn measurable

> > ‚óã Doob's theorem: œÉ(X1, X2, ¬∑¬∑¬∑, Xn) is equivalent to the collection of all functions of the form g(X1, X2, ¬∑¬∑¬∑, Xn)

> > ‚óã The larger the œÉ-algebra, the more functions are measurable with respect to it; i.e., the more information it contains.

> ‚ë° filtration

> > ‚óã a collection of œÉ-algebras ordered increasingly by inclusion

> > ‚óã Ordered by ‚äÜ; if ‚Ñ±1 ‚äÜ ‚Ñ±2, then ‚Ñ±2 is afterwards relative to ‚Ñ±1

> > ‚óã For convenience, let time index t = 0, 1, 2, ‚ãØ; then the filtration is {‚Ñ±t} t‚àà‚Ñ§+ and satisfies ‚Ñ±s ‚äÜ ‚Ñ±t for all s ‚â§ t

> > ‚óã Intuition: represents situations where information increases as observations accumulate over time

> ‚ë¢ **martingale**

> > ‚óã Property of conditional expectation

> > > ‚óã For any random variable Y, ùîº[Y | X1, ¬∑¬∑¬∑, Xn] = ùîº[Y | œÉ(X1, ¬∑¬∑¬∑, Xn)] holds

> > > ‚óã Reason: because œÉ(X1, ¬∑¬∑¬∑, Xn) is equivalent to the set of all functions generated by X1, ¬∑¬∑¬∑, Xn

> > ‚óã Martingale: a stochastic process {Xt}t‚àà‚Ñ§+ adapted to a filtration {‚Ñ±t}t‚àà‚Ñ§+ that satisfies all of the following

> > > ‚óã **Condition 1.** Xt is ‚Ñ±t-measurable for all t ‚àà ‚Ñ§+

> > > ‚óã **Condition 2.** ùîº[|Xt|] is finite for all t ‚àà ‚Ñ§+

> > > ‚óã **Condition 3.** ùîº[Xt | ‚Ñ±s] = Xs almost surely for all s ‚â§ t and all t ‚àà ‚Ñ§+

> > ‚óã Note: an i.i.d. process is generally not a martingale (except for the constant process)

> > ‚óã Application: ùîº[Ug(Xtg) | Xt-1g] = Ug(Xt-1g)

Input: 2025.08.26 23:34
