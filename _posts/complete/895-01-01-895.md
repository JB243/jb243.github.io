## Chapter 9. Stochastic Control Theory

Recommended post: ã€Control Theoryã€‘ [Table of Contents for Control Theory](https://jb243.github.io/pages/1909)

---

**1.** [Sigma-algebra](#1-sigma-algebra)

**2.** [Terminology of Stochastic Control Theory](#2-terminology-of-stochastic-control-theory)

**3.** [Laws of Stochastic Control Theory](#3-laws-of-stochastic-control-theory)

**4.** [Advanced Topics](#4-advanced-topics)

---

**a.** [Game Theory](https://jb243.github.io/pages/1914)

**b.** [Reinforcement Learning](https://jb243.github.io/pages/2162)

**c.** [The Power of Belief](https://jb243.github.io/pages/869)

---

<br>

## **1. Sigma-algebra**

â‘´ [Probability Space](https://jb243.github.io/pages/1623)

â‘µ [Sigma-algebra](https://jb243.github.io/pages/910)(Ïƒ-algebra)

<br>

<br>

## **2. Terminology of Stochastic Control Theory**

â‘´ Variable definitions

> â‘  **State** (system state) x<sub>t</sub>: denotes a specific value or a random variable; same below

> â‘¡ **Observation** y<sub>t</sub>: in the perfect observation case, y<sub>t</sub> = x<sub>t</sub>

> â‘¢ **Noise** (system noise), **Disturbance**, **Error**, **Primitive random variable** w<sub>t</sub>, v<sub>t</sub>

> â‘£ **System state noise** w<sub>t</sub>

> â‘¤ **Observation noise** v<sub>t</sub>

> â‘¥ Primitive random seed creating stochastic uncertainty x<sub>0</sub>

> â‘¦ **Control** u<sub>t</sub>

> â‘§ **Control strategy / Law / Policy** g<sub>t</sub>

<br>

<img width="298" height="231" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-15 á„‹á…©á„Œá…¥á†« 10 06 54" src="https://github.com/user-attachments/assets/8b0ca3a5-17d6-451e-8338-dff0aabc9759" />

<br>

> â‘¨ **System state sequence** x<sub>t+1</sub> := f<sub>t</sub>(x<sub>t</sub>, u<sub>t</sub>, w<sub>t</sub>)

> â‘© **Observation sequence** y<sub>t</sub> := h<sub>t</sub>(x<sub>t</sub>, v<sub>t</sub>)

> â‘ª **Control input**, **Action** u<sub>t</sub> := g<sub>t</sub>(y<sub>0:t</sub>, u<sub>0:t-1</sub>). Using all past information is called **perfect recall**.

â‘µ Classification by system sequence

> â‘  **DDS** (deterministic system): x<sub>t+1</sub> := f<sub>t</sub>(x<sub>t</sub>, u<sub>t</sub>, w<sub>t</sub>) = f<sub>t</sub>(x<sub>t</sub>, u<sub>t</sub>). y<sub>t</sub> := h<sub>t</sub>(x<sub>t</sub>, v<sub>t</sub>) = h<sub>t</sub>(x<sub>t</sub>). Case where at any time t, both the state variable x<sub>t</sub> and the output variable y<sub>t</sub> are known.

> â‘¡ **SDS** (stochastic dynamical model): x<sub>t+1</sub> := f<sub>t</sub>(x<sub>t</sub>, u<sub>t</sub>, w<sub>t</sub>), y<sub>t+1</sub> := h<sub>t</sub>(x<sub>t</sub>, v<sub>t</sub>), w<sub>t</sub>, v<sub>t</sub> â‰¢ 0.

â‘¶ Classification by control input

> â‘  **Open loop control**: u<sub>t</sub> := g<sub>t</sub>(y<sub>0:t</sub>, u<sub>0:t-1</sub>) = g<sub>t</sub>(u<sub>0:t-1</sub>).

> â‘¡ **Feedback control**: Cases where past outputs y<sub>0:t</sub> influence the control action u.

> â‘¢ **Centralized stochastic control:** **(1)** Stochastic dynamical system + **(2)** One controller + **(3)** Controller with perfect recall

> â‘£ **Multi-controller problem**: team problem, competitive game, etc.

â‘· Classification by policy

> â‘  **Decision process**: a general framework that deals with decision-making problems where state, action, and reward follow through a process.

> â‘¡ **Markov process**: (regardless of whether it is a decision process) the future depends only on the current state

<br>

<img width="577" height="29" alt="image" src="https://github.com/user-attachments/assets/5ce57777-d8e9-4464-ad03-14c48e2c9e82" />

<br>

>> â—‹ **Markov chain**: among Markov processes, it refers to those with a finite or countably infinite state space.

>> â—‹ **Controlled Markov chain**: Markov chain + Decision process

<br>

<img width="330" height="26" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 2 11 59" src="https://github.com/user-attachments/assets/4433ae56-3122-497e-b289-8897944f23b9" />

<br>

> â‘¢ **MDP** (Markov decision process): among decision processes, it refers to cases where the future depends only on the current state.

>> â—‹ **Dynamic programming**: recurrence relation (break the time dependence). If MDP refers to the system framework, [dynamic programming](https://jb243.github.io/pages/721) refers to the methodology.

>> â—‹ **POMDP** (partially observed Markov decision process): an MDP system where only partial information rather than full state information can be used.

>> â—‹ **Constrained MDP**, **Constrained POMDP** also exist.

>> â—‹ [Related algorithms](https://jb243.github.io/pages/2162)

> â‘£ **Gaussian process**: the state process {X<sub>t</sub>} is such that any finite subset follows a joint Gaussian distribution.

> â‘¤ **Gaussian-Markov process**

>> â—‹ **Condition 1.** {X<sub>t</sub>} is a Gaussian process.

>> â—‹ **Condition 2.** Markov property: P(X<sub>n+1</sub> âˆˆ A ã…£ X<sub>0</sub>, Â·Â·Â·, X<sub>n</sub>) = P(X<sub>n+1</sub> âˆˆ A ã…£ X<sub>n</sub>)

<br>

<br>

## **3. Laws of Stochastic Control Theory**

â‘´ **Lemma 1.** In open-loop control, x<sub>t</sub> is a function of x<sub>0</sub>, u<sub>0:t-1</sub>, w<sub>0:t-1</sub>, and y<sub>t</sub> is a function of x<sub>0</sub>, u<sub>0:t-1</sub>, w<sub>0:t-1</sub>, v<sub>t</sub>.

<br>

<img width="599" height="207" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 2 13 47" src="https://github.com/user-attachments/assets/ac28b4fd-761c-4b26-948d-c19f489626d5" />

<br>

â‘µ **Lemma 2.** Open-loop system vs. Feedback system

> â‘  **Open loop control**: u<sub>t</sub> := g<sub>t</sub>(y<sub>0:t</sub>, u<sub>0:t-1</sub>) = g<sub>t</sub>(u<sub>0:t-1</sub>).

> â‘¡ **Feedback control**: Cases where past outputs y<sub>0:t</sub> influence the control action u.

> â‘¢ Under DDS, open-loop and feedback systems are equivalent.

>> â—‹ Proof for open-loop â†’ feedback: Given an open-loop control input sequence **u**, define a feedback control that ignores the state (i.e., a map returning the predetermined u<sub>t</sub> at each time t). Then, from the initial state x<sub>0</sub>, it produces the same trajectory and cost. Thus, regardless of DDS/SDS, for any open-loop there exists a feedback policy that is equivalent at that initial state. That is, open-loop âŠ‚ feedback holds always.

>> â—‹ Proof for feedback â†’ open-loop: In a DDS, all control inputs are uniquely determined (determinism). Therefore, if you pre-specify the same input sequence **u** as an open-loop policy, the resulting state evolution is identical and so is the cost.

> â‘£ Under SDS, open-loop and feedback systems are not equivalent.

>> â—‹ **Counterexample 1.**

<br>

<img width="298" height="368" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 2 14 49" src="https://github.com/user-attachments/assets/fcac0b86-524a-4099-bcad-0cfe27647da7" />

<br>

>> â—‹ In the above counterexample, the feedback system outperforms the open-loop system (i.e., it generates the lower cost).

<br>

<img width="504" height="237" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-11 á„‹á…©á„’á…® 5 44 43" src="https://github.com/user-attachments/assets/5c6f4c5d-5fb9-4301-9010-7de04bdf1641" />

<br>

â‘¶ **Lemma 3.** **Policy independence**: If W<sub>t</sub> is independent of X<sub>0:t-1</sub>, U<sub>0:t-1</sub>, then â„™(x<sub>t+1</sub><sup>g</sup> âˆˆ A ã…£ x<sub>0:t</sub>, u<sub>0:t</sub>) = â„™(x<sub>t+1</sub><sup>g</sup> âˆˆ A ã…£ x<sub>t</sub>, u<sub>t</sub>) = â„™(f<sub>t</sub>(x<sub>t</sub>, u<sub>t</sub>, w<sub>t</sub>) âˆˆ A ã…£ x<sub>t</sub>, u<sub>t</sub>) (**Markov property**), so dependence on policy g disappears.

<br>

<img width="300" height="285" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 2 16 25" src="https://github.com/user-attachments/assets/a841d6d1-4d9a-46f9-9f78-b5753b49c126" />

<br>

> â‘  In DDS, if you know the current state, you can know the next state immediately, but in SDS, past states matter, so conditional probabilities on the history are important.

> â‘¡ That is, when w<sub>t</sub> is independent, system evolution follows natural laws + pure noise, so the policy is irrelevant; but if w<sub>t</sub> depends on the policy, the policy changes the noise distribution, so the future state distribution depends on the policy.

> â‘¢ **Philosophy:** Philosophically, â€œpolicy independenceâ€ implies that diversified judgments based on individual value assessments are impossible, and that choices become constrained by factual determinations.

â‘· **Lemma 4.** **Gaussian process** (GP)

> â‘  Definition: the state process {X<sub>t</sub>} is such that any finite subset of it follows a joint Gaussian distribution.

> â‘¡ **4-1.** Even if each X<sub>i</sub> is Gaussian, it does not imply {X<sub>i</sub>}<sub>iâˆˆâ„•</sub> is a GP.

>> â—‹ **Example:** X<sub>2</sub> = X<sub>1</sub> *I*{ã…£X<sub>1</sub>ã…£ â‰¤ k} + (-X<sub>1</sub>) *I*{ã…£X<sub>1</sub>ã…£ > k}, Y = (X<sub>1</sub> + X<sub>2</sub>) / 2 is not a GP.

> â‘¢ **4-2.** For X<sub>t+1</sub> = AX<sub>t</sub> + BU<sub>t</sub> + GW<sub>t</sub>, X<sub>0</sub> ~ ğ’©(0, âˆ‘<sub>0</sub>), W<sub>t</sub> ~ ğ’©(0, Q), {X<sub>t</sub>} is a GP.

> â‘£ **4-3.** Under a feedback policy, {X<sub>t</sub>} is generally not a GP.

>> â—‹ **Example:** If U<sub>t</sub> := g<sub>t</sub>(Y<sub>t</sub>) = g<sub>t</sub>(X<sub>t</sub>) = X<sub>t</sub><sup>2</sup>, then X<sub>1</sub> = AX<sub>0</sub> + BX<sub>0</sub><sup>2</sup> + GW<sub>0</sub>, which is not Gaussian.

>> â—‹ On the other hand, in a linear Gaussian SDS, for a general open-loop policy, the state process {X<sub>t</sub>} is always Gaussian.

> â‘¤ (Note) **MMSE** (minimum mean-square estimator)

<br>

<img width="449" height="38" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 9 49 29" src="https://github.com/user-attachments/assets/ef12dfc3-3813-4761-a9b8-33943b51ad30" />

<br>

> â‘¥ (Note) **Orthogonality principle**

<br>

<img width="300" height="26" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 9 50 20" src="https://github.com/user-attachments/assets/44661695-ddf0-4556-9130-5626fcf459fb" />

<br>

> â‘¦ (Note) **LMMSE** (linear minimum mean-square estimator)

<br>

<img width="161" height="34" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 9 50 46" src="https://github.com/user-attachments/assets/a9ec5dab-5b53-4ea7-b6de-e2257b21fdbe" />

<br>

> â‘§ If X and Y are jointly Gaussian, then LMMSE = MMSE holds.

<br>

<img width="597" height="220" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-13 á„‹á…©á„’á…® 7 55 13" src="https://github.com/user-attachments/assets/fe6205b8-97b7-4488-8394-83b5cbc8bfb2" />

<br>

â‘¸ **Lemma 5.** **Multi-step prediction**

> â‘  In general, â„™(x<sub>t+2</sub><sup>g</sup> âˆˆ A ã…£ x<sub>t</sub>, u<sub>t</sub>, u<sub>t+1</sub>) â‰  â„™(x<sub>t+2</sub><sup>g</sup> âˆˆ A ã…£ x<sub>0:t</sub>, u<sub>0:t+1</sub>)

<br>

<img width="502" height="273" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 9 52 01" src="https://github.com/user-attachments/assets/2382de29-9682-47f6-ae97-f2148d18c4dc" />

<br>

>> â—‹ **Proof:** Let's think of <span style="color: blue;">x<sub>t</sub></span> â†’ y<sub>t</sub> â†’ <span style="color: blue;">u<sub>t</sub></span> â†’ <span style="color: green;">x<sub>t+1</sub></span> â†’ y<sub>t+1</sub> â†’ <span style="color: green;">u<sub>t+1</sub></span> â†’ <span style="color: orange;">x<sub>t+2</sub></span>. Since u<sub>t+1</sub> = g<sub>t</sub>(y<sub>0:t+1</sub>, u<sub>0:t</sub>) that is not independent of u<sub>0:t-1</sub> implies information about w<sub>t</sub> via xt+1 = f(xt, ut, wt), conditioning on u<sub>t+1</sub> breaks the past-independence of w<sub>t</sub>: here â€œpastâ€ means x<sub>0:t-1</sub>, u<sub>0:t-1</sub>.

>> â—‹ **Counterexample 1.** In open-loop control, u<sub>t+1</sub> = g<sub>t</sub>(u<sub>0:t</sub>) holds, so it cannot imply information about w<sub>t</sub>, hence equality holds.

>> â—‹ **Counterexample 2.** When w<sub>t</sub> is a constant

>> â—‹ **Counterexample 3.** When u<sub>t</sub> is defined to have the Markov property and memoryless feedback, e.g., u<sub>t</sub> = Î¼<sub>t</sub>(x<sub>t</sub>): the following is the case y<sub>t</sub> = x<sub>t</sub> = u<sub>t</sub>

<br>

<img width="246" height="333" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 9 54 51" src="https://github.com/user-attachments/assets/375d7041-6970-49e6-9623-19563f95ce8d" />

<br>

> â‘¡ Multi-step prediction with open-loop control

<br>

<img width="466" height="35" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 9 55 21" src="https://github.com/user-attachments/assets/c10995c6-98a2-44bd-94b7-2036f2485582" />

<br>

> â‘¢ **Chapman-Kolmogorov decomposition**

<br>

<img width="546" height="294" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 9 55 47" src="https://github.com/user-attachments/assets/0f52ce9e-6c8f-41a9-9717-2a7e6d5f0218" />

<br>

â‘¹ **Lemma 6.** **Linear Gaussian state-space model**

> â‘  (Note) **Gaussian-Markov process**

>> â—‹ **Condition 1.** {X<sub>t</sub>} is a Gaussian process.

>> â—‹ **Condition 2.** Markov property: P(X<sub>n+1</sub> âˆˆ A ã…£ X<sub>0</sub>, Â·Â·Â·, X<sub>n</sub>) = P(X<sub>n+1</sub> âˆˆ A ã…£ X<sub>n</sub>)

> â‘¡ System definition

<br>

<img width="198" height="208" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 9 57 08" src="https://github.com/user-attachments/assets/1c825a4f-25ea-46b2-b9a4-a7291b845b5f" />

<br>

>> â—‹ Markov property: applies even with feedback policy.

<br>

<img width="278" height="27" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 9 57 35" src="https://github.com/user-attachments/assets/b0448876-e3a5-4e74-a0c1-546142625455" />

<br>

>> â—‹ Multi-step Markov property

<br>

<img width="446" height="248" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 9 58 08" src="https://github.com/user-attachments/assets/dd51a25b-5405-4bc8-a71d-9e3ce6b354f7" />

<br>

>> â—‹ Mean propagation

<br>

<img width="403" height="33" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 9 58 34" src="https://github.com/user-attachments/assets/3931106d-6da3-4e15-a25b-00ab4675831c" />

<br>

>> â—‹ Cross-covariance Cov(**X**<sub>t+m</sub>, **X**<sub>t</sub>)

<br>

<img width="498" height="202" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 9 59 18" src="https://github.com/user-attachments/assets/eabaf28e-9f85-4074-a97d-a04e47017cfb" />

<br>

>> â—‹ Covariance propagation

<br>

<img width="450" height="191" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 9 59 48" src="https://github.com/user-attachments/assets/ece67a8f-b4bb-4e1d-9258-691b8a6ba055" />

<br>

> â‘¢ **DALE** (discrete-time algebraic Lyapunov equation)

>> â—‹ If the absolute values of all eigenvalues (including complex ones) of a square matrix A are less than 1, the matrix is defined as stable: because A<sup>âˆ</sup> = 0.

>> â—‹ If A is stable, then âˆ‘<sub>âˆ</sub> = lim<sub>tâ†’âˆ</sub> âˆ‘<sub>t</sub> = lim<sub>tâ†’âˆ</sub> ğ”¼[(X<sub>t</sub> - ğ”¼[X<sub>t</sub>])(X<sub>t</sub> - ğ”¼[X<sub>t</sub>])áµ€] exists uniquely.

<br>

<img width="636" height="72" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 10 01 17" src="https://github.com/user-attachments/assets/380b05a6-c4db-44e3-a17d-9b8bc1a09f0f" />

<br>

>> â—‹ Proof of uniqueness of âˆ‘<sub>âˆ</sub>

<br>

<img width="330" height="182" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-11 á„‹á…©á„’á…® 11 40 32" src="https://github.com/user-attachments/assets/b4715e3b-71ca-47c4-a2e5-ced52ca8fe7c" />

<br>

>> â—‹ **Remark 1.** Stability of A is a **sufficient**, but **not necessary** condition.

>>> â—‹ âˆ‘<sub>âˆ</sub> may still exist uniquely even if A is not stable. 

>>> â—‹ A trivial example is given by âˆ‘<sub>0</sub> = 0, Q = 0, in which case âˆ‘<sub>k</sub> â‰¡ 0 independent of A. (No noise in the first place.)

>> â—‹ **Remark 2.** âˆ‘<sub>âˆ</sub> may not be strictly positive definite.

>>> â—‹ A trivial example is A = O, rank(GQG<sup>T</sup>) < n. (Noise does not touch all directions in the state.)

>> â—‹ **Remark 3.** If the input disturbance w<sub>k</sub> affects all the components of the state vector, then the stability of A would be necessary for the convergence of âˆ‘<sub>k</sub>, and the limiting covariance âˆ‘<sub>âˆ</sub> would be positive definite â†’ the concept of recapability is related.

> â‘£ **Reachability**

>> â—‹ Definition: Related to controllability and observability.

<br>

<img width="700" height="115" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 10 02 24" src="https://github.com/user-attachments/assets/e8a36fb8-de6d-4914-9044-449c77f95397" />

<br>

>> â—‹ **Theorem 1.** The following are all equivalent: assuming w âˆˆ â„<sup>s</sup>

<br>

<img width="700" height="147" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 10 02 54" src="https://github.com/user-attachments/assets/1c335335-ac4f-464a-bb0f-65990d3c6cce" />

<br>

>>> â—‹ In condition 3, the noise sequence w should be interpreted as the control input applied to the system; due to them, the system can be steered from 0 to a given state x over n time steps.

>> â—‹ **Theorem 2.** **Lyapunov stability test**

<br>

<img width="700" height="131" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 10 03 34" src="https://github.com/user-attachments/assets/aeb1568e-edf9-4db7-b5d5-854602df1dd3" />

<br>

>>> â—‹ Note that in condition 2, it is PD (positive definite), not PSD (positive semidefinite).

â‘º **Lemma 7.** [Graph Theory](https://jb243.github.io/pages/616)

> â‘  Strongly connected (= irreducible, communicable): a condition where from any node i in the graph, one can reach any other node j.

<br>

<img width="181" height="280" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 10 04 05" src="https://github.com/user-attachments/assets/f8a6de25-c6b1-42bf-a3a5-c78649d56a6d" />

**Figure 1.** Example of "irreducible"

<br>

<img width="206" height="311" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 10 04 55" src="https://github.com/user-attachments/assets/051ce217-a9e3-4ad8-9689-86c916ec4473" />

**Figure 2.** Example of "reducible" (state 3 is a sink)

<br>

> â‘¡ Period: the period of a specific node i is the greatest common divisor of the lengths of all paths from i back to i

>> â—‹ Example: with two nodes A, B connected by two edges A=B, the period of each node is 2.

>> â—‹ The transition matrix with period m should have the following form when allowing for state rearrangements like Q<sup>T</sup>PQ given a proper permutation matrix Q. 

<br>

<img width="859" height="170" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 10 05 44" src="https://github.com/user-attachments/assets/d92111d7-767e-4d2d-969a-e689714632ba" />

**Figure 3.** Example of a transition matrix with period m

S<sub>1</sub> â†’ S<sub>2</sub> â†’ Â·Â·Â· â†’ S<sub>m</sub> â†’ S<sub>1</sub> â†’ Â·Â·Â· has such a cycle.

<br>

> â‘¢ Aperiodic: all nodes have period 1.

>> â—‹ Aperiodic âŠ‚ Irreduicible

>> â—‹ Example: if each node has a walk to itself, it is aperiodic.

> â‘£ Stationary state: If Pr(x<sub>n</sub> ã…£ x<sub>n-1</sub>) is independent of n, the Markov process is stationary (time-invariant).

> â‘¤ Regular

>> â—‹ Regular âŠ‚ Irreduicible

>> â—‹ For some natural number k, every entry of the power M<sup>k</sup> of the transition matrix M is positive (i.e., nonzero).

> â‘¥ Transition matrix

<br>

<img width="456" height="177" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 10 06 49" src="https://github.com/user-attachments/assets/faff3d86-9ccb-4cbd-9c57-76adc359f6a9" />

<br>

> â‘¦ Markov policy: u<sub>t</sub> = g<sub>t</sub>(x<sub>t</sub>)

> â‘§ One can prove the second law of thermodynamics (law of increasing entropy) using a Markov process.

>> â—‹ Because one can simulate the law of diffusion: provided a uniform stationary distribution is assumed.

>> â—‹ Related concept: random walk

> â‘¨ **Perron-Frobenius theorem**

>> â—‹ **Theorem 1.** If a Markov chain with transition matrix P is strongly connected, there exists exactly one stationary distribution **q**.

>>> â—‹ The stationary distribution satisfies P**q** = **q**.

>> â—‹ **Theorem 2.** If a **finite** Markov chain with transition matrix P is strongly connected and aperiodic, it is called an **Ergodic Markov chain** and satisfies:

>>> â—‹ P<sub>ij</sub>: probability of transition from node j to node i. âˆ‘<sub>i</sub> P<sub>ij</sub> = 1. Note that P<sub>ij</sub> means the probability of transition from node i to node j in other **Lemma** below.

>>> â—‹ **2-1.** The (i,j) entry of P<sup>k</sup>, P<sub>ij</sub>(k), converges to **q**<sub>i</sub> as k â†’ âˆ: note that it converges to the same value for fixed i regardless of j.

>>> â—‹ **2-2.** Regardless of the initial state **x**<sub>0</sub>, the k-th state **x**<sub>k</sub> converges to **q** as k â†’ âˆ.

<br>

<img width="319" height="88" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-12 á„‹á…©á„’á…® 1 24 03" src="https://github.com/user-attachments/assets/299e8127-0ef5-41b5-a5a0-201769a91b93" />

<br>

â‘» **Lemma 8.** Value function following deterministic Markov property

> â‘  Expected cost and transition probability

<br>

<img width="406" height="244" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-12 á„‹á…©á„’á…® 1 52 21" src="https://github.com/user-attachments/assets/5911d1ab-e6b4-4981-8dc3-33abd34e05ce" />

<br>

> â‘¡ **Recursive and backward iteration**: Dynamic programming 

<br>

<img width="569" height="652" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 11 12 32" src="https://github.com/user-attachments/assets/71c9a2a9-e5b2-422e-8b0b-fb0f06a48a29" />

<br>

>> â—‹ J<sub>T</sub><sup>g</sup> âˆˆ â„<sup>1Ã—1</sup>

>> â—‹ Ï€<sub>0</sub> âˆˆ â„<sup>1Ã—n</sup>: initial distribution of the Markov chain

>> â—‹ V<sub>0</sub><sup>g</sup> âˆˆ â„<sup>nÃ—1</sup>: vector of state-wise value functions collecting expected cumulative cost at each state under policy g

>> â—‹ When T = âˆ, J<sup>g</sup> becomes infinite, being unable to find the optimal policy g; thus, Bellman equation, CesÃ ro limit concepts are introduced.

> â‘¢ **Bellman equation****:** The below describes the discounted cost problem primarily.

>> â—‹ (Note) Time-homogeneous: {x<sub>t</sub><sup>g</sup>}<sub>tâ‰¥0</sub> and {x<sub>t</sub><sup>g</sup>}<sub>tâ‰¥Ï„,âˆ€Ï„âˆˆâ„¤<sub>+</sub></sub> follow the same distribution. Also means strictly stationary.

<br>

<img width="841" height="280" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-07 á„‹á…©á„Œá…¥á†« 1 34 29" src="https://github.com/user-attachments/assets/279f5a7f-dd38-43c8-985a-6ea3fcd7f6fa" />

<br>

>> â—‹ **Condition 1.** Time-homogeneous transition: P<sub>t</sub>(j ã…£ i, u) = P(j ã…£ i, u) âˆ€t

>> â—‹ **Condition 2.** Time-homogeneous cost: C<sub>t</sub>(x, y) = C(x, y) âˆ€t

>> â—‹ **Condition 3.** Stationary policy: g<sub>t</sub> = g âˆ€t

>> â—‹ If all the above hold, one can obtain the following fixed-point equation.

<br>

<img width="398" height="245" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 10 10 36" src="https://github.com/user-attachments/assets/2b7ba5cf-3dcb-428a-a5ef-c764e7e6e193" />

<br>

>>> â—‹ J<sup>g</sup>: The present value of the cost; generally used in an economic context.

>>> â—‹ Since P<sup>g</sup> is stable, all eigenvalues have absolute value less than 1, so det(I - Î²P<sup>g</sup>) = Î² det((1/Î²)I - P<sup>g</sup>) â‰  0

<br>

<img width="201" height="195" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 10 12 01" src="https://github.com/user-attachments/assets/710ac80e-c439-4a5c-b687-ced04c799a26" />

<br>

>>> â—‹ V<sup>g</sup> âˆˆ â„<sup>nÃ—1</sup>: vector of state-wise value functions collecting expected discounted cumulative cost at each state under policy g

<br>

<img width="500" height="65" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 10 12 57" src="https://github.com/user-attachments/assets/bb1d01b0-e238-4732-96cf-e5ad211e37d1" />

<br>

>>> â—‹ P<sup>g</sup> âˆˆ â„<sup>nÃ—n</sup>: transition matrix; the (i,j) entry is the probability of transition from i to j.

> â‘£ **CesÃ ro limit**: related to the long-term average cost problem.

<br>

<img width="553" height="136" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 10 13 39" src="https://github.com/user-attachments/assets/e1f7e8d8-a5df-4c59-a8a8-f7ec5ac58903" />

<br>

> â‘¤ **Poisson equation**: related to average cost.

<br>

<img width="503" height="264" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 10 14 13" src="https://github.com/user-attachments/assets/bb71851f-1c70-4979-8655-dd0002b9c4b1" />

<br>

>> â—‹ J<sup>g</sup> is unique.

<br>

<img width="350" height="253" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 10 14 45" src="https://github.com/user-attachments/assets/9f6ccc83-4392-475d-acc0-552829a2c658" />

<br>

>> â—‹ L<sup>g</sup>: relative value function. L<sup>g</sup> is not unique (**âˆµ** L<sup>g</sup> + Î±**1** âˆ€Î± âˆˆ â„ is also a solution to the Poisson equation)

>> â—‹ Existence of solutions

<br>

<img width="599" height="273" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 10 15 31" src="https://github.com/user-attachments/assets/dc4df273-2ffb-48f3-868b-babcc5be93fa" />

<br>

â‘¼ **Lemma 9.** When not irreducible

> â‘  If P<sup>g</sup> is not irreducible, the state space S splits into the transient states T and one or more recurrent communicating classes C<sub>1</sub>, Â·Â·Â·

> â‘¡ Transient state: visited only finitely many times. Eventually the process leaves the transient states and enters a recurrent state.

>> â—‹ **Theorem:** The stationary distribution Ï€<sup>g</sup> of a finite-state Markov chain assigns probability 0 to all transient states.

<br>

<img width="752" height="278" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-07 á„‹á…©á„’á…® 11 27 58" src="https://github.com/user-attachments/assets/1279e4e6-bb1b-4359-a3c9-8b89164e3686" />

<br>

>> â—‹ (i) Proof: Pigeonhole principle. The finiteness is critical (and needs to be used).

>>> â—‹ _Suppose that a certain transient state i satisfies Q<sub>i</sub>=0. Since a recurrent communicating class is a closed set, no communication occurs with outside nodes once the process enters the recurrent class. Thus, the assumption implies that the transient state i moves to only transient states for all K transitions. Thus, by Pigeonhole Principle, at least one transient state is visited twice among K+1 pigeonholes (counting the start), which yields a directed cycle entirely contained in the transient set. This cycle is closed (no edge leaves it to the recurrent class within these K steps), so it forms a closed communicating class disjoint from the given recurrent class. In a finite Markov chain, every closed communicating class is recurrent; thus, we have produced a second recurrent class, contradicting the assumption of uniqueness. Therefore, the assumption Q<sub>i</sub> is false, and hence Q<sub>i</sub>>0 for every transient state i._

>> â—‹ (ii) Proof

<br>

<img width="439" height="247" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-07 á„‹á…©á„’á…® 11 28 44" src="https://github.com/user-attachments/assets/04025777-b0c7-491f-916b-830831f83486" />

<br>

> â‘¢ Recurrent state: since a recurrent communicating class is a closed set, no communication occurs with outside nodes.

>> â—‹ i â†’ j: means there exists a path with positive probability from i to j.

>> â—‹ i â†”ï¸ j: means i â†’ j and j â†’ i; i and j communicate.

>> â—‹ Positive recurrent: the mean return time to that state is finite.

>>> â—‹ A chain starting in a positive recurrent state has a unique stationary distribution.

>> â—‹ Null recurrent: the mean return time to that state is infinite. No stationary distribution exists.

>>> â—‹ **Example:** X<sub>n+1</sub> = X<sub>n</sub> + Î¾<sub>n</sub>, X<sub>0</sub> = 0, â„™(Î¾<sub>n</sub> = +1) = â„™(Î¾<sub>n</sub> = âˆ’1) = 0.5 â†’ The probability of going back to the origin is 1 but the expected time is âˆ.

>> â—‹ Absorbing state: a state that, once entered, you remain in forever

> â‘£ **Example 1.** Stationary state set F 

<br>

<img width="300" height="270" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-12 á„‹á…©á„’á…® 3 34 50" src="https://github.com/user-attachments/assets/1911891b-ac5c-48c9-b212-1d32ae1ef337" />

<br>

> â‘¤ **Example 2.** Finite state space

>> Let S = {0, 1, Â·Â·Â·, I}. Since V<sup>g</sup>(0) = 0 and C(0, g(0)) = 0, we can focus only on Åš = S \ {0} = {1, Â·Â·Â·, I}, the non-absorbing states. Let á¹¼<sup>g</sup> be the value vector for states in Åš, and let R<sup>g</sup> be the submatrix of P<sup>g</sup> for transitions among states inside Åš. (i.e., the matrix that describes how the chain moves only among the non-absorbing states before hitting 0.) Then the system of equations for these states is á¹¼<sup>g</sup> = cÌƒ + R<sup>g</sup>á¹¼<sup>g</sup>. To show uniqueness of á¹¼<sup>g</sup>, suppose there are two solutions á¹¼<sub>1</sub><sup>g</sup>, á¹¼<sub>2</sub><sup>g</sup>. Let their difference be U<sup>g</sup> = á¹¼<sub>1</sub><sup>g</sup> - á¹¼<sub>2</sub><sup>g</sup>; subtracting the two equations yields U<sup>g</sup> = R<sup>g</sup>U<sup>g</sup> = â‹¯ = (R<sup>g</sup>)<sup>n</sup>U<sup>g</sup> = â‹¯ = **0** (âˆµ lim<sub>nâ†’âˆ</sub> (R<sup>g</sup>)<sup>n</sup> = 0, method of infinite descent) â‡” á¹¼<sub>1</sub><sup>g</sup> = á¹¼<sub>2</sub><sup>g</sup>. Thus, in a finite state space where state 0 is absorbing and all other states can reach 0, the first-passage-time cost equation has a unique nonnegative solution.

> â‘¥ **Example 3.** Countably infinite state space

<br>

<img width="593" height="164" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 10 40 17" src="https://github.com/user-attachments/assets/51e73f3c-8fae-41e3-bc3e-e677123ea938" />

**Figure 4.** Countably infinite state space diagram

<br>

<img width="304" height="176" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-14 á„‹á…©á„’á…® 9 04 27" src="https://github.com/user-attachments/assets/5e685188-771a-40ac-a097-e9f3623c760a" />

<br>

>> Uniqueness is not trivial. Assuming the solution is bounded often allows one to show uniqueness. Consider the equation for the difference of two solutions U<sup>g</sup> = R<sup>g</sup>U<sup>g</sup>. Connecting this to the diagram yields U<sup>g</sup>(â„“+1) - U<sup>g</sup>(â„“) = (Î» - 1)(U<sup>g</sup>(â„“) - U<sup>g</sup>(â„“-1)). The consecutive differences Î”(â„“) = U<sup>g</sup>(â„“+1) - U<sup>g</sup>(â„“) form a geometric sequence with ratio (Î» - 1). If <span>|</span>Î» - 1<span>|</span> < 1, these differences converge to 0, suggesting a bounded solution. If <span>|</span>Î» - 1<span>|</span> â‰¥ 1, the differences may diverge, implying that uniqueness may fail.

â‘½ **Lemma 10.** [Martingale](https://jb243.github.io/pages/910)

> â‘  **Doob's theorem**

>> â—‹ Ïƒ(X<sub>1</sub>, X<sub>2</sub>, Â·Â·Â·, X<sub>n</sub>): the smallest Ïƒ-algebra that makes X<sub>1</sub>, X<sub>2</sub>, Â·Â·Â·, X<sub>n</sub> measurable.

>> â—‹ Doob's theorem: Ïƒ(X<sub>1</sub>, X<sub>2</sub>, Â·Â·Â·, X<sub>n</sub>) is equivalent to the collection of all functions of the form g(X<sub>1</sub>, X<sub>2</sub>, Â·Â·Â·, X<sub>n</sub>).

>> â—‹ The larger the Ïƒ-algebra, the more functions are measurable with respect to it; i.e., the more information it contains.

> â‘¡ Filtration

>> â—‹ A collection of Ïƒ-algebras ordered increasingly by inclusion.

>> â—‹ Ordered by âŠ†; if â„±<sub>1</sub> âŠ† â„±<sub>2</sub>, then â„±<sub>2</sub> is afterwards relative to â„±<sub>1</sub>.

>> â—‹ For convenience, let time index t = 0, 1, 2, â‹¯; then the filtration is {â„±<sub>t</sub>}<sub>tâˆˆâ„¤<sub>+</sub></sub> and satisfies â„±<sub>s</sub> âŠ† â„±<sub>t</sub> for all s â‰¤ t.

>> â—‹ Intuition: represents situations where information increases as observations accumulate over time.

> â‘¢ **Martingale**

>> â—‹ Property of conditional expectation

>>> â—‹ For any random variable Y, ğ”¼[Y ã…£ X<sub>1</sub>, Â·Â·Â·, X<sub>n</sub>] = ğ”¼[Y ã…£ Ïƒ(X<sub>1</sub>, Â·Â·Â·, X<sub>n</sub>)] holds.

>>> â—‹ Reason: because Ïƒ(X<sub>1</sub>, Â·Â·Â·, X<sub>n</sub>) is equivalent to the set of all functions generated by X<sub>1</sub>, Â·Â·Â·, X<sub>n</sub>.

>>> â—‹ Additionally, when Ïƒ(Y) âŠ‚ Ïƒ(Z), ğ”¼[ğ”¼[X ã…£ Z] ã…£ Y] = ğ”¼[ğ”¼[X ã…£ Y] ã…£ Z] = ğ”¼[X ã…£ Y] is established.

>> â—‹ Martingale: a stochastic process {X<sub>t</sub>}<sub>tâˆˆâ„¤<sub>+</sub></sub> adapted to a filtration {â„±<sub>t</sub>}<sub>tâˆˆâ„¤<sub>+</sub></sub> that satisfies all of the following

>>> â—‹ **Condition 1.** X<sub>t</sub> is â„±<sub>t</sub>-measurable for all t âˆˆ â„¤<sup>+</sup>.

>>>> â—‹ If s â‰¤ t â‰¤ sâ€² and â„±<sub>s</sub>â€‹ âŠ† â„±<sub>t</sub> âŠ† â„±<sub>sâ€²</sub>, then X<sub>t</sub> âˆˆ â„±<sub>t</sub> is not â„±<sub>s</sub>-measurable (due to insufficient information) but is â„±<sub>sâ€²</sub>-measurable.

>>> â—‹ **Condition 2.** ğ”¼[ã…£X<sub>t</sub>ã…£] is finite for all t âˆˆ â„¤<sub>+</sub>.

>>> â—‹ **Condition 3.** ğ”¼[X<sub>t</sub> ã…£ â„±<sub>s</sub>] = X<sub>s</sub>, almost surely for all s â‰¤ t and all t âˆˆ â„¤<sub>+</sub>

>>>> â—‹ **Interpretation:** Given only the information up to time s (â„±<sub>s</sub>), the optimal prediction of X<sub>t</sub> equals X<sub>s</sub> (i.e., the prediction is constrained to X<sub>s</sub>; ğ”¼[X<sub>t</sub> ã…£ â„±<sub>s</sub>] is an orthogonal projection of X<sub>t</sub> into the â„±<sub>s</sub>-measurable random variable space ('best prediction')).

>>>> â—‹ **Remark:** The martingale property is needed only when predicting the future from the past. In particular, for s > t we have ğ”¼[X<sub>t</sub> ã…£ â„±<sub>s</sub>] = X<sub>t</sub> regardless of whether X<sub>t</sub> is a martingale (assuming integrability).

>>>> â—‹ For s < t, ğ”¼[X<sub>s</sub> ã…£ â„±<sub>t</sub>] = X<sub>s</sub> also holds, because X<sub>s</sub> is â„±<sub>s</sub>-measurable, but has insufficient information due to â„±<sub>s</sub> âŠ† â„±<sub>t</sub>.

>> â—‹ Note: an i.i.d. process is generally not a martingale (except for the constant process).

>> â—‹ Application: ğ”¼[U<sup>g</sup>(X<sub>t</sub><sup>g</sup>) ã…£ X<sub>t-1</sub><sup>g</sup>] = U<sup>g</sup>(X<sub>t-1</sub><sup>g</sup>)

> â‘£ Martingale and stochastic control theory

<br>

<img width="605" height="95" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-15 á„‹á…©á„Œá…¥á†« 11 58 24" src="https://github.com/user-attachments/assets/0ddcf3f7-d599-4469-96d6-615f0f8356e0" />

<br>

â‘¾ **Lemma 11.** (`Fully observed`) â€• Optimal Policy

> â‘  Problem definition: cost-to-go function under **perfect observation**. Since control input {U<sub>t</sub>, ..., U<sub>1</sub>} is measurable by {X<sub>t</sub>, ..., X<sub>0</sub>}, the following holds:

<br>

<img width="495" height="223" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-07 á„‹á…©á„’á…® 1 59 08" src="https://github.com/user-attachments/assets/a6f57e3f-04a6-465a-abd4-b063fe9d0eeb" />

<br>

> â‘¡ When following Markov property, Bellman equation is established. Here, J<sub>t</sub><sup>g</sup>, V<sub>t</sub><sup>g<sup>M</sup></sup>(X<sub>t</sub>) is a cost-to-go from t to future.

<br>

<img width="602" height="96" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-07 á„‹á…©á„’á…® 2 01 58" src="https://github.com/user-attachments/assets/b7a3e947-d8db-4c15-b0e0-c67e0416eb97" />

<br>
 
> â‘¢ **Markovization theorem** (Markov policy sufficiency, reduction to Markov policy)

>> â—‹ **Theorem:** In a finite-horizon MDP, for any general (possibly history-dependent and randomized) policy g, there exists a behavioral Markov policy g<sup>M</sup> such that, under the same initial distribution Î¼, the joint distributions of (X<sub>t</sub>, U<sub>t</sub>) for all t = 0, ..., Tâˆ’1 and of X<sub>T</sub> â€‹are identical. Consequently, the performance (cost) J<sup>g</sup> = ğ”¼<sup>g</sup>[âˆ‘<sub>t=0 to Tâˆ’1</sub> C<sub>t</sub>(X<sub>t</sub>, U<sub>t</sub>) + C<sub>T</sub>(X<sub>T</sub>)] equals J<sup>g<sup>M</sup></sup>. Hence, without loss of optimality, one may restrict attention to randomized Markov policies.

<br>

<img width="501" height="80" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-21 á„‹á…©á„’á…® 4 11 38" src="https://github.com/user-attachments/assets/8688ece9-0b11-4d99-bd5c-84205faf2b4f" />

<br>

>> â—‹ **Proof**

<br>

<img width="497" height="354" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-10 á„‹á…©á„Œá…¥á†« 1 54 36" src="https://github.com/user-attachments/assets/8922b0d2-330c-49ad-82f7-071bd5085b60" />

<br>

> â‘£ **Comparison principle** 

>> â—‹ **Theorem:** By working backward from the objective and ensuring the Bellman inequality holds at each step, the initial value V<sub>0</sub> serves as a lower bound for the performance of all possible policies. The set of actions that achieve equality at each stage collectively constitute the optimal policy. Thus, the optimality can be verified or constructed by combining locally optimal (stage-wise) choices.

<br>

<img width="602" height="232" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-07 á„‹á…©á„’á…® 2 03 17" src="https://github.com/user-attachments/assets/58b38ece-1ccf-4909-8180-10abdbb725d7" />

<br>

>> â—‹ Proof: Using [mathematical induction](https://www.youtube.com/watch?v=t9RBuyBmFdQ) on backward

>>> â—‹ **Case 1.** t = T: Since J<sub>T</sub><sup>g</sup> = ğ”¼<sup>g</sup>[C<sub>T</sub>(X<sub>T</sub><sup>g</sup>) ã…£ X<sub>T</sub><sup>g</sup>, ..., X<sub>1</sub><sup>g</sup>, X<sub>0</sub>] = C<sub>T</sub>(X<sub>T</sub><sup>g</sup>) â‰¥ V<sub>T</sub>(X<sub>T</sub><sup>g</sup>) (âˆµ (V1)) holds, the induction assumptions are still established.

>>> â—‹ **Case 2.** If the induction assumptions are established on â„“ = t+1, ..., T, the fact that the assumptions still hold for â„“ = t can be verified as follows:

<br>

<img width="621" height="551" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-07 á„‹á…©á„’á…® 2 06 19" src="https://github.com/user-attachments/assets/550f9ab6-e2b5-48bb-b822-51757d4f3b37" />

<br>

>> â—‹ Corollary

<br>

<img width="651" height="77" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-07 á„‹á…©á„’á…® 2 06 44" src="https://github.com/user-attachments/assets/e83a46bd-c749-4dbf-931a-b6ba3746aa93" />

<br>
 
> â‘¤ **Hamiltonian-Jacobi-Bellman (HJB) equation** 

>> â—‹ Theorem: HJB is applicable to fiinite / countably infinite, state / action space. Continuous time version of Bellman equation.

<br>

<img width="723" height="308" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-07 á„‹á…©á„’á…® 2 07 21" src="https://github.com/user-attachments/assets/4e828a43-294b-4596-9f0d-7eacd0d6fd2b" />

<br>
 
>> â—‹ Proof of **theorem 1** is shown at the comparison principle already, so the following explanations are only relevant to **theorem 2**.

>> â—‹ p: A certain Markov policy g<sup>M</sup> = {g<sub>t</sub>} is optimal.

>> â—‹ q: Given âˆ€x, t, g<sub>t</sub>(x) âˆˆ arg inf<sub>uâˆˆğ’°</sub> {c<sub>t</sub>(x, u) + ğ”¼<sub>W<sub>t</sub></sub>[V<sub>t+1</sub>(f<sub>t</sub>(x, u, W<sub>t</sub>))]} (i.e., stepwise Bellman minimization is achieved)

>> â—‹ Proof of sufficiency on **theorem 2** (q â‡’ p): When x<sub>t</sub> is given for each stage, any policies satisfying the infimum is optimal (âˆµ corollary). Then, u<sub>t</sub> should be a measurable function on the current state x<sub>t</sub>, the optimal policy should be Markov policy.

<br>

<img width="405" height="241" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-12 á„‹á…©á„’á…® 5 29 43" src="https://github.com/user-attachments/assets/96663f5b-3b96-4958-8291-fb480085cc69" />

<br>

>> â—‹ Proof of necessity on **theorem 2** (p â‡’ q): If a policy is optimal Markov policy, it should achieve infimum for each stage (w.p.1); otherwise, we can construct a better policy g' with a positive probability set, implying J(g') < J(g).

<br>

<img width="598" height="718" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-14 á„‹á…©á„’á…® 6 28 01" src="https://github.com/user-attachments/assets/f45a458e-ba26-419b-8569-c61dd2c6a4ab" />

<img width="598" height="721" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-14 á„‹á…©á„’á…® 6 28 52" src="https://github.com/user-attachments/assets/ff2e96f0-59e8-4977-926e-998cd5eae7c0" />

<br>

>> â—‹ Corollary 

<br>

<img width="504" height="182" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-09 á„‹á…©á„Œá…¥á†« 1 18 06" src="https://github.com/user-attachments/assets/fa51e0ea-6345-435f-b0aa-5d17b8d11152" />

<br>

>> â—‹ **Application 1.** If an optimal path of 1 â†’ 6 is 1 â†’ 2 â†’ 3 â†’ 6, the optimal path of 2 â†’ 6 should 2 â†’ 3 â†’ 6 by HJB equation.

>> â—‹ **Application 2.** Policy evaluation: discussed below.

>> â—‹ **Application 3.** V<sub>t</sub>(x) obtained from HJB equation is called value function. It effectively decreases the size of search space.

>> â—‹ **Application 4.** Q-value (state-action value function) : Q<sub>t</sub>(x, u) = C<sub>t</sub>(x, u) + ğ”¼<sub>W<sub>t</sub></sub>[V<sub>t+1</sub>(f<sub>t</sub>(x, u, W<sub>t</sub>))], V<sub>t</sub>(x) = inf<sub>uâˆˆğ’°</sub> Q<sub>t</sub>(x, u)

>> â—‹ **Application 5.** For given finite state / action space, inf = min, and the optimal policy is deterministic Markov policy.

<br>

<img width="353" height="129" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-14 á„‹á…©á„’á…® 6 32 30" src="https://github.com/user-attachments/assets/77ec14cb-d146-4d9e-b0d7-f01c80036b04" />

<br>

>> â—‹ **Application 6.** Value function at randomized Markov policy: For u ~ Î¼<sub>t</sub>(u ã…£ i),

<br>

<img width="451" height="127" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-14 á„‹á…©á„’á…® 6 32 46" src="https://github.com/user-attachments/assets/6c0d35bf-3d52-4c1a-ab60-4ac0b4d26610" />

<br>

> â‘¥ (Note) [Blackwellâ€™s Principle of Irrelevant Information](https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-35/issue-2/Memoryless-Strategies-in-Finite-Stage-Dynamic-Programming/10.1214/aoms/1177703586.full)

>> â—‹ If Y is independent of the state and does not appear directly in the reward, then Y is irrelevant for decision-making: a decision rule that ignores y is always at least as good. In other words, having more information is not always better.

<br>

<img width="333" height="40" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-21 á„‹á…©á„’á…® 3 41 29" src="https://github.com/user-attachments/assets/31a584cc-d912-4962-bfa9-c60b67493be1" />

<br>

>> â—‹ **Example:** Suppose a doctor has to decide on a treatment for a patient. Every morning, the doctor can observe the patientâ€™s health status X, and in addition also knows what day of the week it is, Y. The available actions are â€œtreatâ€ and â€œdo not treat.â€ If the day of the week is unrelated to both the expected reward (survival probability) and the health status, then making decisions based only on X yields the same expected survival probability as using both X and Y.

>> â—‹ However, depending on technical assumptions such as the action space and measurability, the statement may need to be formulated in terms of Îµ-optimal policies, and when countability/non-countability and measurability traps are involved (e.g., the projection of a Borel set can be non-Borel), there are counterexamples in which global approximate dominance fails.

>> â—‹ **Conclusion:** In dynamic programming problems, one can mathematically prove that memoryless strategies are sufficient to optimize the expected reward.

>> â—‹ **Application:** In finite-horizon Markov decision problems, one can easily prove that Markov policies are optimal. ([ref](https://infostructuralist.wordpress.com/2010/11/08/deadly-ninja-weapons-blackwells-principle-of-irrelevant-information/))

â‘¿ **Lemma 12.** (`Partially observed`) â€• Information state

> â‘  {z<sub>t</sub>}<sub>t={0,Â·Â·Â·,T}</sub> satisfying the following given partial observation context

>> â—‹ Background: The history H<sub>t</sub> := {y<sub>0</sub>, ..., y<sub>t</sub>, u<sub>0</sub>, ..., u<sub>t-1</sub>} increases in time and the domain of it increases exponentially. (Curse of dimensionality)

>> â—‹ **Condition 1.** Compression: z<sub>t</sub> = â„“<sub>t</sub>(H<sub>t</sub>) âˆ€t 

>> â—‹ **Condition 2.** Policy / Strategy Independent : z<sub>t+1</sub> = ğ’¯<sub>t</sub>(<span style="color: orange;">z<sub>t</sub></span>, <span style="color: purple;">y<sub>t+1</sub></span>, <span style="color: purple;">u<sub>t</sub></span>) (<span style="color: orange;">â– </span> current state, <span style="color: purple;">â– </span> new information) That is, z<sub>t</sub> can be updated recursively using current state and new information without direct reference to the entire past history.

>> â—‹ **Condition 3.** Independence relative to g<sub>0:t-1</sub>: âˆ€t = 0, ..., T-1 

<br>

<img width="344" height="124" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-23 á„‹á…©á„’á…® 8 10 21" src="https://github.com/user-attachments/assets/1203d2c7-fbca-45f1-8b26-2c6dad46b484" />

<br>

> â‘¡ **Condidate 1.** z<sub>t</sub> = H<sub>t</sub> := {y<sub>0</sub>, ..., y<sub>t</sub>, u<sub>0</sub>, ..., u<sub>t-1</sub>}

>> â—‹ Ï€<sub>0</sub>(i) is as follows:

<br>

<img width="257" height="176" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-23 á„‹á…©á„’á…® 8 11 24" src="https://github.com/user-attachments/assets/8211c709-c647-4fd6-b4e6-1c2542683c72" />

<br>

> â‘¢ **Candidate 2.** Belief state: z<sub>t</sub> = Ï€<sub>t</sub> s.t. Ï€<sub>t</sub>(i) := â„™(X<sub>t</sub> = i ã…£ H<sub>t</sub>) = â„™(X<sub>t</sub> = i ã…£ y<sub>0:t</sub>, u<sub>0:t-1</sub>) âˆ€i âˆˆ S

>> â—‹ **Condition 1** is satisfied: z<sub>t</sub> = â„“<sub>t</sub>(H<sub>t</sub>) in time (compression) 

>> â—‹ **Condition 2** is satisfied: we can obtain ğ’¯<sub>t</sub> satisfiying Ï€<sub>t+1</sub> = ğ’¯<sub>t</sub>(Ï€<sub>t</sub>, y<sub>t+1</sub>, u<sub>t</sub>) directly using Bayes' rule. This is the update equation for the belief state, often called a nonlinear filter.

<br>

<img width="857" height="474" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-23 á„‹á…©á„’á…® 8 15 21" src="https://github.com/user-attachments/assets/c5c00e7f-a564-44ca-b7c4-331af9769031" />

<br>
 
>> â—‹ **Condition 3** is satisfied: using backward [mathematical induction](https://www.youtube.com/watch?v=t9RBuyBmFdQ).

>>> â—‹ **Case 1.** t = T-1: All terms do not depend on g<sub>0:T-1</sub>.

<br>

<img width="717" height="467" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-24 á„‹á…©á„Œá…¥á†« 9 58 09" src="https://github.com/user-attachments/assets/c28941d6-0a1d-4960-9ec8-c9817ae624c0" />

<br>

>>> â—‹ **Case 2.** The backward induction can be established as follows: 

<br>

<img width="645" height="448" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-24 á„‹á…©á„Œá…¥á†« 10 22 35" src="https://github.com/user-attachments/assets/52c0171f-2992-4094-8a14-4fb4d5b4f484" />

<br>

>> â—‹ **Bellman equation of belief-MDP:** The key message of this theorem is that, although the overall policy space ğ’¢ of an MDP consists of all mappings from histories to actions and is therefore extremely large, the belief Î <sub>t</sub> is sufficient, so we lose no optimality by restricting attention to separated policies of the form â€œbelief â†’ action.â€

<br>

<img width="725" height="341" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-24 á„‹á…©á„Œá…¥á†« 10 31 26" src="https://github.com/user-attachments/assets/87e90a0b-d8d9-48b8-ab37-62ab7658e6f5" />

<br>

>> â—‹ Proof

<br>

<img width="599" height="460" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-29 á„‹á…©á„Œá…¥á†« 10 21 30" src="https://github.com/user-attachments/assets/d0620de5-6974-48b8-8109-b8ed969951f5" />

<br>

>> â—‹ **Application 1.** (One-way) Separation theorem 

<br>

<img width="704" height="148" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-24 á„‹á…©á„Œá…¥á†« 11 10 57" src="https://github.com/user-attachments/assets/e97dc972-96ba-41be-9f75-f99b0de5b9d8" />

<br>

>> â—‹ **Application 2.** Cost function in belief space

<br>

<img width="552" height="64" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-24 á„‹á…©á„Œá…¥á†« 11 11 17" src="https://github.com/user-attachments/assets/52df06ce-70ff-402b-99be-071c42d34581" />

<br>

>> â—‹ **Application 3.** The following is strategy-dependent unlike belief state because it dependes on g<sub>t-1</sub>.

<br>

<img width="255" height="33" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-24 á„‹á…©á„Œá…¥á†« 11 11 34" src="https://github.com/user-attachments/assets/8543fe60-3f83-4cc4-9ab1-86f31129111d" />

<br>

>> â—‹ **Application 4.** The following is strategy-dependent unlike belief state: because if not including u<sub>t</sub> as a condition Ï€<sub>t+1</sub> = ğ”¼<sub>u<sub>t</sub> ~ p(Â· ã…£ Y<sub>0:t</sub>) [ğ’¯<sub>t</sub>(Ï€<sub>t</sub>, y<sub>t+1</sub>, u<sub>t</sub>)] is established so the information state transition is affected by the policy.

<br>

<img width="192" height="33" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-24 á„‹á…©á„Œá…¥á†« 11 11 51" src="https://github.com/user-attachments/assets/16b92058-3428-4443-abb1-d1cbd2285bc5" />

<br>

>> â—‹ **Application 5.** Generally, P<sup>g</sup>(X<sub>t+1</sub> âˆˆ A ã…£ H<sub>t</sub>, u<sub>t</sub>) â‰  P<sup>g</sup>(X<sub>t+1</sub> âˆˆ A ã…£ y<sub>t</sub>, u<sub>t</sub>), H<sub>t</sub> = (Y<sub>0:t</sub>, U<sub>0:t-1</sub>) 

>>> â—‹ **Proof:** Let's suppose t = 1. Let's think of x<sub>0</sub> â†’ y<sub>0</sub> â†’ u<sub>0</sub> â†’ x<sub>1</sub> â†’ y<sub>1</sub> â†’ u<sub>1</sub> â†’ x<sub>2</sub> â†’ â‹¯. Given H<sub>1</sub> = (y<sub>0:1</sub>, u<sub>0</sub>), we can determine the distribution of x<sub>0</sub> via y<sub>0</sub> â†’ x<sub>0</sub>. Thus, we can **more accurately** determine the distribution of x<sub>1</sub> via y<sub>1</sub> â†’ x<sub>1</sub> and (x<sub>0</sub>, u<sub>0</sub>) â†’ x<sub>1</sub>. Afterward, we can determine the distribution of x<sub>2</sub> via x<sub>1</sub>, u<sub>1</sub>, w<sub>1</sub>. However, in the right-hand side, we can only use y<sub>1</sub> â†’ x<sub>1</sub> and the limited information for u<sub>1</sub> to determine the distribution of x<sub>1</sub>, leading to less accurate distribution of x<sub>2</sub>. Thus, both sides differ.

>> â—‹ **Application 6.** P<sup>g</sup>(X<sub>t+1</sub> âˆˆ A ã…£ H<sub>t</sub>, u<sub>t</sub>) = P<sup>g</sup>(X<sub>t+1</sub> âˆˆ A ã…£ y<sub>0:t</sub>, u<sub>t</sub>)

>>> â—‹ **Proof:** Let's think of x<sub>0</sub> â†’ y<sub>0</sub> â†’ u<sub>0</sub> â†’ x<sub>1</sub> â†’ y<sub>1</sub> â†’ u<sub>1</sub> â†’ x<sub>2</sub> â†’ â‹¯. Given policy g, in the right-hand side, we can determine the distribution of u<sub>0</sub> and x<sub>0</sub> via y<sub>0</sub> â†’ u<sub>0</sub> and y<sub>0</sub> â†’ x<sub>0</sub>, respectively. Thus, we can determine the distribution of x<sub>1</sub> via (x<sub>0</sub>, u<sub>0</sub>) â†’ x<sub>1</sub>. Now by utilizing u<sub>t</sub> = g<sub>t</sub>(y0:t, u<sub>0:t-1</sub>) and x<sub>t+1</sub> = f<sub>t</sub>(x<sub>t</sub>, u<sub>t</sub>, w<sub>t</sub>), we can determine the distribution of all the variables, thus leading to the distribution of x<sub>t+1</sub> thoroughly. It is identical that from the left-hand side.

>>> â—‹ **Conclusion:** The information state Z<sub>t</sub><sup>g</sup> := (Y<sub>0:t</sub><sup>g</sup>, U<sub>0:t-1</sub><sup>g</sup>) is a function of Y<sub>0:t</sub><sup>g</sup>.

>> â—‹ **Application 7.** P(X<sub>t+1</sub> âˆˆ A ã…£ H<sub>t</sub>, u<sub>t</sub>) â‰  P(X<sub>t+1</sub> âˆˆ A ã…£ y<sub>0:t</sub>, u<sub>t</sub>)

>>> â—‹ **Proof:** Let's think of x<sub>0</sub> â†’ y<sub>0</sub> â†’ u<sub>0</sub> â†’ x<sub>1</sub> â†’ y<sub>1</sub> â†’ u<sub>1</sub> â†’ x<sub>2</sub> â†’ â‹¯. In the right-hand side, we can determine the distribution of x<sub>0</sub> via y<sub>0</sub> â†’ x<sub>0</sub> but can only determine the distribution of u<sub>0</sub> via y0 â†’ u0 on the probability over policy set. However, u<sub>0</sub> is exactly given in the left-hand side. Therefore, both sides differ. We can conclude that P<sup>g</sup>(X<sub>t+1</sub> âˆˆ A ã…£ H<sub>t</sub>, u<sub>t</sub>) is policy-independent, while P<sup>g</sup>(X<sub>t+1</sub> âˆˆ A ã…£ y<sub>0:t</sub>, u<sub>t</sub>) is policy-dependent.

â’€ **Lemma 13.** Dynamic Program

> â‘  If V<sub>t</sub>(i) = max{r(i), a + bâˆ‘<sub>jâˆˆS</sub> â„™(j ã…£ i) V<sub>t+1</sub>(j)} = max{r(i), a + bğ”¼[V<sub>t+1</sub>(j) ã…£ i]}, V<sub>t</sub>(i) â‰¥ V<sub>t+1</sub>(i) is established.

> â‘¡ If V<sub>t</sub>(x) = max{-c + p(x)(1 + V<sub>t+1</sub>(x-1)) + (1 - p(x))V<sub>t+1</sub>(x), V<sub>t+1</sub>(x)}, V<sub>N+1</sub>(x) = 0, the following is established:

>> â—‹ **Monotonicity on time:** V<sub>t</sub>(x) â‰¥ V<sub>t+1</sub>(x) ([Proof](https://jb243.github.io/pages/1802))

>> â—‹ **Monotonicity on x:** V<sub>t</sub>(x) â‰¥ V<sub>t</sub>(x-1) ([Proof](https://jb243.github.io/pages/1802))

>> â—‹ **Supremum of marginal value:** 1 â‰¥ V<sub>t</sub>(x) - V<sub>t</sub>(x-1) ([Proof](https://jb243.github.io/pages/1802))

>> â—‹ <span style="color: red">Concavity on x is not established</span>: V<sub>t</sub>(x) - V<sub>t</sub>(x-1) â‰¤ V<sub>t</sub>(x-1) - V<sub>t</sub>(x-2) (Counterexamples exist)

>> â—‹ **Relationship between marginal value and time:** V<sub>t</sub>(x) - V<sub>t</sub>(x-1) â‰¥ V<sub>t+1</sub>(x) - V<sub>t+1</sub>(x-1) ([Proof](https://jb243.github.io/pages/1802))

>> â—‹ **Existence of threshold:** G<sub>t</sub>(x) = p(x)(1 - Î”<sub>t+1</sub>(x)) is nondecreasing on x ([Proof](https://jb243.github.io/pages/1802))

> â‘¢ Convex

>> â—‹ **Lemma 1.** Given two convex functions f<sub>1</sub> and f<sub>2</sub>, max{f<sub>1</sub>, f<sub>2</sub>} is also convex.

<br>

<img width="284" height="177" alt="image" src="https://github.com/user-attachments/assets/ed125d32-36b4-4e4c-b6f1-7a22ec5062ce" />

**Figure 5.** Maximum of two convex functions is convex.

<br>

>> â—‹ **Lemma 2.** Sum of two convex functions is convex.

<br>

<img width="504" height="219" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-03 á„‹á…©á„Œá…¥á†« 12 41 32" src="https://github.com/user-attachments/assets/fefd8bd4-6627-4539-9334-a0ac44bd9168" />

<br>

>> â—‹ **Lemma 3.** If V(x) is a non-decreasing convex function, V(max{x, a}), âˆ€a is also a convex convex: it can be easily understood geometrically.

>> â—‹ **Lemma 4.** If L(Ï€) is a convex function, L(Ï€) = sup<sub>iâˆˆI</sub> {Î±<sub>i</sub> Ï€ + Î²<sub>i</sub>}, Î±<sub>i</sub>, Î²<sub>i</sub> âˆˆ â„ is established. 

>>> â—‹ The above comes from the inequality f(x) â‰¥ f(x<sub>0</sub>) + f'(x<sub>0</sub>)(x - x<sub>0</sub>); it is not a claim that the formula holds for arbitrary choices of Î±<sub>i</sub> and Î²<sub>i</sub>.

>>> â—‹ Example: If L(Ï€) = Ï€<sup>2</sup>, then Ï€<sup>2</sup> = sup<sub>x<sub>0</sub>âˆˆâ„</sub> {2x<sub>0</sub>Ï€ - x<sub>0</sub><sup>2</sup>}.

>>> â—‹ Practical point: Using transformation that preserve convexity in the linear (affine) representation, one can show that applying the same transformation to the original convex function also preserves convexity. Example is as follows.

<br>

<img width="592" height="400" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-08 á„‹á…©á„’á…® 3 02 29" src="https://github.com/user-attachments/assets/b706e04b-6c32-4d43-a5b9-3a612a3d3df6" />

<br>

â’ **Lemma 14.** (`Stochastic policy`) DCOE(discounted cost optimality equation, infinite-horizon discounted cost Bellman equation)

> â‘  [Banach fixed point theorem](https://jb243.github.io/pages/1827) 

>> â—‹ **Theorem**

>>> â—‹ Let F be a Banach space. Here, Banach space refers to a complete normed space, and a set being â€œcompleteâ€ means every Cauchy sequence in the set converges to a certain element in the set. Let T: F â†’ F be a transformation satisfying the following: ã…£ã…£Tx - Tyã…£ã…£ â‰¤ Î²ã…£ã…£x - yã…£ã…£, âˆƒÎ² âˆˆ (0,1), âˆ€x, y âˆˆ F. Then, the following is established:

>>>> â—‹ There is a unique fixed point w âˆˆ F satisfying Tw = w.

>>>> â—‹ For any x âˆˆ X, lim<sub>nâ†’âˆ</sub> T<sup>n</sup>x = w.

>>> â—‹ This essentially implies the following: 

>>>> â—‹ The transformation satisfying the above is called contraction. Strictly speaking, it requires that sup Î²(x, y) < 1.

>>>> â—‹ T is continuous, and specifically is Lipschitz continuous. 

>> â—‹ **Proof**

>>> â—‹ Let x âˆˆ F, Î± = ã…£ã…£x - Txã…£ã…£. Then, we have ã…£ã…£T<sup>n</sup>x - T<sup>n+1</sup>xã…£ã…£ â‰¤ Î²<sup>n</sup>Î±. If we set the Cauchy sequence of {x, Tx, T<sup>2</sup>x, Â·Â·Â·}, we can obtain the following: âˆ€Ïµ > 0, âˆƒN<sub>Ïµ</sub> s.t. âˆ€n, m â‰¥ N<sub>Ïµ</sub>, ã…£ã…£T<sup>n</sup>x - T<sup>m</sup>xã…£ã…£ < Ïµ. Without loss of generality, we can set n > m. Then, we have 

<br>

<img width="301" height="316" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-02 á„‹á…©á„Œá…¥á†« 8 23 50" src="https://github.com/user-attachments/assets/81a98477-742e-48a5-8585-1f5350369220" />

<br>

>>> â—‹ Thus, we have N suject to Î±Î²<sup>N</sup> / (1 - Î²) < Ïµ. As F is a Banach space, we have w subject to lim<sub>nâ†’âˆ</sub> T<sup>n</sup>x = w. Since T(lim<sub>nâ†’âˆ</sub> T<sup>n</sup>x) = Tx = lim<sub>nâ†’âˆ</sub> T<sup>n+1</sup>x = w, so w is a fixed point. IF we set w<sub>1</sub>, w<sub>2</sub> as T's fixed points, we have ã…£ã…£w<sub>1</sub> - w<sub>2</sub>ã…£ã…£ = ã…£ã…£Tw<sub>1</sub> - Tw<sub>2</sub>ã…£ã…£ = â‹¯ = ã…£ã…£T<sup>n</sup>w<sub>1</sub> - Tnw<sub>2</sub>ã…£ã…£ = â‹¯ = 0, so w<sub>1</sub> and w<sub>2</sub> are same.

>> â—‹ **Intuition**

<br>

![img](https://github.com/user-attachments/assets/cd14d51f-adf9-495f-8f65-f99cd25aa826)

<br>

> â‘¡ Bellman operator and contraction theorem

>> â—‹ **Theorem**

>>> â—‹ Let F be a set of functions such tat F = {z: S â†’ â„}. Here, S = {1, 2, Â·Â·Â·, I} and z := (z(1), Â·Â·Â·, z(I))<sup>T</sup>. Let's define the following norm: ã…£ã…£zã…£ã…£ = max<sub>i</sub> ã…£z(i)ã…£ (i.e., ã…£ã…£Â·ã…£ã…£<sub>âˆ</sub>) Let's define the operator T: F â†’ F for each component. Then, for all i âˆˆ S, we have Tz(i) = min<sub>uâˆˆğ’°</sub> [C(i, u) + Î²âˆ‘<sub>jâˆˆS</sub> â„™(j ã…£ i, u) z(j)]. Then, T is a contraction mapping.

>> â—‹ **Proof**

>>> â—‹ Let â„<sup>I</sup> be a Banach space with a norm of ã…£ã…£Â·ã…£ã…£<sub>âˆ</sub>. If z, y âˆˆ F, then we can prove the theorem as follows:

<br>

<img width="606" height="487" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-04 á„‹á…©á„’á…® 5 30 19" src="https://github.com/user-attachments/assets/12aaf12c-d0c4-4dd1-a708-635e18ef0ddc" />

<br>

>>> â—‹ Even if we replace it with a peculiar definition that maximizes the cost, the contraction theorem still holds with the same proof structure.

> â‘¢ Corollary 

>> â—‹ âˆ€i âˆˆ S, W<sub>âˆ</sub>(i) = min<sub>uâˆˆğ’°</sub> C(i, u) + Î²âˆ‘<sub>jâˆˆS</sub> â„™(j ã…£ i, u) W<sub>âˆ</sub>(i) has a unique solution of w<sub>âˆ</sub>.

>> â—‹ Relationship between DCOE and geometric distribution

<br>

<img width="497" height="226" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-21 á„‹á…©á„’á…® 5 20 09" src="https://github.com/user-attachments/assets/d5185470-34f0-417d-99c3-2b4faa9baaa8" />

<br>

>> â—‹ W<sub>âˆ</sub>(i) = inf<sub>gâˆˆğ’¢</sub> J<sup>g</sup>(i) = inf<sub>gâˆˆğ’¢</sub> ğ”¼<sup>g</sup>[âˆ‘<sub>t=0 to âˆ</sub> Î²<sup>t</sup>c(x<sub>t</sub>, u<sub>t</sub>) ã…£ X<sub>0</sub> = i], âˆ€i âˆˆ S

>>> â—‹ Definition of J<sup>g</sup>(i) and bound 

<br>

<img width="298" height="91" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-09 á„‹á…©á„’á…® 5 28 13" src="https://github.com/user-attachments/assets/5f1a91e4-8ad7-474d-aa86-d227083b5838" />

<br>

>>> â—‹ Definition of W<sub>âˆ</sub>: By the property of contraction mapping, the decreasing sequence {W<sub>n</sub>} converges to W<sub>âˆ</sub>.

<br>

<img width="333" height="100" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-09 á„‹á…©á„’á…® 5 28 41" src="https://github.com/user-attachments/assets/612c73f4-836a-47fc-a18f-576d482f0d55" />

<br>

>>> â—‹ J<sup>g</sup>(i) â‰¥ W<sub>âˆ</sub> (lower bound)

<br>

<img width="501" height="97" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-09 á„‹á…©á„’á…® 5 29 15" src="https://github.com/user-attachments/assets/5a661967-caa6-4ca2-80e1-7d776684df7c" />

<br>

>>> â—‹ W<sub>âˆ</sub>(i) â‰¥ inf<sub>g</sub> J<sup>g</sup>(i) (upper bound): it can be represented as J(X<sub>0</sub>, Ï€*) â‰¤ âˆ‘<sub>Ï„=0 to t-1</sub> Î²<sup>Ï„</sup> C<sub>Ï„</sub>(X<sub>Ï„</sub>, Ï€<sub>Ï„</sub>\*) + Î²<sup>t</sup>ğ”¼[V<sub>t</sub>(X<sub>t</sub>, Ï€<sub>t</sub>)].

<br>

<img width="503" height="177" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-09 á„‹á…©á„’á…® 5 29 56" src="https://github.com/user-attachments/assets/e878167a-af89-40bf-9aa2-e09b59bf6a7d" />

<br>

>>> â—‹ **Conclusion:** W<sub>âˆ</sub>(i) = inf<sub>g</sub> J<sup>g</sup>(i) 

>> â—‹ The optimal stationary Markov policy g*(i) âˆˆ argmin<sub>uâˆˆğ’¢</sub> c(i, u) + Î²âˆ‘<sub>jâˆˆS</sub> â„™(j ã…£ i, u)V<sub>âˆ</sub>(j) 

>> <span style="color: white;">â—‹</span> â‡” W<sub>âˆ</sub> = c<sup>g\*</sup> + Î²P<sup>g\*</sup>w<sub>âˆ</sub>

>> <span style="color: white;">â—‹</span> â‡” W<sub>âˆ</sub> = (I - Î²P<sup>g\*</sup>)<sup>-1</sup> c<sup>g\*</sup>

>> â—‹ Optimal operator T is defined as follows:

<br>

<img width="502" height="72" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-21 á„‹á…©á„’á…® 5 24 14" src="https://github.com/user-attachments/assets/153d29a0-d827-4360-9938-154be5077b7b" />

<br>

>>> â—‹ By definition, TZ â‰¤ T<sup>g</sup>Z 

>>> â—‹ T<sup>g</sup> and T are both contraction mapping on â„“<sub>âˆ</sub> norm.

>>> â—‹ T<sup>g</sup> and T both satisfy the monotonicity: if z â‰¤ y, T<sup>g</sup>z â‰¤ T<sup>g</sup>y and Tz â‰¤ Ty

>>>> â—‹ Proof 1

<br>

<img width="600" height="114" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-21 á„‹á…©á„’á…® 9 02 06" src="https://github.com/user-attachments/assets/38e68738-71ec-49e2-ae0e-9f29fcca06cc" />

<br>

>>>> â—‹ Proof 2

<br>

<img width="184" height="154" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-21 á„‹á…©á„’á…® 5 26 27" src="https://github.com/user-attachments/assets/2c6a0a1f-3840-4702-a918-d1a0f88cd57d" />

<br>

>> â—‹ If h, g âˆˆ ğ’¢<sub>SMP</sub>, and T<sub>h</sub>W<sub>âˆ</sub><sup>g</sup> â‰¤ W<sub>âˆ</sub><sup>g</sup>, W<sub>âˆ</sub><sup>h</sup> â‰¤ W<sub>âˆ</sub><sup>g</sup> is established: we can obtain T<sub>h</sub><sup>n</sup>W<sub>âˆ</sub><sup>g</sup> â‰¤ â‹¯ â‰¤ T<sub>h</sub>W<sub>âˆ</sub><sup>g</sup> â‰¤ W<sub>âˆ</sub><sup>g</sup>; then, we can make n to infinite to obtain the conclusion.

> â‘£ **Algorithm 1.** Value iteration

>> â—‹ A method that updates the value function by repeatedly applying the Bellman optimality operator T as V<sub>k+1</sub> = TV<sub>k</sub>.

>> â—‹ Because of the contraction mapping property, if Î² < 1 then V<sub>k</sub> â†’ V<sup>\*</sup> converges to the unique fixed point, and the greedy policy with respect to V<sup>\*</sup> is optimal.

>> â—‹ A typical stopping criterion is  ã…£ã…£V<sub>k+1</sub> - V<sub>k</sub>ã…£ã…£<sub>âˆ</sub> < Îµ , etc.

>> â—‹ **Characteristic:** It has the advantage of simple computation, but the disadvantage of requiring many iterations.

> â‘¤ **Algorithm 2.** Policy iteration

>> â—‹ **Step 1.** Choose any stationary Markov policy g<sub>n</sub> âˆˆ ğ’¢<sub>SMP</sub>.

>> â—‹ **Step 2.** **Policy evaluation:** compute W<sub>âˆ</sub><sup>g<sub>n</sub></sup> = (I - Î²P<sup>g<sub>n</sub></sup>)<sup>-1</sup>C<sup>g<sub>n</sub></sup>.

>> â—‹ **Step 3.** **Stopping criterion:** if TW<sub>âˆ</sub><sup>g<sub>n</sub></sup> = W<sub>âˆ</sub><sup>g<sub>n</sub></sup>, stop and take g<sub>n</sub> as the optimal policy; otherwise go to **Step 4**.

>> â—‹ **Step 4.** **Policy improvement:** define g<sub>n+1</sub> as follows.

<br>

<img width="499" height="123" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-21 á„‹á…©á„’á…® 5 40 20" src="https://github.com/user-attachments/assets/2446a9c6-11b7-45fa-987a-c71076b97fd1" />

<br>

>> â—‹ **Theorem:** The sequence ({g<sub>0</sub>, g<sub>1</sub>, g<sub>2</sub>, Â·Â·Â·}) reaches an optimal policy after finitely many iterations.

>> â—‹ **Proof:** When the stopping condition W<sub>âˆ</sub><sup>g<sub>n</sub></sup> = TW<sub>âˆ</sub><sup>g<sub>n</sub></sup> holds, g<sub>n</sub> is already an optimal policy. Otherwise, in **Step 4** we have T<sub>g<sub>n+1</sub></sub> W<sub>âˆ</sub><sup>g<sub>n</sub></sup> â‰¤ W<sub>âˆ</sub><sup>g<sub>n</sub></sup>, and there is at least one state where the inequality is strict. Since the number of possible policies is finite (ã…£Uã…£<sup>ã…£Sã…£</sup>) and at each step the cost is strictly improved (in at least one state), the same policy cannot be revisited. Therefore, the algorithm reaches the stopping condition in finitely many steps and yields an optimal policy.

>> â—‹ **Characteristic:** It has the advantage of needing far fewer iterations, but at each iteration it must alternate between two operators (policy evaluation (more expensive due to inverse matrix calculation) and policy improvement).

> â‘¥ **Algorithm 3.** Linear programming

>> â—‹ **Theorem**

<br>

<img width="448" height="202" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-21 á„‹á…©á„’á…® 5 45 43" src="https://github.com/user-attachments/assets/9e07e6b0-5312-4b17-b760-ca81558303c2" />

<br>

>> â—‹ **Proof**

<br>

<img width="561" height="699" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-21 á„‹á…©á„’á…® 5 46 25" src="https://github.com/user-attachments/assets/982082c8-85d8-42f5-a2c9-0574dc55f9af" />

>> â—‹ **Implementation:** [Lagrange multiplier method](https://jb243.github.io/pages/1813), dual optimal variables.

<br>

<img width="564" height="728" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-21 á„‹á…©á„’á…® 5 47 24" src="https://github.com/user-attachments/assets/8a34fc47-040a-4f01-bb4b-aebdc8df282a" />

<br>

â’‚ **Lemma 15.** (`Stochastic policy`) ACOE (average cost optimality equation, infinite-horizon average cost Bellman equation)

> â‘  Overview

>> â—‹ For a finite state space, finite action space, and bounded cost function, define J<sup>g</sup> as follows:

<br>

<img width="304" height="66" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-21 á„‹á…©á„’á…® 5 50 30" src="https://github.com/user-attachments/assets/ff545fcc-5a6b-4415-b3f3-3859a2c71111" />

<br>

>> â—‹ In an irreducible Markov chain, consider the Poisson equation  J<sup>g</sup>**1** + W<sup>g</sup> = C<sup>g</sup> + P<sup>g</sup>W<sup>g</sup>.

>> â—‹ If W<sup>g</sup> is a solution, then W<sup>g</sup> + Î±**1** is also a solution for any constant Î±, but W<sup>g</sup> is uniquely determined up to this additivity constant.

>> â—‹ **Derivation of the ACOE:** the relative value function W<sub>N</sub>(i) - W<sub>N</sub>(j) converges, as N â†’ âˆ, corresponding to W(i) - W(j).

<br>

<img width="498" height="318" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-21 á„‹á…©á„’á…® 5 52 51" src="https://github.com/user-attachments/assets/04e19dea-8f7c-4442-81eb-e0ca4eb0f0e9" />

<br>

>> â—‹ **Significance 1.** J\* is the optimal cost.

>> â—‹ **Significance 2.** The optimal SMP g\* must satisfy the ACOE.

>>> â—‹ Suppose the optimal policy g\* is given as follows, and assume that the equality does not hold for some state i.

<br>

<img width="642" height="66" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-21 á„‹á…©á„’á…® 5 56 46" src="https://github.com/user-attachments/assets/20ad9d71-21fe-40ad-a1af-c075aff355ca" />

<br>

>>> â—‹ Then a contradiction arises because g\* loses optimality, and hence the optimal g\* must satisfy the ACOE.

<br>

<img width="350" height="174" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-21 á„‹á…©á„’á…® 5 57 32" src="https://github.com/user-attachments/assets/03c7334a-4123-4c9e-926c-cc4feeefbfd3" />

<br>

>> â—‹ **Significance 3.** We can use policy iteration.

> â‘¡ **Algorithm 1.** Policy iteration algorithm

>> â—‹ **Step 1.** Choose an arbitrary policy g<sub>0</sub> âˆˆ ğ’¢<sub>SMP</sub>.

>> â—‹ **Step 2.** **Policy evaluation:** Given g<sub>n</sub>, solve the following Poisson equation to obtain (J<sup>g<sub>n</sub></sup>, W<sup>g<sub>n</sub></sup>). Since we have I equations but (I+1) unknowns (J<sup>g<sub>n</sub></sup>, W<sup>g<sub>n</sub></sup>(1), Â·Â·Â·, W<sup>g<sub>n</sub></sup>(I)), we fix one component, e.g. set W<sup>g<sub>n</sub></sup>(I) = 0), to uniquely determine the solution of J<sup>gn</sup>**1** + W<sup>g<sub>n</sub></sup> = C<sup>g<sub>n</sub></sup> + P<sup>g<sub>n</sub></sup>W<sup>g<sub>n</sub></sup>

>> â—‹ **Step 3.** **Stopping criterion:** If g<sub>n</sub> satisfies the ACOE, then g<sub>n</sub> is an optimal SMP. Otherwise, go to **Step 4**. ACOE: J<sup>g<sub>n</sub></sup> + W<sup>g<sub>n</sub></sup>(i) = min<sub>uâˆˆğ’°</sub> {C(i, u) + âˆ‘<sub>jâˆˆS</sub> P(j ã…£ i, u)W<sup>g<sub>n</sub></sup>(j)}, âˆ€i

>> â—‹ **Step 4.** **Policy improvement**: Define a new policy g<sub>n+1</sub> by g<sub>n+1</sub> âˆˆ arg min<sub>uâˆˆğ’°</sub> {C(i, u) + âˆ‘<sub>jâˆˆS</sub> P(j ã…£ i, u)W<sup>g<sub>n</sub></sup>(j)} and return to **Step 2**.

>> â—‹ If g<sub>n</sub> does not satisfy the stopping criterion, then J<sup>g<sub>n+1</sub></sup> < J<sup>g<sub>n</sub></sup> holds.

<br>

<img width="403" height="198" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-21 á„‹á…©á„’á…® 6 05 31" src="https://github.com/user-attachments/assets/d6183813-26a0-4c04-bd7f-9f05fa61eb85" />

<br>

> â‘¢ **Algorithm 2.** ACOE and relative value iteration

>> â—‹ Assumption: For every g âˆˆ ğ’¢<sub>SMP</sub>, the transition matrix P<sup>g</sup> is irreducible and aperiodic.

>> â—‹ **Step 1.** Choose an arbitrary h<sub>0</sub> âˆˆ â„<sup>I</sup>.

>> â—‹ **Step 2.** For any k â‰¥ 1,

>>> â—‹ âˆ€i âˆˆ S, Î»<sup>k</sup>(i) = min<sub>uâˆˆğ’°<sub> {C(i, u) + âˆ‘<sub>jâˆˆS</sub> P(j ã…£ i, u)h<sup>k-1</sup>(j)}

>>> â—‹ Î¼<sup>k</sup> = Î»<sup>k</sup>(I) for some fixed reference state I

>>> â—‹ âˆ€i âˆˆ S, h<sup>k</sup>(i) = Î»<sup>k</sup>(i) - Î¼<sup>k</sup>

>> â—‹ **Step 3.** Check convergence; if it does not converge, return to **Step 2**.

>> â—‹ Then Î¼<sup>k</sup> converges to J\*, and h<sup>k</sup> converges to W (the relative value function).

> â‘£ Other algorithms

>> â—‹ Linear programming: If there is no max condition, J* can go down to âˆ’âˆ due to the â‰¤ inequality (which is a stronger condition than taking the minimum).

<br>

<img width="458" height="761" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-22 á„‹á…©á„’á…® 12 08 56" src="https://github.com/user-attachments/assets/aed80b8b-c7c2-433b-a5ef-b46700568255" />

<br>

>> â—‹ Successive approximation

<br>

<img width="561" height="691" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-21 á„‹á…©á„’á…® 6 10 32" src="https://github.com/user-attachments/assets/ec24ca2c-0769-43a7-8daa-739b44f30f22" />

<br>

>> â—‹ Bertsekasâ€™ algorithm

>> â—‹ Putermanâ€™s algorithm

> â‘¤ MDS (Martingale difference sequence)

>> â—‹ **Overview:** For concrete sample paths (random variables), we want to prove that lim inf<sub>Nâ†’âˆ</sub> Ä´<sub>N</sub><sup>g</sup>(Ï‰) almost surely (a.s.) is as follows for every g, and that lim<sub>Nâ†’âˆ</sub> Ä´<sub>N</sub><sup>g\*</sup>(Ï‰) = J\* a.s. (assuming g\* is optimal)

<br>

<img width="254" height="58" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-21 á„‹á…©á„’á…® 6 13 35" src="https://github.com/user-attachments/assets/d59404d1-d179-4496-8080-403e102a3d3d" />

<br>

>> â—‹ **Theorem 1.** Let {X<sub>k</sub>}<sub>kâˆˆâ„•</sub> be adapted to a filtration {â„±<sub>k</sub>}<sub>kâˆˆâ„•</sub> and satisfy ğ”¼[X<sub>k+1</sub> ã…£ â„±<sub>k</sub>] = 0 almost surely. Then X<sub>k</sub> and Y<sub>k</sub> = âˆ‘<sub>j=1 to k</sub> X<sub>j</sub> are â„±<sub>k</sub>-measurable.

>> â—‹ **Theorem 2.** Martingale stability theorem (LLN)

<br>

<img width="554" height="127" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-21 á„‹á…©á„’á…® 6 16 05" src="https://github.com/user-attachments/assets/d4cb53b9-e7e4-4798-ba58-1dceb9f711de" />

<br>

>> â—‹ **Theorem 3.** For any g âˆˆ ğ’¢, lim<sub>Nâ†’âˆ</sub> inf (1/N) âˆ‘<sub>t=0 to N-1</sub> C(X<sub>t</sub><sup>g</sup>, U<sub>t</sub><sup>g</sup>) â‰¥ J\* a.s. is established, and if equality holds, then g\* satisfies the ACOE.

>>> â—‹ Let (J\*, W) be a solution of the ACOE. Define Z<sub>k+1</sub> = C(X<sub>k</sub>, U<sub>k</sub>) - J\* + W(X<sub>k+1</sub>) - W(X<sub>k</sub>) - h(X<sub>k</sub>, U<sub>k</sub>) and h(i, u) = C(i, u) + âˆ‘<sub>jâˆˆS</sub> P(j ã…£ i, u)W(j) - J\* - W(i) â‰¥ 0. Let â„±<sub>k</sub> = Ïƒ(X<sub>0</sub>, X<sub>1</sub><sup>g</sup>, Â·Â·Â·, X<sub>k</sub><sup>g</sup>). Then, given U<sub>k</sub><sup>g</sup> = g<sub>k</sub>(X<sub>0</sub>, X<sub>1</sub><sup>g</sup>, Â·Â·Â·, X<sub>k</sub><sup>g</sup>), we obtain ğ”¼[Z<sub>k+1</sub> ã…£ â„±<sub>k</sub>] = 0 as follows:

<br>

<img width="600" height="307" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-21 á„‹á…©á„’á…® 6 21 44" src="https://github.com/user-attachments/assets/6fba18d9-3f50-471d-81f2-81f1f8727516" />

<br>

>>> â—‹ Hence {Z<sub>k</sub>}<sub>kâˆˆâ„•</sub> is an MDS, and each term of Z<sub>k+1</sub> is bounded, so Z<sub>k+1</sub> is bounded with respect to MÌƒ. Therefore,

<br>

<img width="461" height="68" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-21 á„‹á…©á„’á…® 6 22 51" src="https://github.com/user-attachments/assets/62b938d4-8088-4d38-a779-beba2bb3ce27" />

<br>
  
>>> â—‹ holds, and by the LLN we have lim<sub>Nâ†’âˆ</sub> (1/N)âˆ‘<sub>k=1 to N</sub> Z<sub>k</sub> = 0 a.s. But

<br>

<img width="596" height="118" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-21 á„‹á…©á„’á…® 6 23 51" src="https://github.com/user-attachments/assets/4e674640-8235-4e5f-a9d3-2ba385de22b1" />

<br>

>>> â—‹ holds and h(Â·, Â·) â‰¥ 0, so we have lim<sub>Nâ†’âˆ</sub> inf (1/N) âˆ‘<sub>t=0 to N-1</sub> C(X<sub>t</sub><sup>g</sup>, U<sub>t</sub><sup>g</sup>) â‰¥ J\* a.s.

> â‘¥ **Theorem:** The DCOE problem becomes equivalent to the ACOE problem as Î² â†’ 1.

<br>

<img width="601" height="474" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-21 á„‹á…©á„’á…® 6 25 12" src="https://github.com/user-attachments/assets/5e137370-8b7a-4e72-956b-989b5a73733f" />

<br>

>> â—‹ Reason why ã…£W<sub>Î²</sub>(i) - W<sub>Î²</sub>(j)ã…£ < âˆ in the above proof.

>>> â—‹ Theorem

<br>

<img width="700" height="152" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-21 á„‹á…©á„’á…® 6 26 04" src="https://github.com/user-attachments/assets/f5bc8fb2-f89e-410a-b122-8b49144e06a5" />

<br>

>>> â—‹ Upper bound

<br>

<img width="639" height="577" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-21 á„‹á…©á„’á…® 6 26 28" src="https://github.com/user-attachments/assets/3b470b09-1147-428e-9d61-093b9cc3a59f" />

<br>

>>> â—‹ Lower bound

<br>

<img width="450" height="375" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-21 á„‹á…©á„’á…® 6 26 55" src="https://github.com/user-attachments/assets/f28b6d8e-b14e-4f36-8dc5-4d17706bb453" />

<br>

>>> â—‹ General proof

<br>

<img width="646" height="534" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-24 á„‹á…©á„Œá…¥á†« 10 37 23" src="https://github.com/user-attachments/assets/ed27ac5f-517e-42e3-bbc7-c6eef2f6f9e8" />

<br>

>> â—‹ **Significance 1.** It is not necessarily true that the optimal policy g<sub>Î²</sub>\* converges to the optimal ACOE policy g\*.

>> â—‹ **Significance 2.** Blackwell optimality: If the same policy g<sub>Î²</sub>* is optimal for all Î² with Î²Ì‚ < Î² < 1, then g<sub>Î²</sub>\* is also optimal for the ACOE. Then, J<sub>Î²</sub>* = ACOE optimal J* also holds.

>>> â—‹ **Proof 1.**

<br>

<img width="502" height="157" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-22 á„‹á…©á„’á…® 12 40 24" src="https://github.com/user-attachments/assets/ac067165-d9f6-4c09-81de-4e72d6c6c6a2" />

<br>

>>> â—‹ **Proof 2.** In the classical Blackwell argument, you compare two stationary policies Ï€ and Î½ by looking at their value difference as a function of the discount factor, f<sub>Ï€,Î½</sub>(Î³) = V<sub>Î³</sub><sup>Ï€</sup> âˆ’ V<sub>Î³</sub><sup>Î½</sup>. In a standard finite MDP this difference is a rational function of Î³, and a nonzero rational function can have only finitely many zeros. That means there are only finitely many discount factors Î³ at which the two policies tie; beyond those points, their ranking cannot keep flipping infinitely often as Î³ â†’ 1. Hence, for all Î³ sufficiently close to 1, the same policy remains optimal, and this policy is also optimal for the average-reward criterion; this is the Blackwell-optimal policy.

â’ƒ **Lemma 16.** Kalman filter

> â‘  Overview

>> â—‹ **Case 1.** Pure prediction problem (U<sub>t</sub> â‰¡ 0): Kalman filter. Given Y<sub>0</sub>, Â·Â·Â·, Y<sub>t</sub>, the problem is to predict X<sub>0</sub>, Â·Â·Â·, X<sub>t</sub>.

>> â—‹ **Case 2.** Pure control problem (Y<sub>t</sub> = X<sub>t</sub>): LQR (linear quadratic regulator)

>> â—‹ **Case 3.** Partial observation with quadratic cost: LQG (linear quadratic Gaussian).

> â‘¡ Review: linear Gaussian process

<br>

<img width="428" height="532" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-21 á„‹á…©á„’á…® 6 30 06" src="https://github.com/user-attachments/assets/fc9ff37b-1131-4026-8e3d-51482a28b7e2" />

<br>

> â‘¢ **Case 1.** u<sub>t</sub> â‰¡ 0 or B<sub>t</sub> = 0

>> â—‹ **Step 1.** Prediction

<br>

<img width="650" height="313" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-21 á„‹á…©á„’á…® 6 30 32" src="https://github.com/user-attachments/assets/b3c106db-0895-4cbe-89d6-5e7dec56bc39" />

<br>

>> â—‹ **Step 2.** Observation prediction

<br>

<img width="650" height="530" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-21 á„‹á…©á„’á…® 6 30 56" src="https://github.com/user-attachments/assets/40e0ffda-6532-42c4-9ece-80035ffd86ac" />

<br>

>> â—‹ **Step 3.** Data update

<br>

<img width="651" height="623" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-21 á„‹á…©á„’á…® 6 31 29" src="https://github.com/user-attachments/assets/d3497d68-d732-4c9b-a61f-ece713590c63" />

<br>

>> â—‹ **Significance:** From the form of L<sub>t</sub>, we see that the filter is deterministic and nonlinear.

>> â—‹ **Application:** Asymptotic behavior of the Kalman filter

>>> â—‹ **Definition:** In the time-invariant case A<sub>t</sub> â‰¡ A, G<sub>t</sub> â‰¡ G, C<sub>t</sub> â‰¡ C, H<sub>t</sub> â‰¡ H.

>>> â—‹ **Background theory:** _The reason observability appears when discussing the asymptotic behavior of the Kalman filter is that, in order to construct a long-term stable observer, the system must be at least observable or, more generally, satisfy observability. If an unobservable mode exists, the state component in that direction cannot be corrected using measurements. In particular, if such a mode is unstable (i.e., its eigenvalue lies outside the unit circle), then its contribution cannot be inferred from the outputs, and the estimation error in that direction grows without bound over time. Consequently, the error covariance P<sub>k</sub> also diverges along that direction, so the Riccati recursion does not converge to a finite limit P<sub>âˆ</sub>, and the observer error cannot be stabilized. Therefore, to guarantee good asymptotic behavior of the Kalman filterâ€”such as convergence of the error covariance, a constant steady-state Kalman gain, and stable estimationâ€”it is essential that all unstable modes be observable, that is, that the pair (A,C) be observability._

>>> â—‹ â€œObservableâ€, "detectabile" and â€œreachableâ€ have the same meaning.

<br>

<img width="600" height="227" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-21 á„‹á…©á„’á…® 6 33 18" src="https://github.com/user-attachments/assets/c253bccc-a926-4bc3-95dc-74b4ddb7dc05" />

<br>

>>> â—‹ ARE(algebraic Riccati equation) 

<br>

<img width="651" height="289" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-21 á„‹á…©á„’á…® 6 33 43" src="https://github.com/user-attachments/assets/7d108251-10db-4642-a640-f39a016c2612" />

<br>

>>> â—‹ If (A,S) is reachable, then the following are equivalent: A is stable. â‡” The equation âˆ‘ = Aâˆ‘A\* + SS\* has a positive definite solution âˆ‘. â‡” Corollary

<br>

<img width="651" height="45" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-21 á„‹á…©á„’á…® 6 34 29" src="https://github.com/user-attachments/assets/69358896-9ae2-46fd-bb12-424179f3e57d" />

<br>

> â‘£ **Case 2.** Arbitrary u<sub>t</sub> = g<sub>t</sub>(H<sub>t</sub>) = g<sub>t</sub>(y<sub>0:t</sub>, u<sub>0:t-1</sub>), and arbitrary C<sub>t</sub>(x<sub>t</sub>, u<sub>t</sub>)

<br>

<img width="301" height="216" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-27 á„‹á…©á„’á…® 4 12 13" src="https://github.com/user-attachments/assets/8bf9d5f8-c51c-4228-8ce2-4bde37645c59" />

<br>

>> â—‹ **Result 1.** (y<sub>0:t</sub><sup>g</sup>, u<sub>0:t-1</sub><sup>g</sup>) and y<sub>0:t</sub> are the same Ïƒ-algebra: each is the function of each other

>>> â—‹ <b>(y<sub>0:t</sub><sup>g</sup>, u<sub>0:t-1</sub><sup>g</sup>) â†’ y<sub>0:t</sub> proof:</b> u<sub>0</sub><sup>g</sup> = g<sub>0</sub>(y<sub>0</sub><sup>g</sup>) = g<sub>0</sub>(y<sub>0</sub> + È³<sub>0</sub><sup>g</sup>) and y<sub>0</sub> = y<sub>0</sub><sup>g</sup> - c<sub>0</sub>xÌ„<sub>0</sub><sup>g</sup> = y<sub>0</sub><sup>g</sup> - C<sub>0</sub>ğ”¼[x<sub>0</sub>]. xÌ„<sub>1</sub><sup>g</sup> = A<sub>0</sub>xÌ„<sub>0</sub><sup>g</sup> + B<sub>0</sub>u<sub>0</sub><sup>g</sup>, so È³<sub>1</sub><sup>g</sup> = C<sub>1</sub>xÌ„<sub>1</sub><sup>g</sup>, leading to y<sub>1</sub> = y<sub>1</sub><sup>g</sup> - È³<sub>1</sub><sup>g</sup>. So, we can construct y<sub>0:t</sub> in this way.

>>> â—‹ <b>y<sub>0:t</sub> â†’ (y<sub>0:t</sub><sup>g</sup>, u<sub>0:t-1</sub><sup>g</sup>) proof:</b> Let's think of x<sub>0</sub> â†’ y<sub>0</sub> â†’ u<sub>0</sub> â†’ x<sub>1</sub> â†’ y<sub>1</sub> â†’ u<sub>1</sub> â†’ x<sub>2</sub> â†’ â‹¯. È³<sub>0</sub><sup>g</sup> = c<sub>0</sub>xÌ„<sub>0</sub><sup>g</sup> is known. y<sub>0</sub><sup>g</sup> = y<sub>0</sub> + È³<sub>0</sub><sup>g</sup> is known, and u<sub>0</sub><sup>g</sup> = g<sub>0</sub>(y<sub>0</sub><sup>g</sup>) is also known. Then, xÌ„<sub>1</sub><sup>g</sup> = A<sub>0</sub>xÌ„<sub>0</sub><sup>g</sup> + B<sub>0</sub>u<sub>0</sub><sup>g</sup> is known, and also we know È³<sub>1</sub><sup>g</sup> = c<sub>1</sub>xÌ„<sub>1</sub><sup>g</sup> and y<sub>1</sub><sup>g</sup> = y<sub>1</sub> + È³<sub>1</sub><sup>g</sup>. Thereforw, u<sub>1</sub><sup>g</sup> = g<sub>1</sub>(y<sub>0:1</sub><sup>g</sup>, y<sub>0</sub><sup>g</sup>) is known. Thus, the proposition can be proved in this way.

>>> â—‹ This is related to the former proposition: P<sup>g</sup>(X<sub>t+1</sub> âˆˆ A ã…£ H<sub>t</sub>, u<sub>t</sub>) = P<sup>g</sup>(X<sub>t+1</sub> âˆˆ A ã…£ y<sub>0:t</sub>, u<sub>t</sub>).

>> â—‹ **Result 2.** Ï€<sub>t</sub> = â„™(x<sub>t</sub><sup>g</sup> ã…£ y<sub>0:t</sub><sup>g</sup>, u<sub>0:t-1</sub><sup>g</sup>) = â„™(x<sub>t</sub><sup>g</sup> ã…£ y<sub>0:t</sub>) 

>> â—‹ **Result 3.** â„™(x<sub>t</sub> ã…£ y<sub>0:t</sub>) (= Kalman filter) is enought to understand the system.

>> â—‹ **Remark 1.** Control only affects xÌ„<sub>t</sub><sup>g</sup> (mean), but not for âˆ‘<sub>tã…£t</sub> (variance).

>> â—‹ **Remark 2.** It does not require learning(active exploration) to decrease the variance, unlike general POMDP.

>> â—‹ **Remark 3.** Separation principle: Kalman filter and control can be divided.

> â‘¤ Dynamic programming

<br>

<img width="600" height="599" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-27 á„‹á…©á„’á…® 4 24 42" src="https://github.com/user-attachments/assets/f8840160-ee66-4ed9-99e9-3c3b56142666" />

<br>

> â‘¥ Quadratic cost

>> â—‹ Assumption: C<sub>t</sub>(x<sub>t</sub>, u<sub>t</sub>) = x<sub>t</sub>\*P<sub>t</sub>x<sub>t</sub> + u<sub>t</sub>\*T<sub>t</sub>u<sub>t</sub>, C<sub>T</sub>(x<sub>T</sub>) = x<sub>T</sub>\*P<sub>T</sub>x<sub>T</sub> 

>> â—‹ Let X ~ ğ’©(XÌ„, âˆ‘) and S be a symmetric matrix, then we have ğ”¼[X\*SX] = XÌ„\*SXÌ„ + Tr(Sâˆ‘).

<br>

<img width="300" height="319" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-27 á„‹á…©á„’á…® 4 26 42" src="https://github.com/user-attachments/assets/b5437823-a881-41db-9ae6-0463a6ad2a64" />

<br>

>> â—‹ Solution for LQG problem

<br>

<img width="655" height="300" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-27 á„‹á…©á„’á…® 4 27 21" src="https://github.com/user-attachments/assets/93a3f049-c427-48e0-bb3b-4a7f312fd443" />

<br>

>> â—‹ Proof

<br>

<img width="553" height="445" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-27 á„‹á…©á„’á…® 4 27 41" src="https://github.com/user-attachments/assets/b790e530-bb4f-47da-9ef9-bf0515b6be9c" />

<br>

>> â—‹ **Remark 1.** Certainty equivalent control: Noise terms {w<sub>t</sub>} and {v<sub>t</sub>} don't apprear in the optimal control policy.

>> â—‹ **Remark 2.** Separation principle: Forward estimation through Kalman filter, and backward computation for control action by solving LQR problem.

>> â—‹ **Remark 3.** If âˆ‘<sub>tã…£t</sub> â‰¡ 0, s<sub>t</sub> â‰¡ 0.

> â‘¦ Quadratic cost and ACOE

<br>

<img width="656" height="227" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-27 á„‹á…©á„’á…® 4 29 19" src="https://github.com/user-attachments/assets/67ed8e49-13db-42fc-9752-75da2d3169c2" />

<br>

<br>

â’„ **Lemma 17.** **MAB**(multi-armed bandit)

<br>

<img width="493" height="99" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-12-30 á„‹á…©á„’á…® 9 33 05" src="https://github.com/user-attachments/assets/9a9e85a7-944a-4875-8345-5abf86e04069" />

**Figure 6.** MAB

<br>

> â‘  Formula

>> â—‹ Premise: N stochastic processes (X<sub>k</sub><sup>n</sup>)<sub>k=0,1,Â·Â·Â·; n=1,2,Â·Â·Â·</sub>

>> â—‹ State set: S = {1, 2, Â·Â·Â·, I} or countably infinite set

>> â—‹ Action set: u<sub>k</sub> âˆˆ {1, 2, Â·Â·Â·, N}

>> â—‹ Transition rule

<br>

<img width="471" height="144" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-12-26 á„‹á…©á„’á…® 5 12 59" src="https://github.com/user-attachments/assets/a6e0e76a-24ce-490e-a3b8-65266b15a0a7" />

<br>

>> â—‹ **Objective:** We want an optimal policy such that sup<sub>gâˆˆğ’¢</sub> ğ”¼<sup>g</sup>[âˆ‘<sub>k=0 to âˆ</sub> Î²<sup>k</sup>R(x<sub>k</sub><sup>u<sub>k</sub></sup>)].

>> â—‹ **Dynamic programming:** W(x<sup>1</sup>, x<sup>2</sup>, Â·Â·Â·, x<sup>N</sup>), x<sup>i</sup> âˆˆ S(s<sup>N</sup>) = max<sub>uâˆˆ{1, 2, Â·Â·Â·, N}</sub> R(x<sup>u</sup>) + Î²âˆ‘<sub>j=1 to I</sub> P(j ã…£ x<sup>u</sup>)W(x<sup>1</sup>, x<sup>2</sup>, Â·Â·Â·, x<sup>u-1</sup>, j, x<sup>u+1</sup>, Â·Â·Â·, x<sup>N</sup>) 

> â‘¡ Idea

>> â—‹ To solve a multi-armed bandit (MAB) with dynamic programming, you have to treat the states of all arms as one huge state vector (x<sub>1</sub>, x<sub>2</sub>, â‹¯, x<sub>N</sub>).

>> â—‹ As the number of arms (N) increases, the state space grows exponentially, making computation infeasible.

>> â—‹ **Key idea:** instead of considering all arms jointly, analyze each arm separately, assign it a score (an index), and then pick the arm with the highest score. â†’ This reduces the complex N-arm problem to a single-arm problem: stochastic arm vs. static arm (a â€œ1.5-armâ€ problem).

>> â—‹ **Setup:** In front of you, there is one slot machine (a stochastic arm). At any time, you can give up this machine and retire to receive a fixed reward (M) (or R*) forever each period (the static arm).

<br>

<img width="349" height="71" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-12-26 á„‹á…©á„’á…® 5 01 04" src="https://github.com/user-attachments/assets/83c82a22-bba0-44c1-965a-5ca66900d62f" />

<br>

>> â—‹ **Question:** When the stochastic machine is in state i, what value of the retirement reward M makes you exactly indifferent between â€œcontinuing to gambleâ€ and â€œretiring immediatelyâ€? â†’ That retirement reward level is the **Gittins index** at state (i).

> â‘¢ **Mathematical definition**

>> â—‹ Optimal stopping time (Ï„): You keep playing the machine, and you stop when the state becomes so unfavorable that you think, â€œItâ€™s better to retire and take M.â€

<br>

<img width="445" height="57" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-12-26 á„‹á…©á„’á…® 4 51 26" src="https://github.com/user-attachments/assets/41fad9af-2cb7-46ac-9e19-e38ad2bdf7a3" />

<br>

>> â—‹ Intuition: it is the total reward you can earn before stopping (before retirement), divided by the (discounted) time it took. In other words, it is the **maximum discounted average reward per unit time** you can extract before retiring.

> â‘£ **Proof of optimality:** interchange argument

>> â—‹ Let i be the arm with the highest current Gittins index (Î³), and j be the arm with the second highest.

>> â—‹ Assume (for contradiction) that a policy gÌƒ plays j first.

>> â—‹ By swapping the order (the interchange argument), we can change the policy to play i first and j later which is optimal policy g, and the total reward becomes greater than or equal. This is because arm i has a higher â€œaverage reward rate (index),â€ so under discounting (Î²), it is better to do the better option earlier when rewards are discounted less.

>> â—‹ Defining appropriate time intervals (e.g., Ï„<sub>i</sub>, Ï„<sub>j</sub>) requires some additional algebraic derivation.

<br>

<img width="599" height="330" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-12-26 á„‹á…©á„’á…® 5 28 38" src="https://github.com/user-attachments/assets/7a083d9f-15dd-4ff2-b277-f7342ce819a8" />

<br>

> â‘¤ **Example 1.** Suppose there are N slot machines, denoted by M<sub>1</sub>, ..., M<sub>N</sub>. The success probability of each slot machine M<sub>i</sub> is Î¸<sub>i</sub>, and the failure probability is 1 âˆ’ Î¸<sub>i</sub>. When we play once, we receive a reward of 1 on success and 0 on failure. The success probabilities Î¸<sub>1</sub>, ..., Î¸<sub>N</sub> are mutually independent random variables taking values in [0, 1], and their prior distributions are denoted by P<sub>1</sub>(dÎ¸<sub>1</sub>), ..., P<sub>N</sub>(dÎ¸<sub>N</sub>) (we assume these distributions admit densities). At each time step, we can choose and play only one of the N slot machines. Find an optimal policy that maximizes
E<sup>g</sup>[âˆ‘<sub>t=0 to âˆ</sub> Î²<sup>t</sup> r<sub>t</sub>].

<br>

<img width="360" height="350" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-28 á„‹á…©á„’á…® 12 40 47" src="https://github.com/user-attachments/assets/c8255892-ae8a-44ee-90f7-15b606ed9016" />

<br>

> â‘¥ **Example 2.** We consider a small network with J queues (nodes). At each node j âˆˆ {1, ..., J}, there are initially n<sub>j</sub> jobs (customers) waiting in line. The time required to serve a single customer at node j is random, and its distribution has cumulative distribution function (CDF) F<sub>j</sub>. When a customer completes service at node j, one of two things happens. With probability q<sub>jâ„“</sub>, the customer moves to another node â„“ and joins the queue there; with the remaining probability 1 âˆ’ âˆ‘<sub>â„“=1 to J</sub> q<sub>jâ„“</sub>, the customer leaves the system entirely. We assume that whenever a customer completes service at node j, we obtain a reward r<sub>j</sub> > 0 (for example, r<sub>j</sub> can be interpreted as the revenue the company earns from completing one job at node j). There is only one server (worker) in the system. Thus, at each moment we must decide â€œwhich nodeâ€™s queue should we serve next?â€ We assume that no new customers arrive from outside, and our goal is to maximize the total discounted reward (total discounted revenue) over time. Viewing this situation as a multi-armed bandit (MAB) problem, each node j corresponds to an arm. At each time step we choose one action from the set {1, 2, ..., J} and serve one customer from the corresponding nodeâ€™s queue. Where the customer goes next is determined by the probabilities q<sub>jâ„“</sub> for that node, and as a result the overall queueing state (how many customers remain at each node, etc.) evolves to the next state. The joint state of all nodes is the â€œsystem stateâ€ in the bandit problem. To make the system look more like a standard bandit model, we can add a fictitious (J+1)-th node that represents the destination of customers who leave the system, thereby making the network â€œclosed.â€ No actual server is assigned to this fictitious node, so it is never chosen as an action; it is just a device to represent the flow of customers leaving the system. In this way, a structure in which a single server chooses among several queues where to spend time, receives a reward whenever a service is completed, and sees the queueing state change accordingly is a canonical example of an MAB. If there are no arrivals at all, each queue shrinks only when we choose to serve it, so the problem is a relatively simple â€œrestedâ€ bandit (cf. the Gittins policy is optimal). However, if customers keep arriving from outside, then the queues change state even when they are not chosen, and the problem becomes an example of a â€œrestless banditâ€ (cf. the Gittins index policy is no longer optimal, and one must resort to concepts such as the Whittle index).

<br>

<br>

## **4. Advanced Topics**

â‘´ Decentralized Teams ([ref](https://arxiv.org/abs/1002.4172), [ref](https://arxiv.org/abs/1209.1695), [ref](https://web.eecs.umich.edu/~teneket/pubs/ASHUTOSH%2CASITYA%2CBOOK.pdf))

â‘µ [Robust MDPs](https://jb243.github.io/pages/1131) ([ref](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqa2x1SXJnOTAxMXM5Tnl3bTY2d3lub2Z2UE5EQXxBQ3Jtc0trSnhILTJab3V2ZENRcnJEaG1JMm9pOHlqeWhIN0tvb1Z1blF0MTJnWl9pWDNLSmxhSk4tWUtOWDBvT2VLZ2JHdTJQRWxYM1JPdWIxc3lab1lpemswdWQ1bmVTbmlQU3BsNGhvR2pPWU41OTJaUHB0Yw&q=https%3A%2F%2Fcdn.aaai.org%2FWorkshops%2F2004%2FWS-04-08%2FWS04-08-013.pdf&v=u5iEl2nlD2U), [ref](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbnlOc0Jkd0VZS1hLdVpzVE1WLWFyal9nYXRUUXxBQ3Jtc0ttU3VNLUpXWEYzQlNxTjZ3OTV5RnZiR0dmR2ZHZVc0UE13emZiY1ZoWTJFODNrV1Z4RGx6Wm9XbFJQcndXU1hjaXdWSnRCaDlNNU8yZ0hDMDAyV01UMjkzVXJHLVVOM3g4a1RMVkZsNnROU1A4TVRVaw&q=http%3A%2F%2Fwww.corc.ieor.columbia.edu%2Freports%2Ftechreports%2Ftr-2002-07.pdf&v=u5iEl2nlD2U))

â‘¶ Constrained MDPs ([ref](https://scispace.com/papers/denumerable-constrained-markov-decision-processes-and-finite-2jeu04xlpx), [ref](https://link.springer.com/article/10.1007/BF00353877))

â‘· Bandits ([ref](https://www.semanticscholar.org/paper/A-dynamic-allocation-index-for-the-discounted-Gittins-Jones/d0c564e32058cd8e5d0bf9455538b64d8a0e2df8), [ref](https://people.eecs.berkeley.edu/~russell/classes/cs294/s11/readings/Gittins:1979.pdf), [ref](https://academic.oup.com/jrsssb/article/42/2/143/7027598), [ref](https://www.sciencedirect.com/science/article/pii/0196885885900028), [ref](https://www.jstor.org/stable/2332286))

â‘¸ Restless Bandits ([ref](https://www.semanticscholar.org/paper/Restless-bandits%3A-activity-allocation-in-a-changing-Whittle/45196e90c3b265cbcd008af6e1aac97128e525dc), [ref](https://arxiv.org/abs/2306.00196))

â‘¹ Bandits and Adaptive Control ([ref](https://ui.adsabs.harvard.edu/abs/1987ITAC...32..968A/abstract), [ref](https://people.eecs.berkeley.edu/~ananth/1987-1989/Pravin/MarkovMultiarmedbandit.pdf), [ref](https://people.eecs.berkeley.edu/~ananth/1987-1989/Agrawal/AgrawalIID89.pdf))

â‘º Adaptive Control ([ref](https://www.semanticscholar.org/paper/Asymptotically-Efficient-Adaptive-Choice-of-Control-Graves-Lai/59ef34c2ecca183b7d0ff1788b49f2d5ca3e5ab9), [ref](https://www.semanticscholar.org/paper/Machine-learning-and-nonparametric-bandit-theory-Lai-Yakowitz/fe611ab14edf4c19fa90e03da42589b0e9c5d5ec))

â‘» System Identification (Linear) ([ref](https://par.nsf.gov/servlets/purl/10312038), [ref](https://www.semanticscholar.org/paper/Finite-Time-Identification-of-Linear-Systems%3A-and-Jedra-Prouti%C3%A8re/9826799d60be5539495bf6df29fcaa928313d94d))

â‘¼ System Identification (Controlled Markov Chains) ([ref](https://arxiv.org/abs/2509.05193), [ref](https://arxiv.org/abs/2208.08480))

â‘½ Policy gradient in Linear SDS ([ref](https://arxiv.org/abs/1801.05039), [ref](https://arxiv.org/abs/2505.01348))

<br>
 
---

_Input: 2025.08.26 23:34_
