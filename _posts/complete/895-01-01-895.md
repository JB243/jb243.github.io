## Stochastic Control Theory

Recommended post: ã€Control Theoryã€‘ [Table of Contents for Control Theory](https://jb243.github.io/pages/1909)

---

**1.** [Sigma-algebra](#1-sigma-algebra)

**2.** [Terminology of Stochastic Control Theory](#2-terminology-of-stochastic-control-theory)

**3.** [Laws of Stochastic Control Theory](#3-laws-of-stochastic-control-theory)

---

**a.** [Reinforcement Learning](https://jb243.github.io/pages/2162)

**b.** [The Power of Belief](https://jb243.github.io/pages/869)

---

<br>

## **1. Sigma-algebra**

â‘´ [Probability Space](https://jb243.github.io/pages/1623)

â‘µ [Sigma-algebra](https://jb243.github.io/pages/910)(Ïƒ-algebra)

<br>

<br>

## **2. Terminology of Stochastic Control Theory**

â‘´ Variable definitions

> â‘  **state** (system state) x<sub>t</sub>**:** denotes a specific value or a random variable; same below

> â‘¡ **observation** y<sub>t</sub>**:** in the perfect observation case, y<sub>t</sub> = x<sub>t</sub>

> â‘¢ **noise** (system noise), **disturbance**, **error**, **primitive random variable** w<sub>t</sub>, v<sub>t</sub>

> â‘£ **system state noise** w<sub>t</sub>

> â‘¤ **observation noise** v<sub>t</sub>

> â‘¥ primitive random seed creating stochastic uncertainty x<sub>0</sub>

> â‘¦ **control** u<sub>t</sub>

> â‘§ **control strategy / law / policy** g<sub>t</sub>

> â‘¨ **system state sequence** x<sub>t+1</sub> := f<sub>t</sub>(x<sub>t</sub>, u<sub>t</sub>, w<sub>t</sub>)

> â‘© **observation sequence** y<sub>t</sub> := h<sub>t</sub>(x<sub>t</sub>, v<sub>t</sub>)

> â‘ª **control input**, **action** u<sub>t</sub> := g<sub>t</sub>(y<sub>0:t</sub>, u<sub>0:t-1</sub>). Using all past information is called **perfect recall**.

â‘µ Classification by system sequence

> â‘  **DDS** (deterministic system)**:** x<sub>t+1</sub> := f<sub>t</sub>(x<sub>t</sub>, u<sub>t</sub>, w<sub>t</sub>) = f<sub>t</sub>(x<sub>t</sub>, u<sub>t</sub>). y<sub>t</sub> := h<sub>t</sub>(x<sub>t</sub>, v<sub>t</sub>) = h<sub>t</sub>(x<sub>t</sub>). Case where at any time t both the state variable xt and the output variable y<sub>t</sub> are known

> â‘¡ **SDS** (stochastic dynamical model): x<sub>t+1</sub> := f<sub>t</sub>(x<sub>t</sub>, u<sub>t</sub>, w<sub>t</sub>), y<sub>t+1</sub> := h<sub>t</sub>(x<sub>t</sub>, v<sub>t</sub>), w<sub>t</sub>, v<sub>t</sub> â‰¢ 0.

â‘¶ Classification by control input

> â‘  **open loop control**: u<sub>t</sub> := g<sub>t</sub>(y<sub>0:t</sub>, u<sub>0:t-1</sub>) = g<sub>t</sub>(u<sub>0:t-1</sub>).

> â‘¡ **feedback control**: cases where past outputs y<sub>0:t</sub> influence the control action u

> â‘¢ **centralized stochastic control :** **(1)** stochastic dynamical system + **(2)** one controller + **(3)** controller with perfect recall

> â‘£ **multi-controller problem**: team problem, competitive game, etc.

â‘· Classification by policy

> â‘  **decision process**: a general framework that deals with decision-making problems where state, action, and reward follow through a process

> â‘¡ **Markov process**: (regardless of whether it is a decision process) the future depends only on the current state

<br>

<img width="577" height="29" alt="image" src="https://github.com/user-attachments/assets/5ce57777-d8e9-4464-ad03-14c48e2c9e82" />

<br>

>> â—‹ **Markov chain**: among Markov processes, refers to those with a finite or countably infinite state space

>> â—‹ **controlled Markov chain**: Markov chain + decision process

<br>

<img width="330" height="26" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 2 11 59" src="https://github.com/user-attachments/assets/4433ae56-3122-497e-b289-8897944f23b9" />

<br>

> â‘¢ **MDP** (Markov decision process): among decision processes, cases where the future depends only on the current state

>> â—‹ **dynamic programming**: recurrence relation (break the time dependence). If MDP refers to the system framework, dynamic programming refers to the methodology.

>> â—‹ **POMDP** (partially observed Markov decision process): an MDP system where only partial information rather than full state information can be used

>> â—‹ **constrained MDP**, **constrained POMDP** also exist

>> â—‹ [Related algorithms](https://jb243.github.io/pages/2162)

> â‘£ **Gaussian process**: the state process {X<sub>t</sub>} is such that any finite subset follows a joint Gaussian distribution

> â‘¤ **Gaussian-Markov process**

>> â—‹ **Condition 1.** {X<sub>t</sub>} is a Gaussian process

>> â—‹ **Condition 2.** Markov property: P(X<sub>n+1</sub> âˆˆ A <span>ã…£</span> X<sub>0</sub>, Â·Â·Â·, X<sub>n</sub>) = P(X<sub>n+1</sub> âˆˆ A <span>ã…£</span> X<sub>n</sub>)

<br>

<br>

## **3. Laws of Stochastic Control Theory**

â‘´ **Lemma 1.** In open-loop control, x<sub>t</sub> is a function of x<sub>0</sub>, u<sub>0:t-1</sub>, w<sub>0:t-1</sub>, and y<sub>t</sub> is a function of x<sub>0</sub>, u<sub>0:t-1</sub>, w<sub>0:t-1</sub>, v<sub>t</sub>

<br>

<img width="599" height="207" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 2 13 47" src="https://github.com/user-attachments/assets/ac28b4fd-761c-4b26-948d-c19f489626d5" />

<br>

â‘µ **Lemma 2.** open-loop system vs. feedback system

> â‘  **open loop control**: u<sub>t</sub> := g<sub>t</sub>(y<sub>0:t</sub>, u<sub>0:t-1</sub>) = g<sub>t</sub>(u<sub>0:t-1</sub>).

> â‘¡ **feedback control**: cases where past outputs y<sub>0:t</sub> influence the control action u

> â‘¢ Under DDS, open-loop and feedback systems are equivalent.

>> â—‹ open-loop â†’ feedback (proof): Given an open-loop control input sequence u, define a feedback control that ignores the state (i.e., a map returning the predetermined u<sub>t</sub> at each time t). Then, from the initial state x<sub>0</sub>, it produces the same trajectory and cost. Thus, regardless of DDS/SDS, for any open-loop there exists a feedback policy that is equivalent at that initial state. That is, open-loop âŠ‚ feedback holds always.

>> â—‹ feedback â†’ open-loop (proof): In a DDS, all control inputs are uniquely determined (determinism). Therefore, if you pre-specify the same input sequence u as an open-loop policy, the resulting state evolution is identical and so is the cost.

> â‘¡ Under SDS, open-loop and feedback systems are not equivalent

>> â—‹ **Counterexample 1.**

<br>

<img width="298" height="368" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 2 14 49" src="https://github.com/user-attachments/assets/fcac0b86-524a-4099-bcad-0cfe27647da7" />

<br>

â‘¶ **Lemma 3.** **policy independence**: If W<sub>t</sub> is independent of X<sub>0:t-1</sub>, U<sub>0:t-1</sub>, then â„™(x<sub>t+1</sub><sup>g</sup> âˆˆ A <span>|</span> x<sub>0:t</sub>, u<sub>0:t</sub>) = â„™(x<sub>t+1</sub><sup>g</sup> âˆˆ A <span>|</span> x<sub>t</sub>, u<sub>t</sub>) = â„™(f<sub>t</sub>(x<sub>t</sub>, u<sub>t</sub>, w<sub>t</sub>) âˆˆ A <span>|</span> x<sub>t</sub>, u<sub>t</sub>) (**Markov property**), so dependence on policy g disappears

<br>

<img width="300" height="285" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 2 16 25" src="https://github.com/user-attachments/assets/a841d6d1-4d9a-46f9-9f78-b5753b49c126" />

<br>

> â‘  In DDS, if you know the current state, you can know the next state immediately, but in SDS, past states matter, so conditional probabilities on the history are important.

> â‘¡ That is, when w<sub>t</sub> is independent, system evolution follows natural laws + pure noise, so the policy is irrelevant; but if w<sub>t</sub> depends on the policy, the policy changes the noise distribution, so the future state distribution depends on the policy.

> â‘¢ Sometimes written as W<sub>t</sub> â«« x<sub>0</sub>, U<sub>0:t-1</sub>, W<sub>0:t-2</sub> instead of W<sub>t</sub> â«« x<sub>0</sub>, U<sub>0:t-1</sub>, but the former is a stronger claim.

â‘· **Lemma 4.** **Gaussian process** (GP)

> â‘  Definition: the state process {X<sub>t</sub>} is such that any finite subset follows a joint Gaussian distribution

> â‘¡ **4-1.** Even if each X<sub>i</sub> is Gaussian, it does not imply {X<sub>i</sub>}<sub>iâˆˆâ„•</sub> is a GP.

>> â—‹ Example: X<sub>2</sub> = X<sub>1</sub> *I*{<span>|</span>X<sub>1</sub><span>|</span> â‰¤ k} + (-X<sub>1</sub>) *I*{<span>|</span>X<sub>1</sub><span>|</span> > k}, Y = (X<sub>1</sub> + X<sub>2</sub>) / 2 is not a GP

> â‘¢ **4-2.** For X<sub>t+1</sub> = AX<sub>t</sub> + BU<sub>t</sub> + GW<sub>t</sub>, X<sub>0</sub> ~ ğ’©(0, âˆ‘<sub>0</sub>), W<sub>t</sub> ~ ğ’©(0, Q), {X<sub>t</sub>} is a GP

> â‘£ **4-3.** Under a feedback policy, {X<sub>t</sub>} is generally not a GP

> > â—‹ Example: If U<sub>t</sub> := g<sub>t</sub>(Y<sub>t</sub>) = g<sub>t</sub>(X<sub>t</sub>) = X<sub>t</sub><sup>2</sup>, then X<sub>1</sub> = AX<sub>0</sub> + BX<sub>0</sub><sup>2</sup> + GW<sub>0</sub>, which is not Gaussian

> â‘¤ (Note) **MMSE** (minimum mean-square estimator)

<br>

<img width="449" height="38" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 9 49 29" src="https://github.com/user-attachments/assets/ef12dfc3-3813-4761-a9b8-33943b51ad30" />

<br>

> â‘¥ (Note) **orthogonality principle**

<br>

<img width="300" height="26" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 9 50 20" src="https://github.com/user-attachments/assets/44661695-ddf0-4556-9130-5626fcf459fb" />

<br>

> â‘¦ (Note) **LMMSE** (linear minimum mean-square estimator): If X and Y are jointly Gaussian, then LMMSE = MMSE holds

<br>

<img width="161" height="34" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 9 50 46" src="https://github.com/user-attachments/assets/a9ec5dab-5b53-4ea7-b6de-e2257b21fdbe" />

<br>

â‘¸ **Lemma 5.** **multi-step prediction**

> â‘  In general, â„™(x<sub>t+2</sub><sup>g</sup> âˆˆ A <span>|</span> x<sub>t</sub>, u<sub>t</sub>, u<sub>t+1</sub>) â‰  â„™(x<sub>t+2</sub><sup>g</sup> âˆˆ A <span>|</span> x<sub>0:t</sub>, u<sub>0:t+1</sub>)

<br>

<img width="502" height="273" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 9 52 01" src="https://github.com/user-attachments/assets/2382de29-9682-47f6-ae97-f2148d18c4dc" />

<br>

> â‘¡ Since u<sub>t+1</sub> = g<sub>t</sub>(y<sub>0:t+1</sub>, u<sub>0:t</sub>) that is not independent of u<sub>0:t-1</sub> implies information about w<sub>t</sub> via observation, conditioning on u<sub>t+1</sub> breaks the past-independence of w<sub>t</sub>: here â€œpastâ€ means x<sub>0:t-1</sub>, u<sub>0:t-1</sub>

>> â—‹ **Counterexample 1.** In open-loop control, u<sub>t+1</sub> = g<sub>t</sub>(u<sub>0:t</sub>) holds, so it cannot imply information about wt, hence equality holds.

>> â—‹ **Counterexample 2.** When w<sub>t</sub> is a constant

>> â—‹ **Counterexample 3.** When u<sub>t</sub> is defined to have the Markov property and memoryless feedback, e.g., u<sub>t</sub> = Î¼<sub>t</sub>(x<sub>t</sub>): the following is the case y<sub>t</sub> = x<sub>t</sub> = u<sub>t</sub>

<br>

<img width="246" height="333" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 9 54 51" src="https://github.com/user-attachments/assets/375d7041-6970-49e6-9623-19563f95ce8d" />

<br>

> â‘¢ multi-step prediction with open-loop control

<br>

<img width="466" height="35" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 9 55 21" src="https://github.com/user-attachments/assets/c10995c6-98a2-44bd-94b7-2036f2485582" />

<br>

> â‘£ **Chapman-Kolmogorov decomposition**

<br>

<img width="546" height="294" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 9 55 47" src="https://github.com/user-attachments/assets/0f52ce9e-6c8f-41a9-9717-2a7e6d5f0218" />

<br>

â‘¹ **Lemma 6.** **linear Gaussian state-space model**

> â‘  (Note) **Gaussian-Markov process**

>> â—‹ **Condition 1.** {X<sub>t</sub>} is a Gaussian process

>> â—‹ **Condition 2.** Markov property: P(X<sub>n+1</sub> âˆˆ A | X<sub>0</sub>, Â·Â·Â·, X<sub>n</sub>) = P(X<sub>n+1</sub> âˆˆ A | X<sub>n</sub>)

> â‘¡ System definition

<br>

<img width="198" height="208" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 9 57 08" src="https://github.com/user-attachments/assets/1c825a4f-25ea-46b2-b9a4-a7291b845b5f" />

<br>

>> â—‹ Markov property: applies even with feedback policy

<br>

<img width="278" height="27" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 9 57 35" src="https://github.com/user-attachments/assets/b0448876-e3a5-4e74-a0c1-546142625455" />

<br>

>> â—‹ multi-step Markov property

<br>

<img width="446" height="248" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 9 58 08" src="https://github.com/user-attachments/assets/dd51a25b-5405-4bc8-a71d-9e3ce6b354f7" />

<br>

>> â—‹ mean propagation

<br>

<img width="403" height="33" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 9 58 34" src="https://github.com/user-attachments/assets/3931106d-6da3-4e15-a25b-00ab4675831c" />

<br>

>> â—‹ cross-covariance Cov(**X**<sub>t+m</sub>, **X**<sub>t</sub>)

<br>

<img width="498" height="202" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 9 59 18" src="https://github.com/user-attachments/assets/eabaf28e-9f85-4074-a97d-a04e47017cfb" />

<br>

>> â—‹ covariance propagation

<br>

<img width="450" height="191" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 9 59 48" src="https://github.com/user-attachments/assets/ece67a8f-b4bb-4e1d-9258-691b8a6ba055" />

<br>

> â‘¢ **DALE** (discrete-time algebraic Lyapunov equation)

>> â—‹ If the absolute values of all eigenvalues (including complex ones) of a square matrix A are less than 1, the matrix is defined as stable: because A<sup>âˆ</sup> = 0

>> â—‹ If A is stable, then âˆ‘<sub>âˆ</sub> = lim<sub>tâ†’âˆ</sub> âˆ‘<sub>t</sub> = lim<sub>tâ†’âˆ</sub> ğ”¼[(X<sub>t</sub> - ğ”¼[X<sub>t</sub>])(X<sub>t</sub> - ğ”¼[X<sub>t</sub>])áµ€] exists uniquely

<br>

<img width="636" height="72" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 10 01 17" src="https://github.com/user-attachments/assets/380b05a6-c4db-44e3-a17d-9b8bc1a09f0f" />

<br>

>> â—‹ Proof of uniqueness of âˆ‘<sub>âˆ</sub>

<br>

<img width="552" height="240" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 10 01 48" src="https://github.com/user-attachments/assets/6f446d79-9a1c-49e8-9cf4-28cd2de53b4f" />

<br>

>> â—‹ stability of A is a **sufficient**, but **not necessary** condition: âˆ‘<sub>âˆ</sub> may still exist uniquely even if A is not stable

> â‘£ **reachability**

>> â—‹ Definition

<br>

<img width="700" height="115" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 10 02 24" src="https://github.com/user-attachments/assets/e8a36fb8-de6d-4914-9044-449c77f95397" />

<br>

>> â—‹ **Theorem 1.** The following are all equivalent: assume w âˆˆ â„<sup>s</sup>

<br>

<img width="700" height="147" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 10 02 54" src="https://github.com/user-attachments/assets/1c335335-ac4f-464a-bb0f-65990d3c6cce" />

<br>

>>> â—‹ In condition 3, the noise sequence w should be interpreted as the control input applied to the system; due to them, the system can be steered from 0 to a given state x over n time steps.

>> â—‹ **Theorem 2.** **Lyapunov stability test**

<br>

<img width="700" height="131" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 10 03 34" src="https://github.com/user-attachments/assets/aeb1568e-edf9-4db7-b5d5-854602df1dd3" />

<br>

>>> â—‹ Note that in condition 2 it is PD (positive definite), not PSD (positive semidefinite)

â‘º **Lemma 7.** [Graph Theory](https://jb243.github.io/pages/616)

> â‘  strongly connected (= irreducible, communicable): a condition where from any node i in the graph one can reach any other node j

<br>

<img width="181" height="280" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 10 04 05" src="https://github.com/user-attachments/assets/f8a6de25-c6b1-42bf-a3a5-c78649d56a6d" />

**Figure 1.** Example of irreducible

<br>

<img width="206" height="311" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 10 04 55" src="https://github.com/user-attachments/assets/051ce217-a9e3-4ad8-9689-86c916ec4473" />

**Figure 2.** Example of reducible (state 3 is a sink)

<br>

> â‘¡ period: the period of a specific node i is the greatest common divisor of the lengths of all paths from i back to i

>> â—‹ Example: with two nodes A, B connected by two edges A=B, the period of each node is 2

<br>

<img width="859" height="170" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 10 05 44" src="https://github.com/user-attachments/assets/d92111d7-767e-4d2d-969a-e689714632ba" />

**Figure 3.** Example of a matrix with period m

S<sub>1</sub> â†’ S<sub>2</sub> â†’ Â·Â·Â· â†’ S<sub>m</sub> â†’ S<sub>1</sub> â†’ Â·Â·Â· has such a cycle

<br>

> â‘¢ aperiodic: all nodes have period 1

>> â—‹ aperiodic âŠ‚ irreduicible

>> â—‹ Example: if each node has a walk to itself, it is aperiodic

> â‘£ stationary state: If Pr(x<sub>n</sub> <span>|</span> x<sub>n-1</sub>) is independent of n, the Markov process is stationary (time-invariant)

> â‘¤ regular

>> â—‹ regular âŠ‚ irreduicible

>> â—‹ For some natural number k, every entry of the power Mk of the transition matrix M is positive (i.e., nonzero)

> â‘¥ transition matrix

<br>

<img width="456" height="177" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 10 06 49" src="https://github.com/user-attachments/assets/faff3d86-9ccb-4cbd-9c57-76adc359f6a9" />

<br>

> â‘¦ Markov policy: u<sub>t</sub> = g<sub>t</sub>(x<sub>t</sub>)

> â‘§ One can prove the second law of thermodynamics (law of increasing entropy) using a Markov process

>> â—‹ Because one can simulate the law of diffusion: provided a uniform stationary distribution is assumed

>> â—‹ Related concept: random walk

> â‘¨ **Perron-Frobenius theorem**

>> â—‹ **Theorem 1.** If a Markov chain with transition matrix P is strongly connected, there exists exactly one stationary distribution **q**

>>> â—‹ The stationary distribution satisfies P**q** = **q**

>> â—‹ **Theorem 2.** If a Markov chain with transition matrix P is strongly connected and aperiodic, it is called an **Ergodic Markov chain** and satisfies:

>>> â—‹ P<sub>ij</sub>: probability of transition from node j to node i. âˆ‘<sub>i</sub> P<sub>ij</sub> = 1

>>> â—‹ **2-1.** The (i, j) entry P<sub>ij</sub>(k) of P<sup>k</sup> converges to **q**<sub>i</sub> as k â†’ âˆ: note it converges to the same value for fixed i regardless of j

>>> â—‹ **2-2.** Regardless of the initial state **x**<sub>0</sub>, the k-th state **x**<sub>k</sub> converges to **q** as k â†’ âˆ

â‘» **Lemma 8.** Value function following Markov property

> â‘  **recursive and backward iteration**: for example, with present value (discounting future value to present),

<br>

<img width="569" height="652" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 11 12 32" src="https://github.com/user-attachments/assets/71c9a2a9-e5b2-422e-8b0b-fb0f06a48a29" />

<br>

>> â—‹ J<sub>T</sub><sup>g</sup> âˆˆ â„<sup>1Ã—1</sup>

>> â—‹ Ï€<sub>0</sub> âˆˆ â„<sup>1Ã—n</sup>: initial distribution of the Markov chain

>> â—‹ V<sub>0</sub><sup>g</sup> âˆˆ â„<sup>nÃ—1</sup>: vector of state-wise value functions collecting expected cumulative cost at each state under policy g

> â‘¡ **Bellman equation****:** related to the discounted cost problem

>> â—‹ (Note) time-homogeneous: {x<sub>t</sub><sup>g</sup>}<sub>tâ‰¥0</sub> and {x<sub>t</sub><sup>g</sup>}<sub>tâ‰¥Ï„,âˆ€Ï„âˆˆâ„¤<sup>+</sup></sub> follow the same distribution. Also means strictly stationary.

<br>

<img width="841" height="280" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-07 á„‹á…©á„Œá…¥á†« 1 34 29" src="https://github.com/user-attachments/assets/279f5a7f-dd38-43c8-985a-6ea3fcd7f6fa" />

<br>

>> â—‹ **Condition 1.** time-homogeneous transition: P<sub>t</sub>(j <span>|</span> i, u) = P(j <span>|</span> i, u) âˆ€t

>> â—‹ **Condition 2.** time-homogeneous cost: C<sub>t</sub>(x, y) = C(x, y) âˆ€t

>> â—‹ **Condition 3.** stationary policy: g<sub>t</sub> = g âˆ€t

>> â—‹ If all the above hold, one can obtain the following fixed-point equation

<br>

<img width="398" height="245" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 10 10 36" src="https://github.com/user-attachments/assets/2b7ba5cf-3dcb-428a-a5ef-c764e7e6e193" />

<br>

>> â—‹ Since P<sup>g</sup> is stable, all eigenvalues have absolute value less than 1, so det(I - Î²P<sup>g</sup>) = Î² det( (1/Î²)I - P<sup>g</sup> ) â‰  0

<br>

<img width="201" height="195" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 10 12 01" src="https://github.com/user-attachments/assets/710ac80e-c439-4a5c-b687-ced04c799a26" />

<br>

>>> â—‹ V<sup>g</sup> âˆˆ â„<sup>nÃ—1</sup>: vector of state-wise value functions collecting expected discounted cumulative cost at each state under policy g

<br>

<img width="500" height="65" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 10 12 57" src="https://github.com/user-attachments/assets/bb1d01b0-e238-4732-96cf-e5ad211e37d1" />

<br>

>>> â—‹ P<sup>g</sup> âˆˆ â„<sup>nÃ—n</sup>: transition matrix; the (i, j) entry is the probability of transition from i to j

> â‘¢ **CesÃ ro limit**: related to the long-term average cost problem

<br>

<img width="553" height="136" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 10 13 39" src="https://github.com/user-attachments/assets/e1f7e8d8-a5df-4c59-a8a8-f7ec5ac58903" />

<br>

> â‘£ **Poisson equation**: related to average cost

<br>

<img width="503" height="264" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 10 14 13" src="https://github.com/user-attachments/assets/bb71851f-1c70-4979-8655-dd0002b9c4b1" />

<br>

>> â—‹ J<sup>g</sup> is unique

<br>

<img width="350" height="253" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 10 14 45" src="https://github.com/user-attachments/assets/9f6ccc83-4392-475d-acc0-552829a2c658" />

<br>

>> â—‹ L<sup>g</sup>: relative value function. L<sup>g</sup> is not unique (**âˆµ** L<sup>g</sup> + Î±**1** âˆ€Î± âˆˆ â„ is also a solution to the Poisson equation)

>> â—‹ Existence of solutions

<br>

<img width="599" height="273" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 10 15 31" src="https://github.com/user-attachments/assets/dc4df273-2ffb-48f3-868b-babcc5be93fa" />

<br>

â‘¼ **Lemma 9.** When not irreducible

> â‘  If P<sup>g</sup> is not irreducible, the state space S splits into the transient states T and one or more recurrent communicating classes C<sub>1</sub>, Â·Â·Â·

> â‘¡ transient state: visited only finitely many times. Eventually the process leaves the transient states and enters a recurrent state.

>> â—‹ **Theorem:** The stationary distribution Ï€<sup>g</sup> assigns probability 0 to all transient states.

<br>

<img width="752" height="278" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-07 á„‹á…©á„’á…® 11 27 58" src="https://github.com/user-attachments/assets/1279e4e6-bb1b-4359-a3c9-8b89164e3686" />

<br>

>> â—‹ (i) Proof: Pigeonhole principle

>> â—‹ (ii) Proof

<br>

<img width="439" height="247" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-07 á„‹á…©á„’á…® 11 28 44" src="https://github.com/user-attachments/assets/04025777-b0c7-491f-916b-830831f83486" />

<br>

> â‘¢ recurrent state: since a recurrent communicating class is a closed set, no communication occurs with outside nodes

>> â—‹ i â†’ j: means there exists a path with positive probability from i to j

>> â—‹ i â†”ï¸ j: means i â†’ j and j â†’ i; i and j communicate

>> â—‹ positive recurrent: the mean return time to that state is finite. A chain starting in a positive recurrent state has a unique stationary distribution.

>> â—‹ null recurrent: the mean return time to that state is infinite. No stationary distribution exists.

>> â—‹ absorbing state: a state that, once entered, you remain in forever

> â‘£ **Example 1.** Finite state space

>> Let S = {0, 1, Â·Â·Â·, I}. Since V<sup>g</sup>(0) = 0 and C(0, g(0)) = 0, we can focus only on Åš = S \ {0} = {1, Â·Â·Â·, I}, the non-absorbing states. Let á¹¼<sup>g</sup> be the value vector for states in Åš, and let R<sup>g</sup> be the submatrix of P<sup>g</sup> for transitions among states inside Åš. Then the system of equations for these states is á¹¼<sup>g</sup> = cÌƒ + R<sup>g</sup>á¹¼<sup>g</sup>. To show uniqueness of á¹¼<sup>g</sup>, suppose there are two solutions á¹¼<sub>1</sub><sup>g</sup>, á¹¼<sub>2</sub><sup>g</sup>. Let their difference be U<sup>g</sup> = á¹¼<sub>1</sub><sup>g</sup> - á¹¼<sub>2</sub><sup>g</sup>; subtracting the two equations yields U<sup>g</sup> = R<sup>g</sup>U<sup>g</sup> = â‹¯ = (R<sup>g</sup>)<sup>n</sup>U<sup>g</sup> = â‹¯ = **0** (âˆµ lim<sub>nâ†’âˆ</sub> (R<sup>g</sup>)<sup>n</sup> = 0, method of infinite descent) â‡” á¹¼<sub>1</sub><sup>g</sup> = á¹¼<sub>2</sub><sup>g</sup>. Thus, in a finite state space where state 0 is absorbing and all other states can reach 0, the first-passage-time cost equation has a unique nonnegative solution.

> â‘¤ **Example 2.** Countably infinite state space

<br>

<img width="593" height="164" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 10 40 17" src="https://github.com/user-attachments/assets/51e73f3c-8fae-41e3-bc3e-e677123ea938" />

<br>

>> Uniqueness is not trivial. Assuming the solution is bounded often allows one to show uniqueness. Consider the equation for the difference of two solutions U<sup>g</sup> = R<sup>g</sup>U<sup>g</sup>. Connecting this to the diagram yields U<sup>g</sup>(â„“+1) - U<sup>g</sup>(â„“) = (Î» - 1)(U<sup>g</sup>(â„“) - U<sup>g</sup>(â„“-1)). The consecutive differences Î”(â„“) = U<sup>g</sup>(â„“+1) - U<sup>g</sup>(â„“) form a geometric sequence with ratio (Î» - 1). If <span>|</span>Î» - 1<span>|</span> < 1, these differences converge to 0, suggesting a bounded solution. If <span>|</span>Î» - 1<span>|</span> â‰¥ 1, the differences may diverge, implying that uniqueness may fail.

â‘½ **Lemma 10.** [Martingale](https://jb243.github.io/pages/910)

> â‘  **Doob's theorem**

>> â—‹ Ïƒ(X<sub>1</sub>, X<sub>2</sub>, Â·Â·Â·, X<sub>n</sub>): the smallest Ïƒ-algebra that makes X<sub>1</sub>, X<sub>2</sub>, Â·Â·Â·, X<sub>n</sub> measurable

>> â—‹ Doob's theorem: Ïƒ(X<sub>1</sub>, X<sub>2</sub>, Â·Â·Â·, X<sub>n</sub>) is equivalent to the collection of all functions of the form g(X<sub>1</sub>, X<sub>2</sub>, Â·Â·Â·, X<sub>n</sub>)

>> â—‹ The larger the Ïƒ-algebra, the more functions are measurable with respect to it; i.e., the more information it contains.

> â‘¡ filtration

>> â—‹ a collection of Ïƒ-algebras ordered increasingly by inclusion

>> â—‹ Ordered by âŠ†; if â„±<sub>1</sub> âŠ† â„±<sub>2</sub>, then â„±<sub>2</sub> is afterwards relative to â„±<sub>1</sub>

>> â—‹ For convenience, let time index t = 0, 1, 2, â‹¯; then the filtration is {â„±<sub>t</sub>}<sub>tâˆˆâ„¤<sup>+</sup></sub> and satisfies â„±<sub>s</sub> âŠ† â„±<sub>t</sub> for all s â‰¤ t

>> â—‹ Intuition: represents situations where information increases as observations accumulate over time

> â‘¢ **martingale**

>> â—‹ Property of conditional expectation

>>> â—‹ For any random variable Y, ğ”¼[Y <span>|</span> X<sub>1</sub>, Â·Â·Â·, X<sub>n</sub>] = ğ”¼[Y <span>|</span> Ïƒ(X<sub>1</sub>, Â·Â·Â·, X<sub>n</sub>)] holds

>>> â—‹ Reason: because Ïƒ(X<sub>1</sub>, Â·Â·Â·, X<sub>n</sub>) is equivalent to the set of all functions generated by X<sub>1</sub>, Â·Â·Â·, X<sub>n</sub>

>> â—‹ Martingale: a stochastic process {X<sub>t</sub>}<sub>tâˆˆâ„¤<sup>+</sup></sub> adapted to a filtration {â„±<sub>t</sub>}<sub>tâˆˆâ„¤<sup>+</sup></sub> that satisfies all of the following

>>> â—‹ **Condition 1.** X<sub>t</sub> is â„±<sub>t</sub>-measurable for all t âˆˆ â„¤<sup>+</sup>

>>>> â—‹ If s â‰¤ t â‰¤ sâ€² and â„±><sub>s</sub>â€‹ âŠ† â„±<sub>t</sub> âŠ† â„±<sub>sâ€²</sub>, then X<sub>t</sub> âˆˆ â„±<sub>t</sub> is not â„±<sub>s</sub>-measurable (insufficient information) but is â„±<sub>sâ€²</sub>-measurable.

>>> â—‹ **Condition 2.** ğ”¼[<span>|</span>X<sub>t</sub><span>|</span>] is finite for all t âˆˆ â„¤<sup>+</sup>

>>> â—‹ **Condition 3.** ğ”¼[X<sub>t</sub> <span>|</span> â„±<sub>s</sub>] = X<sub>s</sub> almost surely for all s â‰¤ t and all t âˆˆ â„¤<sup>+</sup>

>>>> â—‹ **Interpretation:** Given only the information up to time s (â„±<sub>s</sub>), the optimal prediction of X<sub>t</sub> equals X<sub>s</sub> (i.e., the prediction is constrained to X<sub>s</sub>).

>>>> â—‹ **Remark:** The martingale property is needed only when predicting the future from the past. In particular, for s > t we have ğ”¼[X<sub>t</sub> <span>ã…£</span> â„±<sub>s</sub>] = X<sub>t</sub> regardless of whether (X<sub>t</sub>) is a martingale (assuming integrability).

>>>> â—‹ For s < t, ğ”¼[X<sub>s</sub> ã…£ â„±<sub>t</sub>] = Xs also holds, because X<sub>s</sub> is â„±<sub>s</sub>-measurable, but has insufficient information due to â„±<sub>s</sub> âŠ† â„±<sub>t</sub>.

>> â—‹ Note: an i.i.d. process is generally not a martingale (except for the constant process)

>> â—‹ Application: ğ”¼[U<sup>g</sup>(X<sub>t</sub><sup>g</sup>) <span>|</span> X<sub>t-1</sub><sup>g</sup>] = U<sup>g</sup>(X<sub>t-1</sub><sup>g</sup>)

â‘¾ **Lamma 11.** Value function which doesn't follow Markov property

> â‘  Problem definition: cost-to-go function under **perfect observation**. Since control input {U<sub>t</sub>, ..., U<sub>1</sub>} is measurable by {X<sub>t</sub>, ..., X<sub>0</sub>}, the following holds:

<br>

<img width="495" height="223" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-07 á„‹á…©á„’á…® 1 59 08" src="https://github.com/user-attachments/assets/a6f57e3f-04a6-465a-abd4-b063fe9d0eeb" />

<br>

> â‘¡ When following Markov property, Bellman equation is established. Here, J<sub>t</sub><sup>g</sup>, V<sub>t</sub><sup>g<sup>M</sup></sup>(X<sub>t</sub>) is a cost-to-go from t to future.

<br>

<img width="602" height="96" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-07 á„‹á…©á„’á…® 2 01 58" src="https://github.com/user-attachments/assets/b7a3e947-d8db-4c15-b0e0-c67e0416eb97" />

<br>
 
> â‘¢ **Markovization theorem** (Markov policy sufficiency, reduction to Markov policy)

>> â—‹ **Theorem:** In a finite-horizon MDP, for any general (possibly history-dependent and randomized) policy g, there exists a behavioral Markov policy g<sup>M</sup> such that, under the same initial distribution Î¼, the joint distributions of (X<sub>t</sub>, U<sub>t</sub>) for all t = 0, ..., Tâˆ’1 and of X<sub>T</sub> â€‹ are identical. Consequently, the performance J<sup>g</sup> = ğ”¼<sup>g</sup>[âˆ‘<sub>t=0 to Tâˆ’1</sub> C<sub>t</sub>(X<sub>t</sub>, U<sub>t</sub>) + C<sub>T</sub>(X<sub>T</sub>)] equals J<sup>g<sup>M</sup></sup>. Hence, without loss of optimality, one may restrict attention to randomized Markov policies.

>> â—‹ **Proof**

<br>

<img width="497" height="354" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-10 á„‹á…©á„Œá…¥á†« 1 54 36" src="https://github.com/user-attachments/assets/8922b0d2-330c-49ad-82f7-071bd5085b60" />

<br>

> â‘£ **Comparison principle** 

>> â—‹ **Theorem:** By working backward from the objective and ensuring the Bellman inequality holds at each step, the initial value V0 serves as a lower bound for the performance of all possible policies. The set of actions that achieve equality at each stage collectively constitute the optimal policy. Thus, the optimality can be verified or constructed by combining locally optimal (stage-wise) choices.

<br>

<img width="602" height="232" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-07 á„‹á…©á„’á…® 2 03 17" src="https://github.com/user-attachments/assets/58b38ece-1ccf-4909-8180-10abdbb725d7" />

<br>

>> â—‹ Proof: Using [mathematical induction](https://www.youtube.com/watch?v=t9RBuyBmFdQ) on backward

>>> â—‹ **Case 1.** t = T: Since J<sub>T</sub><sup>g</sup> = ğ”¼<sup>g</sup>[C<sub>T</sub>(X<sub>T</sub><sup>g</sup>) ã…£ X<sub>T</sub><sup>g</sup>, ..., X<sub>1</sub><sup>g</sup>, X<sub>0</sub>] = C<sub>T</sub>(X<sub>T</sub><sup>g</sup>) â‰¥ V<sub>T</sub>(X<sub>T</sub><sup>g</sup>) (âˆµ (V1)) holds, the induction assumptions are still established.

>>> â—‹ **Case 2.** If the induction assumptions are established on â„“ = t+1, ..., T, the fact that the assumptions still hold for â„“ = t can be verified as follows:

<br>

<img width="621" height="551" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-07 á„‹á…©á„’á…® 2 06 19" src="https://github.com/user-attachments/assets/550f9ab6-e2b5-48bb-b822-51757d4f3b37" />

<br>

>> â—‹ Corollary

<br>

<img width="651" height="77" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-07 á„‹á…©á„’á…® 2 06 44" src="https://github.com/user-attachments/assets/e83a46bd-c749-4dbf-931a-b6ba3746aa93" />

<br>
 
> â‘¤ **Hamiltonian-Jacobi-Bellman (HJB) equation** 

>> â—‹ Theorem: States that the optimla solution satisfies Markov property.

<br>

<img width="723" height="308" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-07 á„‹á…©á„’á…® 2 07 21" src="https://github.com/user-attachments/assets/4e828a43-294b-4596-9f0d-7eacd0d6fd2b" />

<br>
 
>> â—‹ Proof

>>> â—‹ Proof of theorem 1 is shown at the comparison principle already.

>>> â—‹ Proof of sufficiency on theorem 2: Regarding Markov policy g<sup>M</sup>, 

<br>

<img width="527" height="230" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-07 á„‹á…©á„’á…® 2 08 32" src="https://github.com/user-attachments/assets/38111529-06a1-4c90-a4fc-cfcc04188b71" />

<br>

>>> â—‹ Proof of necessity on theorem 2

<br>

---

_Input: 2025.08.26 23:34_
