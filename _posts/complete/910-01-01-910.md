## **Lecture 3-3. Sigma Algebra**(Ïƒ-algebra)

Recommended post: ã€Statisticsã€‘ Lecture 3. [Probability Space](https://jb243.github.io/pages/1623)

---

**1.** [Sigma Algebra](#1-sigma-algebra)

**2.** [Random Variable](#2-random-variable)

**3.** [Filtration](#3-filtration)

---

<br>

## **1. Sigma Algebra**

â‘´ Probability Space (Î©, â„±)

> â‘  Î©: Sample Space

> â‘¡ â„±: Sigma Algebra (Ïƒ-algebra, event space), i.e., a collection of subsets of Î©

>> â—‹ **Example 1.** When Î© = {1, 2, 3}, the Ïƒ-algebra â„± = {âˆ…, Î©} corresponds to the case of knowing nothing

>> â—‹ **Example 2.** When Î© = {1, 2, 3}, the Ïƒ-algebra â„± = {âˆ…, {1}, {2}, {3}, {1, 2}, {1, 3}, {2, 3}, {1, 2, 3}, Î©} corresponds to the case of being able to see all events

>> â—‹ **Example 3.** When Î© = {1, 2, 3}, the Ïƒ-algebra â„± = {âˆ…, {1}, {2, 3}, Î©} represents an intermediate case

> â‘¢ Ï‰ âˆˆ Î©: Realized sample. In a random process, it means a sample path.

â‘µ Algebra

> â‘  **Condition 1.** non-empty: Î© âˆˆ â„± or âˆ… âˆˆ â„± holds

> â‘¡ **Condition 2.** Closed under complement: If A âˆˆ â„±, then A<sup>C</sup> = Î© - A âˆˆ â„± also holds

>> â—‹ Considering together with the non-empty condition implies that âˆ… and Î© must be elements of â„±

> â‘¢ **Condition 3.** Closed under finite union: If A, B âˆˆ â„±, then A âˆª B âˆˆ â„± also holds

>> â—‹ If A<sub>1</sub>, â‹¯, A<sub>n</sub> âˆˆ â„±, then âˆª<sub>i</sub> A<sub>i</sub> = A<sub>1</sub> + â‹¯ + A<sub>n</sub> âˆˆ â„± also holds

â‘¶ Sigma Algebra (Ïƒ-algebra)

> â‘  Motivation: When the sample space is very large (e.g., Î© = â„), formal probability theory does not apply well (e.g., â„± = 2<sup>Î©</sup>), so it is necessary to restrict â„± to a sigma algebra. Related to Caratheodoryâ€™s extension theorem.

> â‘¡ **Condition 1.** Must be an algebra

> â‘¢ **Condition 2.** Closed under **countably infinite** unions: For A<sub>i</sub> âˆˆ â„±, âˆª<sub>i</sub> A<sub>i</sub> = A<sub>1</sub> + â‹¯ + A<sub>âˆ</sub> âˆˆ â„±

> â‘£ Intuitive meaning of sigma algebra

>> â—‹ A collection of subsets on a non-empty set Î©

>> â—‹ The set of all events to which probability can be assigned

>> â—‹ The set of all functions/random variables that can be generated

> â‘¤ Ïƒ-algebras can vary in size

>> â—‹ Trivial Ïƒ-algebra: {âˆ…, â„} (smallest)

>> â—‹ Ïƒ(ğ’œ): The smallest Ïƒ-algebra containing all elements of ğ’œ, i.e., generated by ğ’œ

>> â—‹ Borel Ïƒ-algebra: â„¬(â„) (the smallest Ïƒ-algebra containing **all open sets**)

>> â—‹ Countable/co-countable Ïƒ-algebra: the collection of sets that are countable or co-countable

>> â—‹ Power-set Ïƒ-algebra: ğ’«(â„) (largest)

>> â—‹ Ïƒ-algebra of Lebesgue measurable sets: â„’ (larger than Borel; a prototypical â€œcompletionâ€)

>> â—‹ Intersections of Ïƒ-algebras are again Ïƒ-algebras.

> â‘¥ Borel Ïƒ-algebra: The smallest sigma algebra containing **all open sets**

>> â—‹ Î© = â„, â„± = â„¬(â„)

>> â—‹ Using the properties of sigma algebra, open intervals â†’ closed intervals, half-open intervals, singletons {x}, [1,3] âˆª [4,5] are also included in the Borel algebra

>> â—‹ Complicated sets like the set of rationals and irrationals are also Borel sets

>> â—‹ More complex sets obtained by countably many unions, differences, or intersections of intervals are all Borel sets

>> â—‹ Not limited only to â„; can be defined for any topological space X: for example, on [0,1], on â„<sup>n</sup>, or on any general topological space, each has its own Borel Ïƒ-algebra

>> â—‹ In fact, there exist sets such as Lebesgue non-measurable sets that cannot be made by the Borel Ïƒ-algebra: related to uncountable infinity

<br>

<br>

## **2. Random Variable**

â‘´ Probability Distribution

> â‘  A function that assigns values to elements of â„±, i.e., â„™: â„± â†¦ [0, 1]

> â‘¡ **Condition 1.** â„™(Î©) = 1

> â‘¢ **Condition 2.** For countably infinite, mutually exclusive {A<sub>i</sub>}<sub>iâˆˆâ„•</sub>, â„™(A<sub>1</sub> + â‹¯ + A<sub>âˆ</sub>) = â„™(A<sub>1</sub>) + â‹¯ + â„™(A<sub>âˆ</sub>)

>> â—‹ Disjoint: A<sub>i</sub> âˆ© A<sub>j</sub> = âˆ…

â‘µ Random Variable (measurable function): Linking events to values

> â‘  **Expression 1.** If there exists a function X such that <sup>âˆ€</sup>B âˆˆ â„¬(â„), X<sup>-1</sup>(B) âˆˆ â„± (measurable), then that function is called a random variable

> â‘¡ **Expression 2.** X: Î© â†’ â„ is a random variable â‡” X<sup>-1</sup>(A) = {Ï‰ âˆˆ Î©: X(Ï‰) âˆˆ A} âˆˆ â„± âˆ€ A âˆˆ â„¬(â„)

>> â—‹ **Meaning 1.** Existence of inverse image: i.e., â„¬(â„) is **â„±-measurable**. If X is continuous, this usually holds.

>> â—‹ **Meaning 2.** Existence of range of inverse image: i.e., the inverse imageâ€™s range is a subset of â„±

>> â—‹ â„± can be viewed as equivalent to the collection of all random variables (or functions) measurable with respect to it

>> â—‹ Example: â„™<sub>X</sub>(x âˆˆ A) = â„™(X<sup>-1</sup>(A))

> â‘¢ **Expression 3.** X is measurable â‡” âˆ€a âˆˆ â„, {Ï‰ : X(Ï‰) â‰¤ a} âˆˆ â„±

> â‘£ Precise distinction between a random variable and "measurable"

>> â—‹ Measurable can be defined without a measure: it only requires the pairs (Î©, â„±) and (â„, ğ’¢). In practice, we usually take ğ’¢ = â„¬(â„).

>> â—‹ A random variable is a measurable function on a probability space, i.e., with the measure â„™ included.

> â‘¤ **Example 1.** Bernoulli distribution 

>> â—‹ Domain = Î© = {Head, Tail}

>> â—‹ â„± = 2<sup>Î©</sup> = {âˆ…, {Head}, {Tail}, {Head, Tail}}

>> â—‹ Codomain = {0, 1}

>> â—‹ ğ’¢ = 2<sup>Codomain</sup> = {âˆ…, {0}, {1}, {0, 1}}

>> â—‹ As there is an element of â„± corresponding to an arbitrary element of ğ’¢, X : Î© â†’ {0, 1} is measurable.

> â‘¥ **Example 2.** An example of a function which is not measurable

>> â—‹ Î© = [0, 1], â„± = {âˆ…, [0, 1]}

>> â—‹ ğ’¢ = â„¬(â„) contains [0, 1/2], but there is no element of â„± corresponding to this.

>> â—‹ Thus, X : (Î©, â„±) â†’ (â„, ğ’¢) is not measurable. Specifically, it is called "not â„±-measurable", and â„± needs more information.

> â‘¦ General measurable space

>> â—‹ Definition: If X: Î© â†’ Î©<sub>1</sub> between two measurable spaces (Î©, â„±) and (Î©<sub>1</sub>, â„±<sub>1</sub>) satisfies the following condition, then X is called a random variable

<br>

<img width="297" height="28" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 10 57 03" src="https://github.com/user-attachments/assets/a4a4364d-ee6e-4c56-9c6f-adbab0387a7a" />

<br>

> â‘§ Random Process (stochastic process)

>> â—‹ Definition: X: â„ Ã— Î© â†¦ E, where for each i âˆˆ â„, there exists a random variable X(i, **Â·**): Î© â†¦ E

â‘¶ Ï€-class and Î»-class

> â‘  Definition of Ï€-class: If A, B âˆˆ ğ’ âŠ‚ 2<sup>Î©</sup>, then A âˆ© B âˆˆ ğ’

> â‘¡ Definition of Î»-class

<br>

<img width="453" height="133" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 10 57 40" src="https://github.com/user-attachments/assets/631065f2-c2e0-4dab-87d8-048b385082ed" />

<br>

> â‘¢ Property of Î»-class

<br>

<img width="505" height="133" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 10 57 59" src="https://github.com/user-attachments/assets/8810852d-203c-4c94-a0e7-a0863110a39c" />

<br>

> â‘£ Dynkin's theorem: If ğ’Ÿ is a Ï€-class, ğ’ is a Î»-class, and ğ’Ÿ âŠ‚ ğ’, then Ïƒ(ğ’Ÿ) âŠ‚ ğ’

â‘· Stationary

> â‘  Strictly stationary

<br>

<img width="400" height="60" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 10 58 21" src="https://github.com/user-attachments/assets/2025080f-9080-4187-9e4d-019b62c260e7" />

<br>

> â‘¡ Wide-sense stationary: Strictly stationary is also wide-sense stationary

<br>

<img width="404" height="63" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 10 58 45" src="https://github.com/user-attachments/assets/12f06f0d-c95d-4b45-8aeb-2f2cb7e1ed03" />

<br>

â‘¸ Independence

> â‘  Definition of independence using joint distribution: â„™(X<sub>1</sub> âˆˆ B<sub>1</sub>, X<sub>2</sub> âˆˆ B<sub>2</sub>) = â„™(X<sub>1</sub> âˆˆ B<sub>1</sub>) â„™(X<sub>2</sub> âˆˆ B<sub>2</sub>) âˆ€B<sub>1</sub>, B<sub>2</sub> âˆˆ â„¬(â„)

> â‘¡ Definition of independence using moments

> â‘¢ Definition of independence using moment generating function

> â‘£ Definition of independence using Ïƒ-algebra: Ïƒ(x<sub>1</sub>) and Ïƒ(x<sub>2</sub>) are independent (where Ïƒ(X) = {X<sup>-1</sup>(A): A âˆˆ â„¬(â„)})

<br>

<img width="202" height="106" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-10-05 á„‹á…©á„Œá…¥á†« 11 00 03" src="https://github.com/user-attachments/assets/f4819f7a-74d7-44a7-ab66-14dc18a2701e" />

<br>

â‘¹ Markov Process

> â‘  Bayes' Rule: â„™(A <span>|</span> B) = â„™(A âˆ© B) / â„™(B) if â„™(B) > 0

> â‘¡ Conditional expectation ğ”¼[X <span>|</span> ğ’¢]

> â‘¢ Markov process: âˆ€A âˆˆ ğ“”, â„™(X<sub>i<sub>n</sub></sub> âˆˆ A <span>|</span> X<sub>i<sub>1</sub></sub>, X<sub>i<sub>2</sub></sub>, â‹¯, X<sub>i<sub>n-1</sub></sub>) = â„™(X<sub>i<sub>n</sub></sub> âˆˆ A <span>|</span> X<sub>i<sub>n-1</sub></sub>), i.e., the current state depends only on the immediately preceding state

<br>

<br>

## **3. Filtration**

â‘´ **Doob's Theorem**

> â‘  Ïƒ(X<sub>1</sub>, X<sub>2</sub>, Â·Â·Â·, X<sub>n</sub>): The smallest Ïƒ-algebra making X<sub>1</sub>, X<sub>2</sub>, Â·Â·Â·, X<sub>n</sub> measurable

> â‘¡ Doob's Theorem: Ïƒ(X<sub>1</sub>, X<sub>2</sub>, Â·Â·Â·, X<sub>n</sub>) is equivalent to the collection of all functions of the form g(X<sub>1</sub>, X<sub>2</sub>, Â·Â·Â·, X<sub>n</sub>)

> â‘¢ The larger the Ïƒ-algebra, the greater the number of measurable functions with respect to it â€” i.e., more information

â‘µ Filtration

> â‘  A collection of Ïƒ-algebras arranged in an increasing order by inclusion

> â‘¡ Ordered by âŠ†, and if â„±<sub>1</sub> âŠ† â„±<sub>2</sub>, then â„±<sub>2</sub> is after â„±<sub>1</sub>

> â‘¢ For convenience, with time index t = 0, 1, 2, â‹¯, filtration is {â„±<sub>t</sub>}<sub>tâˆˆâ„¤<sup>+</sup></sub>, satisfying â„±<sub>s</sub> âŠ† â„±<sub>t</sub> for all s â‰¤ t

> â‘£ Intuitive meaning: Represents a situation where information increases as time passes and observations accumulate

â‘¶ **Martingale**

> â‘  Property of conditional expectation

>> â—‹ For any random variable Y, ğ”¼[Y <span>|</span> X<sub>1</sub>, Â·Â·Â·, X<sub>n</sub>] = ğ”¼[Y <span>|</span> Ïƒ(X<sub>1</sub>, Â·Â·Â·, X<sub>n</sub>)] holds

>> â—‹ Reason: Ïƒ(X<sub>1</sub>, Â·Â·Â·, X<sub>n</sub>) is equivalent to the set of all functions generated by X<sub>1</sub>, Â·Â·Â·, X<sub>n</sub>

> â‘¡ Martingale: A stochastic process {X<sub>t</sub>}<sub>tâˆˆâ„¤<sup>+</sup></sub> adapted to filtration {â„±<sub>t</sub>}<sub>tâˆˆâ„¤<sup>+</sup></sub> satisfies all the following conditions

>> â—‹ **Condition 1.** For all t âˆˆ â„¤<sup>+</sup>, X<sub>t</sub> is â„±<sub>t</sub>-measurable

>>> â—‹ If s â‰¤ t â‰¤ s', â„±<sub>s</sub> âŠ† â„±<sub>t</sub> âŠ† â„±<sub>s'</sub>, x<sub>t</sub> âˆˆ â„±<sub>t</sub> is not â„±<sub>s</sub>-measurable (âˆµ lack of information), but â„±<sub>s'</sub>-measurable.

>> â—‹ **Condition 2.** For all t âˆˆ â„¤<sup>+</sup>, ğ”¼[<span>|</span>X<sub>t</sub><span>|</span>] is finite

>> â—‹ **Condition 3.** For all t âˆˆ â„¤<sup>+</sup>, ğ”¼[X<sub>t</sub> <span>|</span> â„±<sub>s</sub>] = X<sub>s</sub>, almost surely for all s â‰¤ t

>>> â—‹ **Interpretation:** Given only the information up to time s (â„±<sub>s</sub>), the optimal prediction of X<sub>t</sub> equals X<sub>s</sub> (i.e., the prediction is constrained to X<sub>s</sub>).

>>> â—‹ **Remark:** The martingale property is needed only when predicting the future from the past. In particular, for s > t we have ğ”¼[X<sub>t</sub> ã…£ â„±<sub>s</sub>] = X<sub>t</sub> regardless of whether (X<sub>t</sub>) is a martingale (assuming integrability).

>>> â—‹ For s < t, ğ”¼[X<sub>s</sub> ã…£ â„±<sub>t</sub>] = Xs also holds, because X<sub>s</sub> is â„±<sub>s</sub>-measurable, but has insufficient information due to â„±<sub>s</sub> âŠ† â„±<sub>t</sub>.

> â‘¢ Note: An i.i.d. process is generally not a martingale (except when it is a constant process)

<br>

---

_Input: 2025.09.07 08:40_
