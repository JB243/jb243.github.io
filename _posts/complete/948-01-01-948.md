## **Chapter 25. Collection of Numerical Analysis Algorithms**

Recommended Post: 【Algorithm】 [Algorithm Index](https://jb243.github.io/pages/1278)

---

**1.** [Thomas Algorithm](#1-thomas-algorithm)

**2.** [Gauss Elimination Algorithm](#2-gauss-elimination-algorithm)

**3.** [Doolittle Method](#3-doolittle-method)

**4.** [Cholesky Method](#4-cholesky-method)

**5.** [Jacobi Iterative Method](#4-jacobi-iterative-method)

**6.** [Gauss-Seidel Algorithm](#5-gauss-seidel-algorithm)

**7.** [Improved Euler Algorithm (Heum's Method)](#7-improved-euler-algorithm-heum-s-method)

**8.** [Classical RUNGE-KUTTA Algorithm](#8-classifcal-runge-kutta-algorithm)

**9.** [Improved RUNGE-KUTTA Algorithm](#9-improved-runge-kutta-algorithm)

**10.** [RUNGE-KUTTA-FEHLBERG Algorithm](#10-runge-kutta-fehlberg-algorithm)

**11.** [Adams-Bashforth Algorithm](#11-adams-bashforth-algorithm)

**12.** [Adams-Moulton Algorithm](#12-adams-moulton-algorithm)

**13.** [Explicit Method](#13-explicit-method)

**14.** [Fully (Simple) Implicit Method](#14-fully-simple-implicit-method)

**15.** [Crank-Nicolson Method](#15-crank-nicolson-method)

**16.** [Approximation Formulas for Integration](#16-approximation-formulas-for-integration)

**17.** [Optimization Algorithms](#17-optimization-algorithms)

---

<br>

## **1\. Thomas Algorithm** 

A → A* = [a<sub>jk</sub>] = [A**r**]

This algorithm uniquely calculates A**x** = [x<sub>j</sub>] for a tridiagonal matrix A or indicates that a unique solution does not exist.

(Here, let the diagonal elements of A be f, the subdiagonal elements e, and the superdiagonal elements g.)

---

INPUT: Augmented matrix A* = [a<sub>jk</sub>] of n × (n+1) with a<sub>j, n+1</sub> = b<sub>j</sub>

OUTPUT: Solution **x** = [x<sub>j</sub>] of equation (1) or a message indicating no unique solution

> // DECOMPOSITION

> DO k = 2, ..., n

>> e<sub>k</sub> = e<sub>k</sub> / f<sub>k-1</sub>

>> f<sub>k</sub> = f<sub>k</sub> \- e<sub>k</sub> × g<sub>k-1</sub>

> END DO

> // FORWARD SUBSTITUTION

> DO k = 2, ..., n

>> r<sub>k</sub> = r<sub>k</sub> \- e<sub>k</sub> × r<sub>k-1</sub>

> END DO

> // BACKWARD SUBSTITUTION

> x<sub>n</sub> = r<sub>n</sub> / f<sub>n</sub>

> DO k = n - 1, ..., 1

>> x<sub>k</sub> = (r<sub>k</sub> \- g<sub>k</sub> × x<sub>k+1</sub>) / f<sub>k</sub>

> END DO

End GAUSS

---

<br>

<br>

## **2\. Gauss Elimination Algorithm** 

A → A* = [a<sub>jk</sub>] = [A**b**]

This algorithm uniquely calculates **x** = [x<sub>j</sub>] for A**x** = **b** or indicates that a unique solution does not exist.

---

INPUT: Augmented matrix A* = [a<sub>jk</sub>] of n × (n+1) with a<sub>j, n+1</sub> = b<sub>j</sub>

OUTPUT: Solution **x** = [x<sub>j</sub>] of equation (1) or a message indicating no unique solution

> For k = 1, ..., n - 1 do:

>> m = k

>> For j = k + 1, ..., n, do:

>>> If |a<sub>mk</sub>| < |a<sub>jk</sub>| then m = j End

>> End

>> If a<sub>mk</sub> = 0 then

>>> OUTPUT "No unique solution"

>>> Stop

>> End

>> Else exchange row k with row m

>> For j = k + 1, ..., n, do:

>>> m<sub>jk</sub> = a<sub>jk</sub> / a<sub>kk</sub>

>>> For p = k + 1, ..., n + 1, do:

>>>> a<sub>jp</sub> = a<sub>jp</sub> \- m<sub>jk</sub>a<sub>kp</sub>

>>> End

>> End

> End

> If a<sub>nn</sub> = 0 then

>> OUTPUT "No unique solution"

>> Stop

> End

> x<sub>n</sub> = a<sub>n,n+1</sub> / a<sub>nn</sub> [Start Back Substitution]

> For i = n - 1, ..., 1, do:

>> x<sub>i</sub> = (1/a<sub>ii</sub>)(a<sub>i,n+1</sub> \- (a<sub>i,i+1</sub>x<sub>i+1</sub> \+ ... + a<sub>i, n</sub>x<sub>n</sub>))

> End

> OUTPUT x = [x<sub>j</sub>]

> Stop

End GAUSS

---

<br>

<br>

## **3\. Doolittle Method**

---

A**x** = LU**x** = **b**

U**x** = L<sup>-1</sup>**b** = **y**

**x** = U<sup>-1</sup>**y**

---

<br>

<br>

## **4\. Cholesky Method**

---

A**x** = LL<sup>T</sup>**x** = **b**

L<sup>T</sup>**x** = L<sup>-1</sup>**b** = **y**

**x** = (L<sup>T</sup>)<sup>-1</sup>**y**

---

<br>

<br>

## **5\. Jacobi Iterative Method**

---

ex.

> x<sub>1</sub> \- 0.25x<sub>2</sub> \- 0.25x<sub>3</sub> = 50 ⇔ x<sub>1</sub> = 0.25x<sub>2</sub> \+ 0.25x<sub>3</sub> \+ 50

> -0.25x<sub>1</sub> \+ x<sub>2</sub> \- 0.25x<sub>4</sub> = 50 ⇔ x<sub>2</sub> = 0.25x<sub>1</sub> \+ 0.25x<sub>4</sub> \+ 50

> -0.25x<sub>1</sub> \+ x<sub>3</sub> \- 0.25x<sub>4</sub> = 25 ⇔ x<sub>3</sub> = 0.25x<sub>1</sub> \+ 0.25x<sub>4</sub> \+ 25

> -0.25x<sub>2</sub> \- 0.25x<sub>3</sub> \+ x<sub>4</sub> = 25 ⇔ x<sub>4</sub> = 0.25x<sub>2</sub> \+ 0.25x<sub>3</sub> \+ 25

cf.

> **x**<sup>(m+1)</sup> = **b** \- L**x**<sup>(m)</sup> \- U**x**(m) = **b** \- (L+U)**x**<sup>(m)</sup>

---

<br>

<br>

**6\. Gauss-Seidel Algorithm** 

A, **b** , **x**(0), ε, N

This algorithm calculates the solution **x** to A**x** = **b** given an initial approximation **x**<sup>(0)</sup> for an n × n matrix A = [a<sub>jk</sub>] where a<sub>jj</sub> ≠ 0 for j = 1, ..., n.

---

INPUT: A, **b** , initial approximation **x**<sup>(0)</sup>, tolerance ε, maximum iterations N

OUTPUT: Approximation **x**<sup>(m)</sup> = [**x** <sub>j</sub><sup>(m)</sup>] or an error message indicating **x**<sup>(N)</sup> does not satisfy the tolerance condition

> For m = 0, ..., N - 1 do:

>> For j = 1, ..., n do:

>>> x<sub>j</sub><sup>(m)</sup> = (1/a<sub>jj</sub>)(b<sub>j</sub> \- (a<sub>j1</sub>x<sub>1</sub><sup>(m+1)</sup> \+ ... + a<sub>j, j-1</sub>x<sub>j-1</sub><sup>(m+1)</sup>) - (a<sub>j, j+1</sub>x<sub>j+1</sub><sup>(m)</sup> \+ ... + a<sub>jn</sub>x<sub>n</sub><sup>(m)</sup>))

>> End

>> If <sup>∀</sup>j, |x<sub>j</sub><sup>(m+1)</sup> \- x<sub>j</sub><sup>(m)</sup>| then

>>> OUTPUT **x**<sup>(m+1)</sup>

>>> Stop

>> End

> End

> OUTPUT

End GAUSS-SEIDEL

ex.

> x<sub>1</sub> - 0.25x<sub>2</sub> - 0.25x<sub>3</sub> = 50 ⇔ x<sub>1</sub> = 0.25x<sub>2</sub> + 0.25x<sub>3</sub> + 50

> -0.25x<sub>1</sub> + x<sub>2</sub> - 0.25x<sub>4</sub> = 50 ⇔ x<sub>2</sub> = 0.25x<sub>1</sub> + 0.25x<sub>4</sub> + 50

> -0.25x<sub>1</sub> + x<sub>3</sub> - 0.25x<sub>4</sub> = 25 ⇔ x<sub>3</sub> = 0.25x<sub>1</sub> + 0.25x<sub>4</sub> + 25

> -0.25x<sub>2</sub> - 0.25x<sub>3</sub> + x<sub>4</sub> = 25 ⇔ x<sub>4</sub> = 0.25x<sub>2</sub> + 0.25x<sub>3</sub> + 25

cf. 

> **x**<sup>(m+1)</sup> = **b** - L**x**<sup>(m+1)</sup> - U**x**<sup>(m)</sup>  ⇔  **x**<sup>(m+1)</sup> = (I + L)<sup>-1</sup>**b** - (I + L)<sup>-1</sup>U**x**<sup>(m)</sup>

<br>

<br>

## **7. Improved Euler Algorithm (Heum's Method)** 

f, x<sub>0</sub>, y<sub>0</sub>, h, N

This algorithm solves the initial value problem y' = f(x, y), y(x<sub>0</sub>) = y<sub>0</sub> at equally spaced points x<sub>1</sub> = x<sub>0</sub> + h, x<sub>2</sub> = x<sub>0</sub> + 2h, ..., x<sub>N</sub> = x<sub>0</sub> + Nh.

Here, f is a function defined on [x<sub>0</sub>, x<sub>N</sub>] that guarantees a unique solution.

---

INPUT: Initial values x<sub>0</sub>, y<sub>0</sub>, step size h, number of steps N

OUTPUT: Approximation y<sub>n+1</sub> of the solution y(x<sub>n+1</sub>) at x<sub>n+1</sub> = x<sub>0</sub> + (n+1)h (n = 0, ..., N-1)

> For n = 0, 1, ..., N-1 do:

>> x<sub>n+1</sub> = x<sub>n</sub> + h

>> k<sub>1</sub> = hf(x<sub>n</sub>, y<sub>n</sub>)

>> k<sub>2</sub> = hf(x<sub>n+1</sub>, y<sub>n</sub>+k<sub>1</sub>)

>> y<sub>n+1</sub> = y<sub>n</sub> + 0.5(k<sub>1</sub> + k<sub>2</sub>)

>> OUTPUT x<sub>n+1</sub>, y<sub>n+1</sub>

> End

> Stop

End EULER

---

<br>

<br>

## **8. Classical RUNGE-KUTTA Algorithm** 

f, x<sub>0</sub>, y<sub>0</sub>, h, N

This algorithm solves the initial value problem y' = f(x, y), y(x<sub>0</sub>) = y<sub>0</sub> at equally spaced points x<sub>1</sub> = x<sub>0</sub> + h, x<sub>2</sub> = x<sub>0</sub> + 2h, ..., x<sub>N</sub> = x<sub>0</sub> + Nh.

Here, f is a function defined on [x<sub>0</sub>, x<sub>N</sub>] that guarantees a unique solution.

---

INPUT: Initial values x<sub>0</sub>, y<sub>0</sub>, step size h, number of steps N

OUTPUT: Approximation y<sub>n+1</sub> of the solution y(x<sub>n+1</sub>) at x<sub>n+1</sub> = x<sub>0</sub> + (n+1)h (n = 0, ..., N-1)

> For n = 0, 1, ..., N-1 do:

>> k<sub>1</sub> = hf(x<sub>n</sub>, y<sub>n</sub>)

>> k<sub>2</sub> = hf(x<sub>n</sub> + 0.5h, y<sub>n</sub> + 0.5k<sub>1</sub>)

>> k<sub>3</sub> = hf(x<sub>n</sub> + 0.5h, y<sub>n</sub> + 0.5k<sub>2</sub>)

>> k<sub>4</sub> = hf(x<sub>n</sub> + h, y<sub>n</sub> + k<sub>3</sub>)

>> x<sub>n+1</sub> = x<sub>n</sub> + h

>> y<sub>n+1</sub> = y<sub>n</sub> + (1/6)(k<sub>1</sub> + 2k<sub>2</sub> + 2k<sub>3</sub> + k<sub>4</sub>)

>> OUTPUT x<sub>n+1</sub>, y<sub>n+1</sub>

> End

> Stop

End RUNGE-KUTTA

---

<br>

<br>

## **9. Improved RUNGE-KUTTA Algorithm** 

f, x<sub>0</sub>, y<sub>0</sub>, h, N

This algorithm solves the initial value problem y' = f(x, y), y(x<sub>0</sub>) = y<sub>0</sub> at equally spaced points x<sub>1</sub> = x<sub>0</sub> + h, x<sub>2</sub> = x<sub>0</sub> + 2h, ..., x<sub>N</sub> = x<sub>0</sub> + Nh.

Here, f is a function defined on [x<sub>0</sub>, x<sub>N</sub>] that guarantees a unique solution.

---

INPUT: Initial values x<sub>0</sub>, y<sub>0</sub>, step size h, number of steps N

OUTPUT: Approximation y<sub>n+1</sub> of the solution y(x<sub>n+1</sub>) at x<sub>n+1</sub> = x<sub>0</sub> + (n+1)h (n = 0, ..., N-1)

> For n = 0, 1, ..., N-1 do:

>> k<sub>1</sub> = hf(x<sub>n</sub>, y<sub>n</sub>)

>> k<sub>2</sub> = hf(x<sub>n</sub> + (1/4)h, y<sub>n</sub> + (1/4)k<sub>1</sub>)

>> k<sub>3</sub> = hf(x<sub>n</sub> + (3/8)h, y<sub>n</sub> + (3/32)k<sub>1</sub> + (9/32)k<sub>2</sub>)

>> k<sub>4</sub> = hf(x<sub>n</sub> + (12/13)h, y<sub>n</sub> + (1932/2197)k<sub>1</sub> - (7200/2197)k<sub>2</sub> + (7296/2197)k<sub>3</sub>)

>> k<sub>5</sub> = hf(x<sub>n</sub> + h, y<sub>n</sub> + (439/216)k<sub>1</sub> - 8k<sub>2</sub> + (3680/513)k<sub>3</sub> - (845/4104)k<sub>4</sub>)

>> x<sub>n+1</sub> = x<sub>n</sub> + h

>> y<sub>n+1</sub> = y<sub>n</sub> + (25/216)k<sub>1</sub> + (1408/2565)k<sub>3</sub> + (2197/4104)k<sub>4</sub> - (1/5)k<sub>5</sub>

>> OUTPUT x<sub>n+1</sub>, y<sub>n+1</sub>

> End

> Stop

End RUNGE-KUTTA

---

<br>

<br>

## **10. RUNGE-KUTTA-FEHLBERG Algorithm** 

f, x<sub>0</sub>, y<sub>0</sub>, h, N

This algorithm solves the initial value problem y' = f(x, y), y(x<sub>0</sub>) = y<sub>0</sub> at equally spaced points x<sub>1</sub> = x<sub>0</sub> + h, x<sub>2</sub> = x<sub>0</sub> + 2h, ..., x<sub>N</sub> = x<sub>0</sub> + Nh.

Here, f is a function defined on [x<sub>0</sub>, x<sub>N</sub>] that guarantees a unique solution.

---

INPUT: Initial values x<sub>0</sub>, y<sub>0</sub>, step size h, number of steps N

OUTPUT: Approximation yn+1 of the solution y(x<sub>n+1</sub>) at x<sub>n+1</sub> = x<sub>0</sub> + (n+1)h (n = 0, ..., N-1)

> For n = 0, 1, ..., N-1 do:

>> k<sub>1</sub> = hf(x<sub>n</sub>, y<sub>n</sub>)

>> k<sub>2</sub> = hf(x<sub>n</sub> + (1/4)h, y<sub>n</sub> + (1/4)k<sub>1</sub>)

>> k<sub>3</sub> = hf(x<sub>n</sub> + (3/8)h, y<sub>n</sub> + (3/32)k<sub>1</sub> + (9/32)k<sub>2</sub>)

>> k<sub>4</sub> = hf(x<sub>n</sub> + (12/13)h, y<sub>n</sub> + (1932/2197)k<sub>1</sub> - (7200/2197)k<sub>2</sub> + (7296/2197)k<sub>3</sub>)

>> k<sub>5</sub> = hf(x<sub>n</sub> + h, y<sub>n</sub> + (439/216)k<sub>1</sub> - 8k<sub>2</sub> + (3680/513)k<sub>3</sub> - (845/4104)k<sub>4</sub>)

>> k<sub>6</sub> = hf(x<sub>n</sub> + (1/2)h, y<sub>n</sub> - (8/27)k<sub>1</sub> + 2k<sub>2</sub> + (3544/2565)k<sub>3</sub> - (1859/4104)k<sub>4</sub> - (11/40)k<sub>5</sub>)

>> x<sub>n+1</sub> = x<sub>n</sub> + h

>> y<sub>n+1</sub> = y<sub>n</sub> + (16/135)k<sub>1</sub> + (6656/12825)k<sub>3</sub> + (28561/56430)k<sub>4</sub> - (9/50)k<sub>5</sub> + (2/55)k<sub>6</sub>

>> OUTPUT x<sub>n+1</sub>, y<sub>n+1</sub>

> End

> Stop

End RUNGE-KUTTA

---

<br>

<br>

## **11. Adams-Bashforth Algorithm** 

x<sub>n</sub>, x<sub>n-1</sub>, x<sub>n-2</sub>, x<sub>n-3</sub>, f<sub>n</sub>, f<sub>n-1</sub>, f<sub>n-2</sub>, f<sub>n-3</sub>, h, N

This algorithm considers the initial value problem with a unique solution in some open interval containing x<sub>0</sub>:

y' = f(x, y), y(x<sub>0</sub>) = y<sub>0</sub>, f<sub>n</sub> = f(x<sub>n</sub>, y<sub>n</sub>).

This method replaces the integrand f(x, y(x)) with an interpolating polynomial using Newton's backward difference formula and integrates it.

---

INPUT: x<sub>n</sub>, x<sub>n-1</sub>, x<sub>n-2</sub>, x<sub>n-3</sub>, f<sub>n</sub>, f<sub>n-1</sub>, f<sub>n-2</sub>, f<sub>n-3</sub>, h, N

OUTPUT: Approximation yn+m of the solution y(x<sub>n+m</sub>) at step x<sub>n+m</sub> = x<sub>0</sub> + (n+m)h (m = 1, ..., N)

> k<sub>4</sub> = hf<sub>n</sub>

> k<sub>3</sub> = hf<sub>n-1</sub>

> k<sub>2</sub> = hf<sub>n-2</sub>

> k<sub>1</sub> = hf<sub>n-3</sub>

> For m = 1, ..., N do:

>> x<sub>n+m</sub> = x<sub>n+m-1</sub> + h

>> y<sub>n+m</sub> = y<sub>n+(m-1)</sub> + (1/24)(55k<sub>4</sub> - 59k<sub>3</sub> + 37k<sub>2</sub> - 9k<sub>1</sub>)

>> k<sub>1</sub> = k<sub>2</sub>

>> k<sub>2</sub> = k<sub>3</sub>

>> k<sub>3</sub> = k<sub>4</sub>

>> k<sub>4</sub> = hf(x<sub>n+m</sub>, y<sub>n+m</sub>)

>> OUTPUT x<sub>n+m</sub>, y<sub>n+m</sub>

> End

> Stop

End ADAMS-BASHFORTH

---

<br>

<br>

## **12. Adams-Moulton Algorithm** 

x<sub>n</sub>, x<sub>n-1</sub>, x<sub>n-2</sub>, x<sub>n-3</sub>, f<sub>n</sub>, f<sub>n-1</sub>, f<sub>n-2</sub>, f<sub>n-3</sub>, h, N

This algorithm considers the initial value problem with a unique solution in some open interval containing x0:

y' = f(x, y), y(x<sub>0</sub>) = y<sub>0</sub>, f<sub>n</sub> = f(x<sub>n</sub>, y<sub>n</sub>).

This method adds a predictor-corrector step. The error estimate is as follows:

ε<sub>n+1</sub> = (1/15)(y<sub>n+1</sub> - y*<sub>n+1</sub>).

---

INPUT: x<sub>n</sub>, x<sub>n-1</sub>, x<sub>n-2</sub>, x<sub>n-3</sub>, f<sub>n</sub>, f<sub>n-1</sub>, f<sub>n-2</sub>, f<sub>n-3</sub>, h, N

OUTPUT: Approximation yn+m of the solution y(x<sub>n+m</sub>) at step x<sub>n+m</sub> = x<sub>0</sub> + (n+m)h (m = 1, ..., N)

> k<sub>4</sub> = hf<sub>n</sub>

> k<sub>3</sub> = hf<sub>n-1</sub>

> k<sub>2</sub> = hf<sub>n-2</sub>

> k<sub>1</sub> = hf<sub>n-3</sub>

> For m = 1, ..., N do:

>> x<sub>n+m</sub> = x<sub>n+m-1</sub> + h

>> y*<sub>n+m</sub> = y<sub>n+(m-1)</sub> + (1/24)(55k<sub>4</sub> - 59k<sub>3</sub> + 37k<sub>2</sub> - 9k<sub>1</sub>)

>> k* = f(x<sub>n+m</sub>, y*<sub>n+m</sub>)

>> y<sub>n+m</sub> = y<sub>n+(m-1)</sub> + (1/24)(9k* + 19k<sub>4</sub> - 5k<sub>3</sub> + k<sub>2</sub>)

>> k<sub>1</sub> = k<sub>2</sub>

>> k<sub>2</sub> = k<sub>3</sub>

>> k<sub>3</sub> = k<sub>4</sub>

>> k<sub>4</sub> = hf(x<sub>n+m</sub>, y<sub>n+m</sub>)

>> OUTPUT x<sub>n+m</sub>, y<sub>n+m</sub>

> End

> Stop

End ADAMS-MOULTON

---

<br>

<br>

## **13. Explicit Method**

---

**x**<sup>(m+1)</sup> = A**x**<sup>(m)</sup> + **b**

ex.

> 3×3: A = ((1 - 2λ, λ, 0)<sup>T</sup>, (λ, 1 - 2λ, λ)<sup>T</sup>, (0, λ, 1 - 2λ)<sup>T</sup>), b = (λx<sub>0</sub>, 0, λx<sub>n+1</sub>)<sup>T</sup>

---

<br>

<br>

## **14. Fully (Simple) Implicit Method**

---

**x**<sup>(m+1)</sup> = A<sup>-1</sup>**x**<sup>(m)</sup> - A<sup>-1</sup>**b**

 ex.

> 3×3: A = ((1 + 2λ, -λ, 0)<sup>T</sup>, (-λ, 1 + 2λ, -λ)<sup>T</sup>, (0, -λ, 1 + 2λ)<sup>T</sup>), b = (λx<sub>0</sub>, 0, λx<sub>n+1</sub>)<sup>T</sup>

---

<br>

<br>

## **15. Crank-Nicolson Method**

---

**x**<sup>(m+1)</sup> = A<sub>im</sub><sup>-1</sup>(A<sub>ex</sub>**x**<sup>(m)</sup> + **b**<sub>ex</sub>) - A<sup>-1</sup>**b**<sub>im</sub>

ex.

> 3×3: A = ((1 + 2λ, -λ, 0)<sup>T</sup>, (-λ, 1 + 2λ, -λ)<sup>T</sup>, (0, -λ, 1 + 2λ)<sup>T</sup>), b = (λx<sub>0</sub>, 0, λx<sub>n+1</sub>)<sup>T</sup>

---

<br>

<br>

## **16. Approximation Formulas for Integration**

 ⑴ Trapezoidal Rule
 
> ① Numerical integration method.

> ② **Proof:** Assume f is linear (first-degree) and show that the equality holds on the unit interval.

<br>
 
<img width="484" alt="스크린샷 2024-12-04 오전 11 40 59" src="https://github.com/user-attachments/assets/c22c581e-804e-4bc2-bce1-fe1b118f03d9">

<br>

 ⑵ Simpson's Rule
 
> ① Numerical integration method 

> ② **Proof:** Assume f is quadratic (parabolic) and show that the equality holds on the unit interval.

> ③ More accurate than the trapezoidal rule.

<br>

<img width="561" alt="스크린샷 2024-12-04 오전 11 41 19" src="https://github.com/user-attachments/assets/d78329cf-c3e7-4e7c-9e3c-fba67960014e">

<br>

 ⑶ Simpson's 3/8 Rule

> ① Numerical integration method

> ② **Proof:** Assume f is cubic (third-degree) and show that the equality holds on the unit interval

> ③ More accurate than Simpson's rule.

<br>

<img width="602" alt="스크린샷 2024-12-04 오전 11 41 32" src="https://github.com/user-attachments/assets/7ade3e12-6248-466c-aa8a-350d00f1a82d">

<br>

⑷ Bode's Rule

<br>

<img width="690" alt="스크린샷 2024-12-04 오전 11 41 49" src="https://github.com/user-attachments/assets/5effb035-01e1-4374-a5a5-926d3e0ef122">

<br>

⑸ Moving Average Method

## **17. Optimization Algorithms**

⑴ [Newton-Raphson Method](https://jb243.github.io/pages/1773)

⑵ [Optimal Transport Theorem](https://jb243.github.io/pages/2386)

⑶ [Nelder-Mead Method](https://docs.scipy.org/doc/scipy/reference/optimize.minimize-neldermead.html#optimize-minimize-neldermead)

<br>

```python
import numpy as np
from scipy.optimize import minimize
from scipy.stats import spearmanr

def model(X, coeffs):
    return np.sum(X.T * coeffs, axis=1)

def objective_function(coeffs, X, target):
    prediction = model(X, coeffs)
    return -spearmanr(target, prediction)[0]

def optimize_spearman(X, target, initial_guess=None):
    if initial_guess is None:
        initial_guess = np.zeros(X.shape[0])  # Default to zero coefficients

    result = minimize(objective_function, initial_guess, args=(X, target), method='Nelder-Mead')
    optimal_coeffs = result.x
    maximized_corr = -objective_function(optimal_coeffs, X, target)
    return optimal_coeffs, maximized_corr

# Example usage
# new_bulk = np.array(...) # Replace with your 11x471 matrix
# target_vector = np.array(...) # Replace with your 471-D target vector

# optimal_coeffs, maximized_corr = optimize_spearman(new_bulk, target_vector)
# print("Optimal coefficients:", optimal_coeffs)
# print("Maximized Spearman Correlation:", maximized_corr)
```

<br>

⑷ [Powell Method](https://docs.scipy.org/doc/scipy/reference/optimize.minimize-powell.html#optimize-minimize-powell)

⑸ [CG Method](https://docs.scipy.org/doc/scipy/reference/optimize.minimize-cg.html#optimize-minimize-cg)

⑹ [BFGS Method](https://www.sciencedirect.com/science/article/pii/0009261485805741?viaihub)

> ① Definition: Approximates the inverse of the Hessian matrix to reduce computation in Newton's method.

> ② Formula

<br>

<img width="581" alt="스크린샷 2024-12-04 오전 11 42 24" src="https://github.com/user-attachments/assets/26ec8387-1513-48a0-a73b-ca0b30df9f3d">

<br>

⑺ [L-BFGS-B Method](https://docs.scipy.org/doc/scipy/reference/optimize.minimize-lbfgsb.html#optimize-minimize-lbfgsb)

⑻ [TNC Method](https://docs.scipy.org/doc/scipy/reference/optimize.minimize-tnc.html#optimize-minimize-tnc)

⑼ [COBYLA Method](https://docs.scipy.org/doc/scipy/reference/optimize.minimize-cobyla.html#optimize-minimize-cobyla)

⑽ [SLSQP Method](https://docs.scipy.org/doc/scipy/reference/optimize.minimize-slsqp.html#optimize-minimize-slsqp)

⑾ [Trust-Constraint Method](https://docs.scipy.org/doc/scipy/reference/optimize.minimize-trustconstr.html#optimize-minimize-trustconstr)

⑿ [Dogleg Method](https://docs.scipy.org/doc/scipy/reference/optimize.minimize-dogleg.html#optimize-minimize-dogleg)

⒀ [Newton-CG Method](https://docs.scipy.org/doc/scipy/reference/optimize.minimize-newtoncg.html#optimize-minimize-newtoncg)

⒁ [Trust-NCG Method](https://docs.scipy.org/doc/scipy/reference/optimize.minimize-trustncg.html#optimize-minimize-trustncg)

⒂ [Trust-Exact Method](https://docs.scipy.org/doc/scipy/reference/optimize.minimize-trustexact.html#optimize-minimize-trustexact)

⒃ [Trust-Krylov Method](https://docs.scipy.org/doc/scipy/reference/optimize.minimize-trustkrylov.html#optimize-minimize-trustkrylov)

⒄ Nesterov method

⒅ SANN(simulated annealing)

> ① A slow stochastic global optimization method.

> ② Works well with non-differentiable or non-convex functions.

⒆ Related to Programming

> ① Optimization algorithm-related R packages and functions: RSolnp, optim, GenSA, alabama

<br>

---

_Input: 2016.12.11 02:10_

_Revised: 2024.03.28 22:20_
