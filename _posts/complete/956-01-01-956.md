## **Chapter 20. Autoencoder**

Recommended post: 【Algorithm】 [Algorithm Table of Contents](https://jb243.github.io/pages/1278)

---

**1.** [Overview](#1-overview)

**2. Type 1.** [Variational Autoencoder](#2-type-1-variational-autoencoder)

**3. Type 2.** [Graph Autoencoder](#3-type-2-graph-autoencoder)

**4. Type 3.** [Matrix Factorization](#4-type-3-matrix-factorization)

**5. Type 4.** [Latent Topic Model](#5-type-4-latent-topic-model)

**6. Type 5.** [Generative Adversarial Network](#6-type-5-generative-adversarial-network)

**7. Type 6.** [Flow Model](#7-type-6-flow-model)

---

<br>

## **1. Overview**

 ⑴ Definition: A neural network that compresses input data as much as possible, then reconstructs the compressed data back to its original form

> ① **Encoder**: An artificial neural network from the input layer to the hidden layer. Also called a **recognition network**

>> ○ Recognition network: A specialized network that detects patterns and assigns meaning

> ② **Decoder**: An artificial neural network from the hidden layer to the output layer. Also called a **generative network**

>> ○ Generative network: A network capable of generating new data very similar to the input data

> ③ The encoder and decoder are trained together

<br>

![image](https://github.com/user-attachments/assets/db77c95f-6d19-4873-86f7-f2f92799153e)

**Figure 1.** Structure of an Autoencoder

<br>

⑵ Formalization

> Z = E<sub>θ</sub>(X), X̂ = D<sub>ϕ</sub>(Z)

> ① Z: Latent variable

> ② E<sub>θ</sub>: Encoder network

> ③ X̂: Reconstructed input

> ④ D<sub>ϕ</sub>: Decoder network

 ⑶ Characteristics

> ① Unsupervised learning neural network

> ② The encoder performs dimensionality reduction

> ③ The decoder functions as a generative model

> ④ The number of nodes in the input layer is equal to the output layer

> ⑤ The number of nodes in the hidden layer is less than in the input layer

> ⑥ [Dimensionality reduction algorithms](https://jb243.github.io/pages/2158) can also be considered examples of autoencoders

 ⑷ Effects

> ① **Effect 1.** Data compression: If the hidden layer has fewer nodes than the input layer, the network can compress the input data

> ② **Effect 2.** Data abstraction: Converts complex data into a multidimensional vector, enabling classification or recombination of input data

>> ○ These multidimensional vectors are also called hidden layers or features

>> ○ PCA is a representative recombination algorithm

> ③ **Effect 3.** Latent variable inference: If priors forming the input data are treated as latent variables, the autoencoder can infer them

>> ○ Also referred to as deconvolution

<br>

<br>

## **2. Type 1.** Variational Autoencoder

 ⑴ Overview

> ① Uses latent space and [variational inference](https://jb243.github.io/pages/2145) to incorporate known probability distributions

> ② Depending on the type of distribution, examples include the ZINB autoencoder (zero-inflated negative binomial autoencoder)

 ⑵ Structure

> ① Encoder: Maps input data to a probability distribution in the hidden layer, i.e., a parametrized distribution

> ② Decoder: Treats the parameterized distribution as a prior in Bayesian inference and maps to the output layer

>> ○ Training: Minimizes the difference between the parametric posterior and true posterior using a cost function

> ③ Difference from autoencoder: Autoencoder is **deterministic**, whereas variational autoencoder is **probabilistic**

> ④ Variational autoencoder naturally performs latent matching better than the regular autoencoder

 ⑶ Formalization

> Z ~ q<sub>θ</sub>(Z | X), X̂ = D<sub>ϕ</sub>(Z)

> ① Z: Latent variable

> ② q<sub>θ</sub>: Conditional probability distribution

> ③ X̂: Reconstructed input

> ④ [Loss function difference between AE and VAE](https://jb243.github.io/pages/2432) 

⑷ **Application 1.** [β-VAE](https://openreview.net/pdf?id=Sy2fzU9gl): Each embedding axis corresponds to age, sex, race, respectively 

<br>

<br>

## **3. Type 2.** Graph Autoencoder 

 ⑴ Using [graph theory](https://jb243.github.io/pages/616) and Laplacian matrices, a loss function similar to autoencoders is defined

<br>

![image](https://github.com/user-attachments/assets/16cfe8fe-3708-47e0-9939-e0f01273ebb0)

<br>

 ⑵ When graph autoencoder is combined with variational inference, it is called a variational graph autoencoder (VGAE)

 ⑶ [Comparison of AE, GAE, VAE, VGAE loss functions](https://jb243.github.io/pages/2432)

<br>

<br>

## **4. Type 3.** Matrix Factorization

 ⑴ Definition: An algorithm that decomposes a known matrix A into the product of matrices W and H. A ~ W × H

> ① Matrix A: Represents sample-feature. Known from the samples

> ② Matrix H: Represents variable-feature

> ③ Similar to K-means clustering and PCA

> ④ Since autoencoders include non-linear transformations, they are a broader concept than matrix factorization

> ⑤ The following algorithms are based on the least square method, but [gradient descent method](https://jb243.github.io/pages/1140#:1., 9% 98 95,-\ (Gradientdescent)) can also be used

 ⑵ Algorithm: Find U and V such that R = UV, R ∈ ℝ<sup>5×4</sup>, U ∈ ℝ<sup>5×2</sup>, V ∈ ℝ<sup>2×4</sup>

<br>

```
import numpy as np
R = np.outer([3,1,4,2.,3],[1,1,0,1]) + np.outer([1,3,2,1,2],[1,1,4,1])
U=np.random.rand(5,2)
V=np.random.rand(2,4)
for i in range(3):
    V,_,_,_=np.linalg.lstsq(U, R, rcond=None)
    Ut,_,_,_=np.linalg.lstsq(V.T, R.T, rcond=None)
    U=Ut.T
U@V # it is similar to R!
```

<br>

 ⑶ **3-1.** **NMF** (non-negative matrix factorization)

<br>

```
import numpy as np
R = np.outer([3,1,4,2.,3],[1,1,0,1]) + np.outer([1,3,2,1,2],[1,1,4,1])
U=np.random.rand(5,2)
V=np.random.rand(2,4)
for i in range(20):
    V,_,_,_=np.linalg.lstsq(U, R, rcond=None)
    V = np.where(V >= 0, V, 0) #set negative entries equal zero
    Ut,_,_,_=np.linalg.lstsq(V.T, R.T, rcond=None)
    U=Ut.T
    U = np.where(U >= 0, U, 0) #set negative entries equal zero
U@V # it is similar to R!
```

<br>

 ⑷ **3-2.** **Matrix completion** (Netflix algorithm): Performs matrix factorization on masked R

<br>

```
import numpy as np
R = np.outer([3,1,4,2.,3],[1,1,0,1]) + np.outer([1,3,2,1,2],[1,1,4,1])
mask=np.array([[1.,1,1,1],
               [1,0,0,0],
               [1,0,0,0],
               [1,0,0,0],
               [1,0,0,0]])
               
U=np.random.rand(5,2)
V=np.random.rand(2,4)
RR = U@V
RR = RR*(1-mask)+R*mask

for i in range(20):
    V,_,_,_=np.linalg.lstsq(U, R, rcond=None)
    V = np.where(V >= 0, V, 0)
    Ut,_,_,_=np.linalg.lstsq(V.T, R.T, rcond=None)
    U=Ut.T
    U = np.where(U >= 0, U, 0)
    RR = U@V
    RR = RR*(1-mask)+R*mask

RR*(1-mask)+R*mask # it is similar to R!
```

<br>

 ⑸ **3-3.** [**SVD**](https://jb243.github.io/pages/2158) (singular value decomposition)

 ⑹ **Application 1.** Cell type classification

> ① To obtain cell type proportions from scRNA-seq data collected from tissues

> ② Reducing confounding effects due to cell type heterogeneity is important

> ③ **1-1.** Constrained linear regression

> ④ **1-2.** Reference-based approach

>> ○ **1-2-1.** CIBERSORT (cell-type identification by estimating relative subsets of RNA transcript): Can check cell type proportion and p-value per sample

 ⑺ **Application 2.** Joint NMF: Enables extension to multi-omics

 ⑻ **Application 3.** Metagene extraction

 ⑼ **Application 4.** [Starfysh](https://www.nature.com/articles/s41587-024-02173-8#Methods): An algorithm that infers archetypes in spatial transcriptomics and determines representative anchors for each archetype.

> ① **Step 1.** Construct the autoencoder

>> Y = WBX

>> ○ X ∈ ℝ<sup>S×G</sup>: Input data (spot × gene)

>> ○ D: Number of archetypes

>> ○ B ∈ ℝ<sup>D×S</sup>: Encoder. Infers archetypes; for each archetype, the total distribution across all spots must sum to 1

>> ○ H = BX: Latent variable

>> ○ W ∈ ℝ<sup>S×D</sup>: Decoder. Reconstructs input data; for each spot, the total weight across archetypes must sum to 1.

>> ○ Y = WBX: Reconstructed input

> ② **Step 2.** Solve the optimization algorithm to compute W and B

<br>

<img width="373" alt="스크린샷 2025-06-22 오후 7 17 42" src="https://github.com/user-attachments/assets/0f21c504-b694-4bec-96df-be181b73cd84" />

<br>

> ③ **Step 3.** Spots with the highest weights for each archetype in the W matrix are selected as anchor spots

> ④ **Step 4.** Granularity adjustment: When distances between archetypes are close, merge or adjust them using a hierarchical structure

> ⑤ **Step 5.** For each anchor, search nearest spots to form archetypal communities and identify marker genes

> ⑥ **Step 6.** If a signature gene set is given, add archetypal marker genes to the existing set and recalculate the anchors

>> ○ In this case, use the stable marriage matching algorithm to pair each archetype with the most similar signature

<br>

<br>

## **5. Type 4.** Latent Topic Model

 ⑴ **4-1.** Unigram

 ⑵ **4-2.** LSI (latent semantic indexing)

> ① **Application 1.** Probabilistic LSI

>> ○ **Limitation 1.** Not suitable as a generative model for natural language documents: Difficult to probabilistically model unseen documents

>> ○ **Limitation 2.** The number of parameters increases linearly with the number of documents used in training

 ⑶ **4-3.** Latent Dirichlet Allocation ([LDA](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf))

> ① Overview

>> ○ Definition: Given a document × word proportion, separating it into document × topic and topic × word proportion.

>> ○ In the LDA model, the latent multinomial variable is called a topic.

>> ○ Like NMF, generates a sparse matrix

> ② Formulation: With respect to the topic distribution θ<sub>d</sub> of document d, and the word distribution β<sub>k</sub> of topic k,

<br>

<img width="162" height="68" alt="스크린샷 2025-08-06 오후 5 14 46" src="https://github.com/user-attachments/assets/bc5fc3a4-b5d3-49b7-856b-dab71c3049dc" />

<br>

> ③ **Application 1.** Formalization

>> ○ Premise: Parameters α, β, joint distribution of topic mixture θ, topic set **z** of size N, and word set **w** of size N

>> ○ Objective: Use [Bayes theorem](https://jb243.github.io/pages/1623#:-,,-\(Bayestheroem\)) to find θ and **z**.

<br>

<img width="263" alt="스크린샷 2025-06-22 오후 7 18 38" src="https://github.com/user-attachments/assets/c5017a0c-5566-4f35-8907-0250f5b01b1f" />

<br>

>> ○ Joint probability distribution

<br>

<img width="401" alt="스크린샷 2025-06-22 오후 7 18 57" src="https://github.com/user-attachments/assets/e23490ab-e061-4492-9dac-50c17876e5d2" />

<br>

>> ○ Uses variational EM (expectation maximization) to find θ and **z**

> ④ **Application 2.** Two-level model: On the hidden topic variable z

<br>

<img width="302" alt="스크린샷 2025-06-22 오후 7 19 30" src="https://github.com/user-attachments/assets/980e927f-7f1d-465c-adc9-e13e405f0233" />

<br>

> ⑤ **Application 3.** [ProdLDA](https://arxiv.org/pdf/1703.01488) ([Github](https://github.com/hyqneuron/pytorch-avitm/tree/master))

>> ○ While traditional LDA models the probability of word generation as a mixture of multinomials, ProdLDA models it as a product of experts.

<br>

<img width="497" height="174" alt="스크린샷 2025-08-06 오후 5 17 08" src="https://github.com/user-attachments/assets/24762130-0c09-47c6-9c42-6ec3bc85c06e" />

<br>

>> ○ **Advantage 1.** LDA directly models sampling from probability distributions such as θ<sub>d</sub> ~ Dirichlet(α) and uses inference methods such as variational inference or MCMC-based approaches like Gibbs sampling, making it slow. In contrast, ProdLDA enables much faster inference.

>> ○ **Advantage 2.** While LDA implements topic-word distributions using a mixture model, ProdLDA expresses them as a product over multiple topics. As a result, the word distribution becomes much sharper—since if any component assigns zero probability, the product will also be zero.

>> ○ **Advantage 3.** ProdLDA generates p, whose entries sum to 1 and are non-zero, from z that follows a Gaussian distribution (cf. p = Softmax(z)). This setup allows the integration of pre-trained embeddings, graph features, and other information from the embedding space.

> ⑥ Python Programming: [`sklearn.decomposition.LatentDirichletAllocation`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)

<br>

```python
from sklearn.decomposition import LatentDirichletAllocation
lda = LatentDirichletAllocation(n_components=10, random_state=0)

spotwise_celltype_probability.shape
# (10000, 14)

theta = lda.fit_transform(spotwise_celltype_probability)
print(theta.shape) 
# (10000, 10)

beta = lda.components_
print(beta.shape)
# (10, 14)
```

<br>

<br>

## **6. Type 5.** Generative Adversarial Network

Will be updated later.

<br>

<br>

## **7. Type 6.** Flow Model

⑴ XVFI: A kind of optical flow.

⑵ FILM(Frame Interpolation for Large Motion): Encoder + U-Net like decoder

<br>

---

_Input: 2023.06.27 00:55_

_Modified: 2025.06.22 18:49_
